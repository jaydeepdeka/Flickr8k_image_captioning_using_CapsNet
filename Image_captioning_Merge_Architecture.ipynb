{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "UsageError: Line magic function `%inline` not found.\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "import os\n",
    "import string\n",
    "from utils import model_utils\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump, load\n",
    "\n",
    "# Decoder model imports\n",
    "import numpy as np\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array, prod\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#plot curve\n",
    "import matplotlib.pyplot as plt\n",
    "%inline matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File split for train, test and validation\n",
    "if not ((os.path.exists('train.pkl')) and (os.path.exists('valid.pkl')) and (os.path.exists('test.pkl'))):\n",
    "    train_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.devImages.txt'\n",
    "    test_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.trainImages.txt'\n",
    "    valid_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.testImages.txt'\n",
    "    paths = []\n",
    "    for path in [train_path, valid_path, test_path]:\n",
    "        with open(path, 'r') as fh:\n",
    "            paths = paths + fh.readlines()\n",
    "    sample_idx = np.random.choice(len(paths), size=int(len(paths)), replace=False)\n",
    "\n",
    "    # Train Set 80% of the data\n",
    "    train_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[:int(len(sample_idx)*.80)]]\n",
    "    valid_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.80):int(len(sample_idx)*.90)]]\n",
    "    test_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.90):]]\n",
    "    dump(train_set, open('train.pkl', 'wb'))\n",
    "    dump(valid_set, open('valid.pkl', 'wb'))\n",
    "    dump(test_set, open('test.pkl', 'wb'))\n",
    "else:\n",
    "    train_set = load(open('train.pkl', 'rb'))\n",
    "    valid_set = load(open('valid.pkl', 'rb')) \n",
    "    test_set = load(open('test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_encoder(arch='capsnet'):\n",
    "    \"\"\"\n",
    "        Description: Initiate the encoder \n",
    "        :arch: 'capsnet' or 'vgg'\n",
    "    \"\"\"\n",
    "    if arch=='capsnet':\n",
    "        encoder_model = model_utils.load_DeepCapsNet(input_shape=(64,64,3), n_class=10, routings=3, \\\n",
    "                        weights=r'..\\weights\\deep_caps_best_weights.h5')\n",
    "    else:\n",
    "        encoder_model = model_utils.load_VGG()\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, directory, arch, path):\n",
    "    \"\"\"\n",
    "        Description: Function to extract features through the model\n",
    "        :model: The model object\n",
    "        :directory: Path of the directory of images\n",
    "        :path: Path to save the file\n",
    "    \"\"\"\n",
    "    features = dict()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    print('Feature extraction started')\n",
    "    for name in os.listdir(directory):\n",
    "        image_path = directory + '/' + name\n",
    "        target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "        try:\n",
    "            image = load_img(image_path, target_size=target_size)\n",
    "        except:\n",
    "            print('{} could not be opened. Skipping'.format(image_path))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # Extract the features from the last layer\n",
    "        if arch=='capsnet':\n",
    "            feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "        else:\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        # Populate the dictionary\n",
    "        features[image_id] = feature\n",
    "    path = os.path.join(path, 'features_{}.pkl'.format(arch))\n",
    "    dump(features, open(path, 'wb'))\n",
    "    print('Features extracted and stored at {}'.format(path))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv_capsule_layer3d_1/stack:0\", shape=(5,), dtype=int32)\n",
      "WARNING:tensorflow:From D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\capslayers.py:346: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\capslayers.py:580: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:74: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=32, activation=\"relu\", units=1024)`\n",
      "  decoder.add(Dense(input_dim=32, activation=\"relu\", output_dim=8 * 8 * 16))\n",
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:77: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(64, (3, 3), strides=(1, 1), padding=\"same\")`\n",
      "  decoder.add(Deconvolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:78: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(32, (3, 3), strides=(2, 2), padding=\"same\")`\n",
      "  decoder.add(Deconvolution2D(32, 3, 3, subsample=(2, 2), border_mode='same'))\n",
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:79: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(16, (3, 3), strides=(2, 2), padding=\"same\")`\n",
      "  decoder.add(Deconvolution2D(16, 3, 3, subsample=(2, 2), border_mode='same'))\n",
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:80: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(8, (3, 3), strides=(2, 2), padding=\"same\")`\n",
      "  decoder.add(Deconvolution2D(8, 3, 3, subsample=(2, 2), border_mode='same'))\n",
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:81: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(3, (3, 3), strides=(1, 1), padding=\"same\")`\n",
      "  decoder.add(Deconvolution2D(3, 3, 3, subsample=(1, 1), border_mode='same'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Capsule Architecture\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 128)  3584        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "convert_to_caps_1 (ConvertToCap (None, 64, 64, 128,  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_1 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      convert_to_caps_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_3 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_4 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_2 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 32, 4 0           conv2d_caps_4[0][0]              \n",
      "                                                                 conv2d_caps_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_5 (Conv2DCaps)      (None, 16, 16, 32, 8 294912      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_7 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_8 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_6 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 32, 8 0           conv2d_caps_8[0][0]              \n",
      "                                                                 conv2d_caps_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_9 (Conv2DCaps)      (None, 8, 8, 32, 8)  589824      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_11 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_12 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_10 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 32, 8)  0           conv2d_caps_12[0][0]             \n",
      "                                                                 conv2d_caps_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_13 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_14 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_15 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_capsule_layer3d_1 (ConvCap (None, 4, 4, 32, 8)  18688       conv2d_caps_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 4, 32, 8)  0           conv2d_caps_15[0][0]             \n",
      "                                                                 conv_capsule_layer3d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_1 (FlattenCaps)    (None, 512, 8)       0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_2 (FlattenCaps)    (None, 2048, 8)      0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2560, 8)      0           flatten_caps_1[0][0]             \n",
      "                                                                 flatten_caps_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "digit_caps (CapsuleLayer)       (None, 10, 32)       6553920     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_cid_1 (Mask_CID)           (None, 32)           0           digit_caps[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (CapsToScalars)         (None, 10)           0           digit_caps[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 64, 64, 3)    67603       mask_cid_1[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 13,427,283\n",
      "Trainable params: 13,426,995\n",
      "Non-trainable params: 288\n",
      "__________________________________________________________________________________________________\n",
      "Capsule Network as feature extractor\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 128)  3584        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "convert_to_caps_1 (ConvertToCap (None, 64, 64, 128,  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_1 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      convert_to_caps_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_3 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_4 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_2 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 32, 4 0           conv2d_caps_4[0][0]              \n",
      "                                                                 conv2d_caps_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_5 (Conv2DCaps)      (None, 16, 16, 32, 8 294912      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_7 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_8 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_6 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 32, 8 0           conv2d_caps_8[0][0]              \n",
      "                                                                 conv2d_caps_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_9 (Conv2DCaps)      (None, 8, 8, 32, 8)  589824      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_11 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_12 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_10 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 32, 8)  0           conv2d_caps_12[0][0]             \n",
      "                                                                 conv2d_caps_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_13 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_14 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_15 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_capsule_layer3d_1 (ConvCap (None, 4, 4, 32, 8)  18688       conv2d_caps_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 4, 32, 8)  0           conv2d_caps_15[0][0]             \n",
      "                                                                 conv_capsule_layer3d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_1 (FlattenCaps)    (None, 512, 8)       0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_2 (FlattenCaps)    (None, 2048, 8)      0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2560, 8)      0           flatten_caps_1[0][0]             \n",
      "                                                                 flatten_caps_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "digit_caps (CapsuleLayer)       (None, 10, 32)       6553920     concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 13,359,680\n",
      "Trainable params: 13,359,424\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_dir = r'..\\Flickr8k\\Flicker8k_Dataset'\n",
    "arch = 'capsnet'\n",
    "encoder_model = initiate_encoder(arch=arch)\n",
    "if not os.path.exists('features_{}.pkl'.format(arch)):\n",
    "    extract_features(encoder_model, img_dir, arch, r'..\\Flickr8k_image_captioning_using_CapsNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filename):\n",
    "    \"\"\"\n",
    "        Description: Generic function to read files and return contents\n",
    "        :filename: Path of the files\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fh:\n",
    "        content = fh.readlines()\n",
    "    return ''.join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean descriptions of the images\n",
    "def map_descriptions(desc_content):\n",
    "    \"\"\"\n",
    "        Description: Map the descriptions <image>:[description_list]\n",
    "        :desc_content: File content\n",
    "    \"\"\"\n",
    "    # Each image contains 5 descriptions in the format\n",
    "    # <image_name>#<1-5> sentence\n",
    "    mapping = dict()\n",
    "    lines = list()\n",
    "    for line in desc_content.split('\\n'):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], ' '.join(tokens[1:])\n",
    "        image_id = image_id.split('.')[0]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        image_desc = image_desc.split()\n",
    "        image_desc = [word.lower() for word in image_desc]\n",
    "        image_desc = [w.translate(table) for w in image_desc]\n",
    "        image_desc = [word for word in image_desc if (len(word)>1 and word.isalpha())]\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # Append the list of the dictionary\n",
    "        clean_desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "        mapping[image_id].append(clean_desc)\n",
    "        lines.append(image_id+' '+clean_desc)\n",
    "    # Write the files to a clean description file\n",
    "    with open('descriptions.txt', 'w') as fh:\n",
    "        fh.writelines('\\n'.join(lines))\n",
    "    return mapping\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Desciptions: 8092 \n",
      "Total Vocabulary: 8765\n"
     ]
    }
   ],
   "source": [
    "filename = r'D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k\\Flickr8k_text\\Flickr8k.token.txt'\n",
    "doc = read_files(filename)\n",
    "descriptions = map_descriptions(doc)\n",
    "print('Total Desciptions: %d ' % len(descriptions))\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Total Vocabulary: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparaing Training Set\n",
    "* The dataset contains multiple files inside Flickr8k_text. The 8000 images are divided into:\n",
    "    * Training Set: 6000\n",
    "    * Validation Set: 1000\n",
    "    * Test Set: 1000\n",
    "* The images names for the training names are stored in the Flickr_8k.trainImages.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training dataset: 6400\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Training dataset: {}\".format(len(set(train_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"\n",
    "    Description: Tokenize the description\n",
    "    \"\"\"\n",
    "    all_desc = list()\n",
    "    for _, desc in descriptions.items():\n",
    "        [all_desc.append(d) for d in desc]\n",
    "    tokenizer = Tokenizer()\n",
    "    max_length = max([len(desc.split()) for desc in all_desc])\n",
    "    tokenizer.fit_on_texts(all_desc)\n",
    "    dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "    return tokenizer, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training descriptions\n",
    "train_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in train_set}\n",
    "# Tokenize the the train description\n",
    "train_tokenizer, max_length = create_tokenizer(train_desc)\n",
    "# Get the features of training dataset\n",
    "feature_path = \"features_{}.pkl\".format(arch)\n",
    "# feature_path = \"features_VGG.pkl\"\n",
    "all_features = load(open(feature_path, 'rb'))\n",
    "train_features = {image_id:feat for image_id, feat in all_features.items() if image_id in train_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7864\n",
      "Maximum Legth: 34\n",
      "loaded photo features: 6400\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(train_tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: {}\\nMaximum Legth: {}\\nloaded photo features: {}'\\\n",
    "      .format(vocab_size, max_length, len(train_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = {image_id:feat for image_id, feat in all_features.items() if image_id in valid_set}\n",
    "valid_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in valid_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(encoder_shape, vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    Description: Define the decoder model\n",
    "    :encoder_shape: Input from the image feature\n",
    "    :vocab_size: \n",
    "    :max_length: maximum length of the description\n",
    "    \"\"\"\n",
    "    inputs1 = Input(shape=(encoder_shape,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 320)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 34, 256)      2013184     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 320)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 34, 256)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          82176       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          525312      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 256)          0           dense_5[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          65792       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 7864)         2021048     dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,707,512\n",
      "Trainable params: 4,707,512\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_op_shape = prod(list(filter(None, encoder_model.layers[-1].output.shape.as_list())))\n",
    "model = define_model(encoder_op_shape, vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    \"\"\"\n",
    "    Description: Create seqences for input <photo>, <description>, <output>\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Merge VGG loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 526s 82ms/step - loss: 4.5899 - val_loss: 4.0731\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.07309, saving model to model-ep001-loss4.605-val_loss4.073_CapsNet.h5\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 522s 82ms/step - loss: 3.8356 - val_loss: 3.9065\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.07309 to 3.90646, saving model to model-ep002-loss3.849-val_loss3.906_CapsNet.h5\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 576s 90ms/step - loss: 3.5732 - val_loss: 3.8604\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.90646 to 3.86041, saving model to model-ep003-loss3.587-val_loss3.860_CapsNet.h5\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 603s 94ms/step - loss: 3.4121 - val_loss: 3.8531\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.86041 to 3.85309, saving model to model-ep004-loss3.426-val_loss3.853_CapsNet.h5\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 600s 94ms/step - loss: 3.2982 - val_loss: 3.8754\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.85309\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 606s 95ms/step - loss: 3.2131 - val_loss: 3.8814\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.85309\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 595s 93ms/step - loss: 3.1427 - val_loss: 3.9087\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.85309\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 491s 77ms/step - loss: 3.0897 - val_loss: 3.9409\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.85309\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 470s 73ms/step - loss: 3.0400 - val_loss: 3.9682\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.85309\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 465s 73ms/step - loss: 3.0038 - val_loss: 3.9923\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.85309\n"
     ]
    }
   ],
   "source": [
    "# Train model for CapsNet\n",
    "epochs = 10\n",
    "train_steps = len(train_desc)\n",
    "val_steps = len(valid_desc)\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}_CapsNet.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    # create the data generator\n",
    "train_generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "valid_generator = data_generator(valid_desc, valid_features, train_tokenizer, max_length, vocab_size)\n",
    "    # fit for one epoch\n",
    "history = model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, verbose=1, validation_data=valid_generator,\\\n",
    "                    validation_steps=val_steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Merge CapsNet loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statement to instantiate the models\n",
    "from utils import model_utils\n",
    "# Import statements for other calculations\n",
    "from pickle import load\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from numpy import array\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_beam_search(model, tokenizer, photo, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Description: This function can be used to create description\n",
    "    :model: The decoder model object\n",
    "    :tokenizer: The tokenizer object used to get the words from predicted indexes\n",
    "    :max_length: The maximum length of the sentence to be generated\n",
    "    :beam_length: Length to check conditional probability. \n",
    "                1: for greedy search\n",
    "                1+: For beam search\n",
    "    \"\"\"\n",
    "    in_text = 'startseq'\n",
    "    beam_list = list()\n",
    "    for i in range(max_length):\n",
    "        if not beam_list:\n",
    "            sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "            yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "            yhat_idx = yhat.argsort()[-beam_length:]\n",
    "            for idx in yhat_idx:\n",
    "                word = tokenizer.index_word[idx]\n",
    "                in_text += ' ' + word\n",
    "                beam_list.append((in_text, log(yhat[idx])))\n",
    "        else:\n",
    "            combination_list = list()\n",
    "            for elems in beam_list:\n",
    "                if elems[0].endswith('endseq'):\n",
    "                    combination_list.append(elems)\n",
    "                    continue\n",
    "                in_text = elems[0]\n",
    "                sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "                sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "                yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "                yhat_idx = yhat.argsort()[-beam_length:]\n",
    "                for idx in yhat_idx:\n",
    "                    word = tokenizer.index_word[idx]\n",
    "                    if word is None:\n",
    "                        continue\n",
    "                    in_text += ' ' + word\n",
    "                    combination_list.append((in_text, elems[1]*log(yhat[idx])))\n",
    "            probs = array([combinations[1] for combinations in combination_list])\n",
    "            top_idx = probs.argsort()[-beam_length:]\n",
    "            for i, idx in enumerate(top_idx):\n",
    "                beam_list[i] = combination_list[idx]\n",
    "    probs = array([prob[1] for prob in beam_list])\n",
    "    top_idx = argmax(probs)        \n",
    "    return beam_list[top_idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BLEU(model, test_desc, photo_feature, tokenizer, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Decription: This function can be used to evaluate BLEU score of the model word by word\n",
    "    :model: The Decoder model\n",
    "    :test_desc: test description\n",
    "    :photo_feature: Extracted features of photos\n",
    "    :tokenizer: Tokenizer object\n",
    "    :max_length: Maximum length of the expected sentence\n",
    "    :beam_length: Beam Length for beam search\n",
    "    \"\"\"\n",
    "    actual, predicted = list(), list()\n",
    "    count = 0\n",
    "    for key, desc_list in test_desc.items():\n",
    "        yhat = generate_desc_beam_search(model, tokenizer, photo_feature[key], max_length, beam_length)\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "features = 'features_vgg.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "test_features = {image_id:feat for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.534923\n",
      "BLEU-2: 0.286939\n",
      "BLEU-3: 0.196144\n",
      "BLEU-4: 0.091162\n"
     ]
    }
   ],
   "source": [
    "### Get the BLEU score VGG extracted features with beam length 1\n",
    "vgg_decoder_path = \"model-ep006-loss3.382-val_loss3.825_VGG.h5\"\n",
    "test_model = load_model(vgg_decoder_path)\n",
    "beam_length = 1\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.351184\n",
      "BLEU-2: 0.127337\n",
      "BLEU-3: 0.056491\n",
      "BLEU-4: 0.014168\n"
     ]
    }
   ],
   "source": [
    "# Beam length 2\n",
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by Capsule Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.535765\n",
      "BLEU-2: 0.267472\n",
      "BLEU-3: 0.177909\n",
      "BLEU-4: 0.087625\n"
     ]
    }
   ],
   "source": [
    "features = 'features_capsnet.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "test_features = {image_id:feat for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}\n",
    "capsnet_decoder_path = \"model-ep004-loss3.426-val_loss3.853_CapsNet.h5\"\n",
    "beam_length = 1\n",
    "test_model = load_model(capsnet_decoder_path)\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.335305\n",
      "BLEU-2: 0.106694\n",
      "BLEU-3: 0.040855\n",
      "BLEU-4: 0.006679\n"
     ]
    }
   ],
   "source": [
    "# Beam length 2\n",
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model, arch, image_path):\n",
    "    \"\"\"\n",
    "    Description: Extract features for a given image\n",
    "    :model: The Encoder model\n",
    "    :arch: The arch type\n",
    "    :image_path: Path to the image\n",
    "    \"\"\"\n",
    "    feature = None\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "    try:\n",
    "        image = load_img(image_path, target_size=target_size)\n",
    "    except Exception as e:\n",
    "        print('{} could not be opened. Skipping\\n {}'.format(image_path,e))\n",
    "        return None\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    if arch=='capsnet':\n",
    "        feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "    else:\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'capsnet'\n",
    "encoder_model = initiate_encoder(arch)\n",
    "# Extract features of the image\n",
    "image_path = r'C:\\Users\\jayde\\OneDrive\\Desktop\\247778426_fd59734130.jpg'\n",
    "photo_feature = extract_feature(encoder_model, arch, image_path)\n",
    "max_length = 34\n",
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# Load the decoder model\n",
    "decoder_path = r''\n",
    "test_model = load_model(decoder_path)\n",
    "# Beam Search length\n",
    "beam_length = 1\n",
    "print(generate_desc_beam_search(test_model, tokenizer, photo_feature, max_length, beam_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_gpu",
   "language": "python",
   "name": "keras_tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%inline` not found.\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "import os\n",
    "import string\n",
    "from utils import model_utils\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump, load\n",
    "\n",
    "# Decoder model imports\n",
    "import numpy as np\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array, prod\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#plot curve\n",
    "import matplotlib.pyplot as plt\n",
    "%inline matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File split for train, test and validation\n",
    "if not ((os.path.exists('train.pkl')) and (os.path.exists('valid.pkl')) and (os.path.exists('test.pkl'))):\n",
    "    train_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.devImages.txt'\n",
    "    test_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.trainImages.txt'\n",
    "    valid_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.testImages.txt'\n",
    "    paths = []\n",
    "    for path in [train_path, valid_path, test_path]:\n",
    "        with open(path, 'r') as fh:\n",
    "            paths = paths + fh.readlines()\n",
    "    sample_idx = np.random.choice(len(paths), size=int(len(paths)), replace=False)\n",
    "\n",
    "    # Train Set 80% of the data\n",
    "    train_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[:int(len(sample_idx)*.80)]]\n",
    "    valid_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.80):int(len(sample_idx)*.90)]]\n",
    "    test_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.90):]]\n",
    "    dump(train_set, open('train.pkl', 'wb'))\n",
    "    dump(valid_set, open('valid.pkl', 'wb'))\n",
    "    dump(test_set, open('test.pkl', 'wb'))\n",
    "else:\n",
    "    train_set = load(open('train.pkl', 'rb'))\n",
    "    valid_set = load(open('valid.pkl', 'rb')) \n",
    "    test_set = load(open('test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_encoder(arch='capsnet'):\n",
    "    \"\"\"\n",
    "        Description: Initiate the encoder \n",
    "        :arch: 'capsnet' or 'vgg'\n",
    "    \"\"\"\n",
    "    if arch=='capsnet':\n",
    "        encoder_model = model_utils.load_DeepCapsNet(input_shape=(64,64,3), n_class=10, routings=3, \\\n",
    "                        weights=r'weights\\deep_caps_best_weights.h5')\n",
    "    else:\n",
    "        encoder_model = model_utils.load_VGG()\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, directory, arch, path):\n",
    "    \"\"\"\n",
    "        Description: Function to extract features through the model\n",
    "        :model: The model object\n",
    "        :directory: Path of the directory of images\n",
    "        :path: Path to save the file\n",
    "    \"\"\"\n",
    "    features = dict()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    print('Feature extraction started')\n",
    "    for name in os.listdir(directory):\n",
    "        image_path = directory + '/' + name\n",
    "        target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "        try:\n",
    "            image = load_img(image_path, target_size=target_size)\n",
    "        except:\n",
    "            print('{} could not be opened. Skipping'.format(image_path))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # Extract the features from the last layer\n",
    "        if arch=='capsnet':\n",
    "            feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "        else:\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        # Populate the dictionary\n",
    "        features[image_id] = feature\n",
    "    path = os.path.join(path, 'features_{}.pkl'.format(arch))\n",
    "    dump(features, open(path, 'wb'))\n",
    "    print('Features extracted and stored at {}'.format(path))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Capsule Architecture\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "VGG16 as feature extractor\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_dir = r'..\\Flickr8k\\Flicker8k_Dataset'\n",
    "arch = 'vgg'\n",
    "encoder_model = initiate_encoder(arch=arch)\n",
    "if not os.path.exists('features_{}.pkl'.format(arch)):\n",
    "    extract_features(encoder_model, img_dir, arch, r'..\\Flickr8k\\Flickr8k_image_captioning_using_CapsNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filename):\n",
    "    \"\"\"\n",
    "        Description: Generic function to read files and return contents\n",
    "        :filename: Path of the files\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fh:\n",
    "        content = fh.readlines()\n",
    "    return ''.join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean descriptions of the images\n",
    "def map_descriptions(desc_content):\n",
    "    \"\"\"\n",
    "        Description: Map the descriptions <image>:[description_list]\n",
    "        :desc_content: File content\n",
    "    \"\"\"\n",
    "    # Each image contains 5 descriptions in the format\n",
    "    # <image_name>#<1-5> sentence\n",
    "    mapping = dict()\n",
    "    lines = list()\n",
    "    for line in desc_content.split('\\n'):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], ' '.join(tokens[1:])\n",
    "        image_id = image_id.split('.')[0]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        image_desc = image_desc.split()\n",
    "        image_desc = [word.lower() for word in image_desc]\n",
    "        image_desc = [w.translate(table) for w in image_desc]\n",
    "        image_desc = [word for word in image_desc if (len(word)>1 and word.isalpha())]\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # Append the list of the dictionary\n",
    "        clean_desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "        mapping[image_id].append(clean_desc)\n",
    "        lines.append(image_id+' '+clean_desc)\n",
    "    # Write the files to a clean description file\n",
    "    with open('descriptions.txt', 'w') as fh:\n",
    "        fh.writelines('\\n'.join(lines))\n",
    "    return mapping\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Desciptions: 8092 \n",
      "Total Vocabulary: 8765\n"
     ]
    }
   ],
   "source": [
    "filename = r'..\\Flickr8k\\Flickr8k_text\\Flickr8k.token.txt'\n",
    "doc = read_files(filename)\n",
    "descriptions = map_descriptions(doc)\n",
    "print('Total Desciptions: %d ' % len(descriptions))\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Total Vocabulary: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparaing Training Set\n",
    "* The dataset contains multiple files inside Flickr8k_text. The 8000 images are divided into:\n",
    "    * Training Set: 6000\n",
    "    * Validation Set: 1000\n",
    "    * Test Set: 1000\n",
    "* The images names for the training names are stored in the Flickr_8k.trainImages.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training dataset: 6400\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Training dataset: {}\".format(len(set(train_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"\n",
    "    Description: Tokenize the description\n",
    "    \"\"\"\n",
    "    all_desc = list()\n",
    "    for _, desc in descriptions.items():\n",
    "        [all_desc.append(d) for d in desc]\n",
    "    tokenizer = Tokenizer()\n",
    "    max_length = max([len(desc.split()) for desc in all_desc])\n",
    "    tokenizer.fit_on_texts(all_desc)\n",
    "    dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "    return tokenizer, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training descriptions\n",
    "train_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in train_set}\n",
    "# Tokenize the the train description\n",
    "train_tokenizer, max_length = create_tokenizer(train_desc)\n",
    "# Get the features of training dataset\n",
    "feature_path = \"features_{}.pkl\".format(arch)\n",
    "# feature_path = \"features_VGG.pkl\"\n",
    "all_features = load(open(feature_path, 'rb'))\n",
    "train_features = {image_id:feat for image_id, feat in all_features.items() if image_id in train_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7864\n",
      "Maximum Legth: 34\n",
      "loaded photo features: 6400\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(train_tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: {}\\nMaximum Legth: {}\\nloaded photo features: {}'\\\n",
    "      .format(vocab_size, max_length, len(train_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = {image_id:feat for image_id, feat in all_features.items() if image_id in valid_set}\n",
    "valid_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in valid_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(encoder_shape, vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    Description: Define the decoder model\n",
    "    :encoder_shape: Input from the image feature\n",
    "    :vocab_size: \n",
    "    :max_length: maximum length of the description\n",
    "    \"\"\"\n",
    "    inputs1 = Input(shape=(encoder_shape,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 34, 256)      2013184     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 4096)         0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 34, 256)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          1048832     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          525312      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 256)          0           dense_8[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          65792       add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 7864)         2021048     dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,674,168\n",
      "Trainable params: 5,674,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_op_shape = prod(list(filter(None, encoder_model.layers[-1].output.shape.as_list())))\n",
    "model = define_model(encoder_op_shape, vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    \"\"\"\n",
    "    Description: Create seqences for input <photo>, <description>, <output>\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 651s 102ms/step - loss: 4.6375 - val_loss: 4.0898\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.08976, saving model to model-ep001-loss4.658-val_loss4.090_VGG.h5\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 636s 99ms/step - loss: 3.8758 - val_loss: 3.8771\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.08976 to 3.87713, saving model to model-ep002-loss3.896-val_loss3.877_VGG.h5\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 642s 100ms/step - loss: 3.6245 - val_loss: 3.8236\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.87713 to 3.82357, saving model to model-ep003-loss3.645-val_loss3.824_VGG.h5\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 649s 101ms/step - loss: 3.4826 - val_loss: 3.7976\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.82357 to 3.79762, saving model to model-ep004-loss3.504-val_loss3.798_VGG.h5\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 538s 84ms/step - loss: 3.3954 - val_loss: 3.8033\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.79762\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 493s 77ms/step - loss: 3.3335 - val_loss: 3.8158\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.79762\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 477s 74ms/step - loss: 3.2911 - val_loss: 3.8371\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.79762\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 483s 75ms/step - loss: 3.2547 - val_loss: 3.8558\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.79762\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 470s 73ms/step - loss: 3.2278 - val_loss: 3.8668\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.79762\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 476s 74ms/step - loss: 3.2080 - val_loss: 3.8771\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.79762\n"
     ]
    }
   ],
   "source": [
    "# Train model for CNN model\n",
    "epochs = 10\n",
    "train_steps = len(train_desc)\n",
    "val_steps = len(valid_desc)\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}_VGG.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    # create the data generator\n",
    "train_generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "valid_generator = data_generator(valid_desc, valid_features, train_tokenizer, max_length, vocab_size)\n",
    "    # fit for one epoch\n",
    "history = model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, verbose=1, validation_data=valid_generator,\\\n",
    "                    validation_steps=val_steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Merge VGG loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 475s 74ms/step - loss: 4.5856 - val_loss: 4.0623\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.06230, saving model to model-ep001-loss4.601-val_loss4.062_CapsNet.h5\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 444s 69ms/step - loss: 3.8331 - val_loss: 3.8919\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.06230 to 3.89191, saving model to model-ep002-loss3.846-val_loss3.892_CapsNet.h5\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 440s 69ms/step - loss: 3.5745 - val_loss: 3.8607\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.89191 to 3.86071, saving model to model-ep003-loss3.588-val_loss3.861_CapsNet.h5\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 437s 68ms/step - loss: 3.4112 - val_loss: 3.8501\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.86071 to 3.85007, saving model to model-ep004-loss3.425-val_loss3.850_CapsNet.h5\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 440s 69ms/step - loss: 3.2949 - val_loss: 3.8580\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.85007\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 441s 69ms/step - loss: 3.2094 - val_loss: 3.8772\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.85007\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 446s 70ms/step - loss: 3.1413 - val_loss: 3.9017\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.85007\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 444s 69ms/step - loss: 3.0871 - val_loss: 3.9235\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.85007\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 447s 70ms/step - loss: 3.0418 - val_loss: 3.9440\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.85007\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 447s 70ms/step - loss: 3.0032 - val_loss: 3.9693\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.85007\n"
     ]
    }
   ],
   "source": [
    "# Train model for CapsNet\n",
    "epochs = 10\n",
    "train_steps = len(train_desc)\n",
    "val_steps = len(valid_desc)\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}_CapsNet.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    # create the data generator\n",
    "train_generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "valid_generator = data_generator(valid_desc, valid_features, train_tokenizer, max_length, vocab_size)\n",
    "    # fit for one epoch\n",
    "history = model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, verbose=1, validation_data=valid_generator,\\\n",
    "                    validation_steps=val_steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Merge CapsNet loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statement to instantiate the models\n",
    "from utils import model_utils\n",
    "# Import statements for other calculations\n",
    "from pickle import load\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from numpy import array\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_beam_search(model, tokenizer, photo, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Description: This function can be used to create description\n",
    "    :model: The decoder model object\n",
    "    :tokenizer: The tokenizer object used to get the words from predicted indexes\n",
    "    :max_length: The maximum length of the sentence to be generated\n",
    "    :beam_length: Length to check conditional probability. \n",
    "                1: for greedy search\n",
    "                1+: For beam search\n",
    "    \"\"\"\n",
    "    in_text = 'startseq'\n",
    "    beam_list = list()\n",
    "    for i in range(max_length):\n",
    "        if not beam_list:\n",
    "            sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "            yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "            yhat_idx = yhat.argsort()[-beam_length:]\n",
    "            for idx in yhat_idx:\n",
    "                word = tokenizer.index_word[idx]\n",
    "                in_text += ' ' + word\n",
    "                beam_list.append((in_text, log(yhat[idx])))\n",
    "        else:\n",
    "            combination_list = list()\n",
    "            for elems in beam_list:\n",
    "                if elems[0].endswith('endseq'):\n",
    "                    combination_list.append(elems)\n",
    "                    continue\n",
    "                in_text = elems[0]\n",
    "                sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "                sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "                yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "                yhat_idx = yhat.argsort()[-beam_length:]\n",
    "                for idx in yhat_idx:\n",
    "                    word = tokenizer.index_word[idx]\n",
    "                    if word is None:\n",
    "                        continue\n",
    "                    in_text += ' ' + word\n",
    "                    combination_list.append((in_text, elems[1]*log(yhat[idx])))\n",
    "            probs = array([combinations[1] for combinations in combination_list])\n",
    "            top_idx = probs.argsort()[-beam_length:]\n",
    "            for i, idx in enumerate(top_idx):\n",
    "                beam_list[i] = combination_list[idx]\n",
    "    probs = array([prob[1] for prob in beam_list])\n",
    "    top_idx = argmax(probs)        \n",
    "    return beam_list[top_idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BLEU(model, test_desc, photo_feature, tokenizer, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Decription: This function can be used to evaluate BLEU score of the model word by word\n",
    "    :model: The Decoder model\n",
    "    :test_desc: test description\n",
    "    :photo_feature: Extracted features of photos\n",
    "    :tokenizer: Tokenizer object\n",
    "    :max_length: Maximum length of the expected sentence\n",
    "    :beam_length: Beam Length for beam search\n",
    "    \"\"\"\n",
    "    actual, predicted = list(), list()\n",
    "    count = 0\n",
    "    for key, desc_list in test_desc.items():\n",
    "        yhat = generate_desc_beam_search(model, tokenizer, photo_feature[key], max_length, beam_length)\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "features = 'features_vgg.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "test_features = {image_id:feat for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.540652\n",
      "BLEU-2: 0.297865\n",
      "BLEU-3: 0.203725\n",
      "BLEU-4: 0.094188\n"
     ]
    }
   ],
   "source": [
    "### Get the BLEU score VGG extracted features with beam length 1\n",
    "vgg_decoder_path = \"model-ep004-loss3.504-val_loss3.798_VGG.h5\"\n",
    "test_model = load_model(vgg_decoder_path)\n",
    "beam_length = 1\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.269442\n",
      "BLEU-2: 0.097745\n",
      "BLEU-3: 0.041995\n",
      "BLEU-4: 0.009466\n"
     ]
    }
   ],
   "source": [
    "# Beam length 2\n",
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by Capsule Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.496194\n",
      "BLEU-2: 0.236004\n",
      "BLEU-3: 0.156967\n",
      "BLEU-4: 0.073708\n"
     ]
    }
   ],
   "source": [
    "features = 'features_capsnet.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "test_features = {image_id:feat for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}\n",
    "capsnet_decoder_path = \"model-ep004-loss3.425-val_loss3.850_CapsNet.h5\"\n",
    "beam_length = 1\n",
    "test_model = load_model(capsnet_decoder_path)\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.328121\n",
      "BLEU-2: 0.106384\n",
      "BLEU-3: 0.033642\n",
      "BLEU-4: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Beam length 2\n",
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model, arch, image_path):\n",
    "    \"\"\"\n",
    "    Description: Extract features for a given image\n",
    "    :model: The Encoder model\n",
    "    :arch: The arch type\n",
    "    :image_path: Path to the image\n",
    "    \"\"\"\n",
    "    feature = None\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "    try:\n",
    "        image = load_img(image_path, target_size=target_size)\n",
    "    except Exception as e:\n",
    "        print('{} could not be opened. Skipping\\n {}'.format(image_path,e))\n",
    "        return None\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    if arch=='capsnet':\n",
    "        feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "    else:\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Capsule Architecture\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "VGG16 as feature extractor\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "startseq brown dog is running through the grass endseq\n"
     ]
    }
   ],
   "source": [
    "arch = 'vgg'\n",
    "encoder_model = initiate_encoder(arch)\n",
    "# Extract features of the image\n",
    "image_path = r'D:\\CapsuleNetwork_ImageCaptioning\\Test_image\\2567035103_3511020c8f.jpg'\n",
    "photo_feature = extract_feature(encoder_model, arch, image_path)\n",
    "max_length = 34\n",
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# Load the decoder model\n",
    "decoder_path = r\"model-ep004-loss3.504-val_loss3.798_VGG.h5\"\n",
    "test_model = load_model(decoder_path)\n",
    "# Beam Search length\n",
    "beam_length = 1\n",
    "print(generate_desc_beam_search(test_model, tokenizer, photo_feature, max_length, beam_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_gpu",
   "language": "python",
   "name": "keras_tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

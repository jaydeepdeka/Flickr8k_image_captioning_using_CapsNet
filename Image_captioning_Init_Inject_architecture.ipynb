{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "UsageError: Line magic function `%inline` not found.\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "import os\n",
    "import string\n",
    "from utils import model_utils\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump, load\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Flatten\n",
    "# Decoder model imports\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array, prod\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#plot curve\n",
    "import matplotlib.pyplot as plt\n",
    "%inline matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File split for train, test and validation\n",
    "if not ((os.path.exists('train.pkl')) and (os.path.exists('valid.pkl')) and (os.path.exists('test.pkl'))):\n",
    "    train_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.devImages.txt'\n",
    "    test_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.trainImages.txt'\n",
    "    valid_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.testImages.txt'\n",
    "    paths = []\n",
    "    for path in [train_path, valid_path, test_path]:\n",
    "        with open(path, 'r') as fh:\n",
    "            paths = paths + fh.readlines()\n",
    "    sample_idx = np.random.choice(len(paths), size=int(len(paths)), replace=False)\n",
    "\n",
    "    # Train Set 80% of the data\n",
    "    train_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[:int(len(sample_idx)*.80)]]\n",
    "    valid_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.80):int(len(sample_idx)*.90)]]\n",
    "    test_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.90):]]\n",
    "    dump(train_set, open('train.pkl', 'wb'))\n",
    "    dump(valid_set, open('valid.pkl', 'wb'))\n",
    "    dump(test_set, open('test.pkl', 'wb'))\n",
    "else:\n",
    "    train_set = load(open('train.pkl', 'rb'))\n",
    "    valid_set = load(open('valid.pkl', 'rb')) \n",
    "    test_set = load(open('test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_encoder(arch='capsnet'):\n",
    "    \"\"\"\n",
    "        Description: Initiate the encoder \n",
    "        :arch: 'capsnet' or 'vgg'\n",
    "    \"\"\"\n",
    "    if arch=='capsnet':\n",
    "        encoder_model = model_utils.load_DeepCapsNet(input_shape=(64,64,3), n_class=10, routings=3, \\\n",
    "                        weights=r'weights\\deep_caps_best_weights.h5')\n",
    "    else:\n",
    "        encoder_model = model_utils.load_VGG()\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, directory, arch, path):\n",
    "    \"\"\"\n",
    "        Description: Function to extract features through the model\n",
    "        :model: The model object\n",
    "        :directory: Path of the directory of images\n",
    "        :path: Path to save the file\n",
    "    \"\"\"\n",
    "    features = dict()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    print('Feature extraction started')\n",
    "    for name in os.listdir(directory):\n",
    "        image_path = directory + '/' + name\n",
    "        target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "        try:\n",
    "            image = load_img(image_path, target_size=target_size)\n",
    "        except:\n",
    "            print('{} could not be opened. Skipping'.format(image_path))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # Extract the features from the last layer\n",
    "        if arch=='capsnet':\n",
    "            feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "        else:\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        # Populate the dictionary\n",
    "        features[image_id] = feature\n",
    "    path = os.path.join(path, 'features_{}.pkl'.format(arch))\n",
    "    dump(features, open(path, 'wb'))\n",
    "    print('Features extracted and stored at {}'.format(path))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"conv_capsule_layer3d_1/stack:0\", shape=(5,), dtype=int32)\n",
      "WARNING:tensorflow:From D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\capslayers.py:346: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\capslayers.py:580: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:74: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=32, activation=\"relu\", units=1024)`\n",
      "  decoder.add(Dense(input_dim=32, activation=\"relu\", output_dim=8 * 8 * 16))\n",
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:77: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(64, (3, 3), strides=(1, 1), padding=\"same\")`\n",
      "  decoder.add(Deconvolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:78: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(32, (3, 3), strides=(2, 2), padding=\"same\")`\n",
      "  decoder.add(Deconvolution2D(32, 3, 3, subsample=(2, 2), border_mode='same'))\n",
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:79: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(16, (3, 3), strides=(2, 2), padding=\"same\")`\n",
      "  decoder.add(Deconvolution2D(16, 3, 3, subsample=(2, 2), border_mode='same'))\n",
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:80: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(8, (3, 3), strides=(2, 2), padding=\"same\")`\n",
      "  decoder.add(Deconvolution2D(8, 3, 3, subsample=(2, 2), border_mode='same'))\n",
      "D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\utils\\model_utils.py:81: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(3, (3, 3), strides=(1, 1), padding=\"same\")`\n",
      "  decoder.add(Deconvolution2D(3, 3, 3, subsample=(1, 1), border_mode='same'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Capsule Architecture\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 128)  3584        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "convert_to_caps_1 (ConvertToCap (None, 64, 64, 128,  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_1 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      convert_to_caps_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_3 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_4 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_2 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 32, 4 0           conv2d_caps_4[0][0]              \n",
      "                                                                 conv2d_caps_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_5 (Conv2DCaps)      (None, 16, 16, 32, 8 294912      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_7 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_8 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_6 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 32, 8 0           conv2d_caps_8[0][0]              \n",
      "                                                                 conv2d_caps_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_9 (Conv2DCaps)      (None, 8, 8, 32, 8)  589824      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_11 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_12 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_10 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 32, 8)  0           conv2d_caps_12[0][0]             \n",
      "                                                                 conv2d_caps_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_13 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_14 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_15 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_capsule_layer3d_1 (ConvCap (None, 4, 4, 32, 8)  18688       conv2d_caps_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 4, 4, 32, 8)  0           conv2d_caps_15[0][0]             \n",
      "                                                                 conv_capsule_layer3d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_1 (FlattenCaps)    (None, 512, 8)       0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_2 (FlattenCaps)    (None, 2048, 8)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2560, 8)      0           flatten_caps_1[0][0]             \n",
      "                                                                 flatten_caps_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "digit_caps (CapsuleLayer)       (None, 10, 32)       6553920     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_cid_1 (Mask_CID)           (None, 32)           0           digit_caps[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (CapsToScalars)         (None, 10)           0           digit_caps[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 64, 64, 3)    67603       mask_cid_1[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 13,427,283\n",
      "Trainable params: 13,426,995\n",
      "Non-trainable params: 288\n",
      "__________________________________________________________________________________________________\n",
      "Capsule Network as feature extractor\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 128)  3584        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "convert_to_caps_1 (ConvertToCap (None, 64, 64, 128,  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_1 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      convert_to_caps_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_3 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_4 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_2 (Conv2DCaps)      (None, 32, 32, 32, 4 147456      conv2d_caps_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 32, 4 0           conv2d_caps_4[0][0]              \n",
      "                                                                 conv2d_caps_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_5 (Conv2DCaps)      (None, 16, 16, 32, 8 294912      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_7 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_8 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_6 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 32, 8 0           conv2d_caps_8[0][0]              \n",
      "                                                                 conv2d_caps_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_9 (Conv2DCaps)      (None, 8, 8, 32, 8)  589824      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_11 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_12 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_10 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 32, 8)  0           conv2d_caps_12[0][0]             \n",
      "                                                                 conv2d_caps_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_13 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_14 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_15 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_capsule_layer3d_1 (ConvCap (None, 4, 4, 32, 8)  18688       conv2d_caps_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 4, 4, 32, 8)  0           conv2d_caps_15[0][0]             \n",
      "                                                                 conv_capsule_layer3d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_1 (FlattenCaps)    (None, 512, 8)       0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_2 (FlattenCaps)    (None, 2048, 8)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2560, 8)      0           flatten_caps_1[0][0]             \n",
      "                                                                 flatten_caps_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "digit_caps (CapsuleLayer)       (None, 10, 32)       6553920     concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 13,359,680\n",
      "Trainable params: 13,359,424\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_dir = r'..\\Flickr8k\\Flicker8k_Dataset'\n",
    "arch = 'capsnet'\n",
    "encoder_model = initiate_encoder(arch=arch)\n",
    "if not os.path.exists('features_{}.pkl'.format(arch)):\n",
    "    extract_features(encoder_model, img_dir, arch, r'..\\Flickr8k_image_captioning_using_CapsNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filename):\n",
    "    \"\"\"\n",
    "        Description: Generic function to read files and return contents\n",
    "        :filename: Path of the files\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fh:\n",
    "        content = fh.readlines()\n",
    "    return ''.join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean descriptions of the images\n",
    "def map_descriptions(desc_content):\n",
    "    \"\"\"\n",
    "        Description: Map the descriptions <image>:[description_list]\n",
    "        :desc_content: File content\n",
    "    \"\"\"\n",
    "    # Each image contains 5 descriptions in the format\n",
    "    # <image_name>#<1-5> sentence\n",
    "    mapping = dict()\n",
    "    lines = list()\n",
    "    for line in desc_content.split('\\n'):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], ' '.join(tokens[1:])\n",
    "        image_id = image_id.split('.')[0]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        image_desc = image_desc.split()\n",
    "        image_desc = [word.lower() for word in image_desc]\n",
    "        image_desc = [w.translate(table) for w in image_desc]\n",
    "        image_desc = [word for word in image_desc if (len(word)>1 and word.isalpha())]\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # Append the list of the dictionary\n",
    "        clean_desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "        mapping[image_id].append(clean_desc)\n",
    "        lines.append(image_id+' '+clean_desc)\n",
    "    # Write the files to a clean description file\n",
    "    with open('descriptions.txt', 'w') as fh:\n",
    "        fh.writelines('\\n'.join(lines))\n",
    "    return mapping\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    \"\"\"\n",
    "    Description: Build the vocabulary from the complete set\n",
    "    :descriptions: Get the mappings of the dataset\n",
    "    \"\"\"\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Desciptions: 8092 \n",
      "Total Vocabulary: 8765\n"
     ]
    }
   ],
   "source": [
    "filename = r'..\\Flickr8k\\Flickr8k_text\\Flickr8k.token.txt'\n",
    "doc = read_files(filename)\n",
    "descriptions = map_descriptions(doc)\n",
    "print('Total Desciptions: %d ' % len(descriptions))\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Total Vocabulary: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparaing Training Set\n",
    "* The dataset contains multiple files inside Flickr8k_text. The 8000 images are divided into:\n",
    "    * Training Set: 6400\n",
    "    * Validation Set: 800\n",
    "    * Test Set: 800\n",
    "* The images names for the training names are stored in the Flickr_8k.trainImages.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training dataset: 6400\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Training dataset: {}\".format(len(set(train_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"\n",
    "    Description: Tokenize the description\n",
    "    \"\"\"\n",
    "    all_desc = list()\n",
    "    for _, desc in descriptions.items():\n",
    "        [all_desc.append(d) for d in desc]\n",
    "    tokenizer = Tokenizer()\n",
    "    max_length = max([len(desc.split()) for desc in all_desc])\n",
    "    tokenizer.fit_on_texts(all_desc)\n",
    "    dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "    return tokenizer, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training descriptions\n",
    "train_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in train_set}\n",
    "# Tokenize the the train description\n",
    "train_tokenizer, max_length = create_tokenizer(train_desc)\n",
    "# Get the features of training dataset\n",
    "feature_path = \"features_{}.pkl\".format(arch)\n",
    "# feature_path = \"features_VGG.pkl\"\n",
    "all_features = load(open(feature_path, 'rb'))\n",
    "train_features = {image_id:feat for image_id, feat in all_features.items() if image_id in train_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7864\n",
      "Maximum Legth: 34\n",
      "loaded photo features: 6400\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(train_tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: {}\\nMaximum Legth: {}\\nloaded photo features: {}'\\\n",
    "      .format(vocab_size, max_length, len(train_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = {image_id:feat for image_id, feat in all_features.items() if image_id in valid_set}\n",
    "val_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in valid_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(encoder_shape, vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    Description: Define the decoder model\n",
    "    :encoder_shape: Input from the image feature\n",
    "    :vocab_size: \n",
    "    :max_length: maximum length of the description\n",
    "    \"\"\"\n",
    "    # Image Encoder\n",
    "    inputs1 = Input(shape=(None, encoder_shape))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    encoder, state_h, state_c = LSTM(256, return_state=True)(fe2)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    # Text encoder\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    # SeqToSeq Word Model\n",
    "    decoder1,_,_ = LSTM(256, return_sequences=True, return_state=True)(se2, initial_state=encoder_states)\n",
    "    nonmasking = model_utils.NonMasking()(decoder1)\n",
    "    flatten1 = Flatten()(nonmasking)\n",
    "    decoder2 = Dense(256, activation='relu')(flatten1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None, 320)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 320)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      2013184     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 256)    82176       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 525312      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 34, 256), (N 525312      dropout_2[0][0]                  \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "non_masking_1 (NonMasking)      (None, 34, 256)      0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8704)         0           non_masking_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          2228480     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 7864)         2021048     dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,395,512\n",
      "Trainable params: 7,395,512\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_op_shape = prod(list(filter(None, encoder_model.layers[-1].output.shape.as_list())))\n",
    "model = define_model(encoder_op_shape, vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    \"\"\"\n",
    "    Description: Create seqences for input <photo>, <description>, <output>\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(np.expand_dims(photo, axis=0))\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 518s 81ms/step - loss: 4.7653 - val_loss: 4.3339\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.33389, saving model to model-ep001-loss4.784-val_loss4.334_VGG_init.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 513s 80ms/step - loss: 4.1987 - val_loss: 4.1605\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.33389 to 4.16054, saving model to model-ep002-loss4.215-val_loss4.161_VGG_init.h5\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 501s 78ms/step - loss: 4.0262 - val_loss: 4.1069\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.16054 to 4.10695, saving model to model-ep003-loss4.042-val_loss4.107_VGG_init.h5\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 515s 81ms/step - loss: 3.9309 - val_loss: 4.1117\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.10695\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 521s 81ms/step - loss: 3.8769 - val_loss: 4.1049\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.10695 to 4.10493, saving model to model-ep005-loss3.892-val_loss4.105_VGG_init.h5\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 518s 81ms/step - loss: 3.8433 - val_loss: 4.1354\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.10493\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 512s 80ms/step - loss: 3.8218 - val_loss: 4.1724\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.10493\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 519s 81ms/step - loss: 3.8086 - val_loss: 4.1572\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.10493\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 514s 80ms/step - loss: 3.8022 - val_loss: 4.1622\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4.10493\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 595s 93ms/step - loss: 3.7997 - val_loss: 4.1794\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.10493\n"
     ]
    }
   ],
   "source": [
    "# Train model for VGG\n",
    "epochs = 10\n",
    "train_steps = len(train_desc)\n",
    "val_steps = len(val_desc)\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}_VGG_init.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    # create the data generator\n",
    "train_generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "valid_generator = data_generator(val_desc, val_features, train_tokenizer, max_length, vocab_size)\n",
    "# fit for one epoch\n",
    "history = model.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps, verbose=1, validation_data=valid_generator,\\\n",
    "                        validation_steps=val_steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Init-inject VGG loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 568s 89ms/step - loss: 4.6719 - val_loss: 4.1331\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.13309, saving model to model-ep001-loss4.689-val_loss4.133_CapsNet_init.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 496s 77ms/step - loss: 3.9170 - val_loss: 3.9725\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.13309 to 3.97248, saving model to model-ep002-loss3.931-val_loss3.972_CapsNet_init.h5\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 490s 77ms/step - loss: 3.6423 - val_loss: 3.9565\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.97248 to 3.95653, saving model to model-ep003-loss3.656-val_loss3.957_CapsNet_init.h5\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 490s 77ms/step - loss: 3.4611 - val_loss: 3.9775\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.95653\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 491s 77ms/step - loss: 3.3290 - val_loss: 4.0468\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.95653\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 488s 76ms/step - loss: 3.2257 - val_loss: 4.1093\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.95653\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 496s 77ms/step - loss: 3.1391 - val_loss: 4.1602\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.95653\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 489s 76ms/step - loss: 3.0734 - val_loss: 4.2376\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.95653\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 514s 80ms/step - loss: 3.0134 - val_loss: 4.3295\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.95653\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 677s 106ms/step - loss: 2.9654 - val_loss: 4.3883\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.95653\n"
     ]
    }
   ],
   "source": [
    "# Train model for CapsNet\n",
    "epochs = 10\n",
    "train_steps = len(train_desc)\n",
    "val_steps = len(val_desc)\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}_CapsNet_init.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "train_generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "valid_generator = data_generator(val_desc, val_features, train_tokenizer, max_length, vocab_size)\n",
    "# fit for one epoch\n",
    "history = model.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps, verbose=1, validation_data=valid_generator,\\\n",
    "                        validation_steps=val_steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Init-inject CapsNet loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statement to instantiate the models\n",
    "from utils import model_utils\n",
    "# Import statements for other calculations\n",
    "from pickle import load\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from numpy import array\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_beam_search(model, tokenizer, photo, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Description: This function can be used to create description\n",
    "    :model: The decoder model object\n",
    "    :tokenizer: The tokenizer object used to get the words from predicted indexes\n",
    "    :max_length: The maximum length of the sentence to be generated\n",
    "    :beam_length: Length to check conditional probability. \n",
    "                1: for greedy search\n",
    "                1+: For beam search\n",
    "    \"\"\"\n",
    "    in_text = 'startseq'\n",
    "    beam_list = list()\n",
    "    for i in range(max_length):\n",
    "        if not beam_list:\n",
    "            sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "            yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "            yhat_idx = yhat.argsort()[-beam_length:]\n",
    "            for idx in yhat_idx:\n",
    "                word = tokenizer.index_word[idx]\n",
    "                in_text += ' ' + word\n",
    "                beam_list.append((in_text, log(yhat[idx])))\n",
    "        else:\n",
    "            combination_list = list()\n",
    "            for elems in beam_list:\n",
    "                if elems[0].endswith('endseq'):\n",
    "                    combination_list.append(elems)\n",
    "                    continue\n",
    "                in_text = elems[0]\n",
    "                sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "                sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "                yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "                yhat_idx = yhat.argsort()[-beam_length:]\n",
    "                for idx in yhat_idx:\n",
    "                    word = tokenizer.index_word[idx]\n",
    "                    if word is None:\n",
    "                        continue\n",
    "                    in_text += ' ' + word\n",
    "                    combination_list.append((in_text, elems[1]*log(yhat[idx])))\n",
    "            probs = array([combinations[1] for combinations in combination_list])\n",
    "            top_idx = probs.argsort()[-beam_length:]\n",
    "            for i, idx in enumerate(top_idx):\n",
    "                beam_list[i] = combination_list[idx]\n",
    "    probs = array([prob[1] for prob in beam_list])\n",
    "    top_idx = argmax(probs)        \n",
    "    return beam_list[top_idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BLEU(model, test_desc, photo_feature, tokenizer, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Decription: This function can be used to evaluate BLEU score of the model word by word\n",
    "    :model: The Decoder model\n",
    "    :test_desc: test description\n",
    "    :photo_feature: Extracted features of photos\n",
    "    :tokenizer: Tokenizer object\n",
    "    :max_length: Maximum length of the expected sentence\n",
    "    :beam_length: Beam Length for beam search\n",
    "    \"\"\"\n",
    "    actual, predicted = list(), list()\n",
    "    count = 0\n",
    "    for key, desc_list in test_desc.items():\n",
    "        yhat = generate_desc_beam_search(model, tokenizer, photo_feature[key], max_length, beam_length)\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "features = 'features_vgg.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "test_features = {image_id: np.expand_dims(feat, axis=0) for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, None, 4096)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 4096)   0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 34, 256)      2013184     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 256)    1048832     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 34, 256)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 256), (None, 525312      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 34, 256), (N 525312      dropout_4[0][0]                  \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "non_masking_2 (NonMasking)      (None, 34, 256)      0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8704)         0           non_masking_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          2228480     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 7864)         2021048     dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,362,168\n",
      "Trainable params: 8,362,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "BLEU-1: 0.560408\n",
      "BLEU-2: 0.281611\n",
      "BLEU-3: 0.182150\n",
      "BLEU-4: 0.080150\n"
     ]
    }
   ],
   "source": [
    "### Get the BLEU score VGG extracted features with beam length 1\n",
    "vgg_decoder_path = \"model-ep005-loss3.892-val_loss4.105_VGG_init.h5\"\n",
    "test_model = define_model(encoder_op_shape, vocab_size, max_length)\n",
    "test_model.load_weights(vgg_decoder_path)\n",
    "beam_length = 1\n",
    "get_BLEU(model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.403167\n",
      "BLEU-2: 0.137777\n",
      "BLEU-3: 0.073039\n",
      "BLEU-4: 0.018981\n"
     ]
    }
   ],
   "source": [
    "# Beam length 2\n",
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by Capsule Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "features = 'features_capsnet.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "test_features = {image_id: np.expand_dims(feat, axis=0) for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, None, 320)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 320)    0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 34, 256)      2013184     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 256)    82176       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 34, 256)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 256), (None, 525312      dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 34, 256), (N 525312      dropout_4[0][0]                  \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "non_masking_2 (NonMasking)      (None, 34, 256)      0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8704)         0           non_masking_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          2228480     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 7864)         2021048     dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,395,512\n",
      "Trainable params: 7,395,512\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "BLEU-1: 0.505907\n",
      "BLEU-2: 0.213543\n",
      "BLEU-3: 0.128443\n",
      "BLEU-4: 0.047210\n"
     ]
    }
   ],
   "source": [
    "capsnet_decoder_path = \"model-ep003-loss3.656-val_loss3.957_CapsNet_init.h5\"\n",
    "beam_length = 1\n",
    "test_model = define_model(encoder_op_shape, vocab_size, max_length)\n",
    "test_model.load_weights(capsnet_decoder_path)\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.455746\n",
      "BLEU-2: 0.135960\n",
      "BLEU-3: 0.065984\n",
      "BLEU-4: 0.013652\n"
     ]
    }
   ],
   "source": [
    "# Beam length 2\n",
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model, arch, image_path):\n",
    "    \"\"\"\n",
    "    Description: Extract features for a given image\n",
    "    :model: The Encoder model\n",
    "    :arch: The arch type\n",
    "    :image_path: Path to the image\n",
    "    \"\"\"\n",
    "    feature = None\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "    try:\n",
    "        image = load_img(image_path, target_size=target_size)\n",
    "    except Exception as e:\n",
    "        print('{} could not be opened. Skipping\\n {}'.format(image_path,e))\n",
    "        return None\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    if arch=='capsnet':\n",
    "        feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "    else:\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv_capsule_layer3d_4/stack:0\", shape=(5,), dtype=int32)\n",
      "Complete Capsule Architecture\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 128)  3584        input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "convert_to_caps_4 (ConvertToCap (None, 64, 64, 128,  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_46 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      convert_to_caps_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_48 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_49 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_47 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 32, 32, 4 0           conv2d_caps_49[0][0]             \n",
      "                                                                 conv2d_caps_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_50 (Conv2DCaps)     (None, 16, 16, 32, 8 294912      add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_52 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_53 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_51 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 16, 32, 8 0           conv2d_caps_53[0][0]             \n",
      "                                                                 conv2d_caps_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_54 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_56 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_57 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_55 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 32, 8)  0           conv2d_caps_57[0][0]             \n",
      "                                                                 conv2d_caps_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_58 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_59 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_60 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_capsule_layer3d_4 (ConvCap (None, 4, 4, 32, 8)  18688       conv2d_caps_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 32, 8)  0           conv2d_caps_60[0][0]             \n",
      "                                                                 conv_capsule_layer3d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_7 (FlattenCaps)    (None, 512, 8)       0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_8 (FlattenCaps)    (None, 2048, 8)      0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 2560, 8)      0           flatten_caps_7[0][0]             \n",
      "                                                                 flatten_caps_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "digit_caps (CapsuleLayer)       (None, 10, 32)       6553920     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_cid_7 (Mask_CID)           (None, 32)           0           digit_caps[0][0]                 \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (CapsToScalars)         (None, 10)           0           digit_caps[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 64, 64, 3)    67603       mask_cid_7[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 13,427,283\n",
      "Trainable params: 13,426,995\n",
      "Non-trainable params: 288\n",
      "__________________________________________________________________________________________________\n",
      "Capsule Network as feature extractor\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 128)  3584        input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "convert_to_caps_4 (ConvertToCap (None, 64, 64, 128,  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_46 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      convert_to_caps_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_48 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_49 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_47 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 32, 32, 4 0           conv2d_caps_49[0][0]             \n",
      "                                                                 conv2d_caps_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_50 (Conv2DCaps)     (None, 16, 16, 32, 8 294912      add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_52 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_53 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_51 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 16, 32, 8 0           conv2d_caps_53[0][0]             \n",
      "                                                                 conv2d_caps_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_54 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_56 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_57 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_55 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 32, 8)  0           conv2d_caps_57[0][0]             \n",
      "                                                                 conv2d_caps_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_58 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_59 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_60 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_capsule_layer3d_4 (ConvCap (None, 4, 4, 32, 8)  18688       conv2d_caps_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 32, 8)  0           conv2d_caps_60[0][0]             \n",
      "                                                                 conv_capsule_layer3d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_7 (FlattenCaps)    (None, 512, 8)       0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_8 (FlattenCaps)    (None, 2048, 8)      0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 2560, 8)      0           flatten_caps_7[0][0]             \n",
      "                                                                 flatten_caps_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "digit_caps (CapsuleLayer)       (None, 10, 32)       6553920     concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 13,359,680\n",
      "Trainable params: 13,359,424\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, None, 320)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, None, 320)    0           input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 34, 256)      2013184     input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, None, 256)    82176       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 34, 256)      0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 256), (None, 525312      dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, 34, 256), (N 525312      dropout_10[0][0]                 \n",
      "                                                                 lstm_9[0][1]                     \n",
      "                                                                 lstm_9[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "non_masking_5 (NonMasking)      (None, 34, 256)      0           lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 8704)         0           non_masking_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 256)          2228480     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 7864)         2021048     dense_18[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,395,512\n",
      "Trainable params: 7,395,512\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "startseq man in red shirt is sitting on the street endseq\n"
     ]
    }
   ],
   "source": [
    "arch = 'capsnet'\n",
    "encoder_model = initiate_encoder(arch)\n",
    "# Extract features of the image\n",
    "image_path = r'D:\\CapsuleNetwork_ImageCaptioning\\Test_image\\2567035103_3511020c8f.jpg'\n",
    "photo_feature = extract_feature(encoder_model, arch, image_path)\n",
    "max_length = 34\n",
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# Load the decoder model\n",
    "decoder_path = r\"model-ep003-loss3.656-val_loss3.957_CapsNet_init.h5\"\n",
    "test_model = define_model(encoder_op_shape, vocab_size, max_length)\n",
    "test_model.load_weights(decoder_path)\n",
    "# Beam Search length\n",
    "beam_length = 1\n",
    "print(generate_desc_beam_search(test_model, tokenizer, np.expand_dims(photo_feature, axis=0), max_length, beam_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_gpu",
   "language": "python",
   "name": "keras_tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

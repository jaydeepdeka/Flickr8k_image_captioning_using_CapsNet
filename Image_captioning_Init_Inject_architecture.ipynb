{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "UsageError: Line magic function `%inline` not found.\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "import os\n",
    "import string\n",
    "from utils import model_utils\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump, load\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Flatten\n",
    "# Decoder model imports\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array, prod\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#plot curve\n",
    "import matplotlib.pyplot as plt\n",
    "%inline matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File split for train, test and validation\n",
    "if not ((os.path.exists('train.pkl')) and (os.path.exists('valid.pkl')) and (os.path.exists('test.pkl'))):\n",
    "    train_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.devImages.txt'\n",
    "    test_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.trainImages.txt'\n",
    "    valid_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.testImages.txt'\n",
    "    paths = []\n",
    "    for path in [train_path, valid_path, test_path]:\n",
    "        with open(path, 'r') as fh:\n",
    "            paths = paths + fh.readlines()\n",
    "    sample_idx = np.random.choice(len(paths), size=int(len(paths)), replace=False)\n",
    "\n",
    "    # Train Set 80% of the data\n",
    "    train_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[:int(len(sample_idx)*.80)]]\n",
    "    valid_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.80):int(len(sample_idx)*.90)]]\n",
    "    test_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.90):]]\n",
    "    dump(train_set, open('train.pkl', 'wb'))\n",
    "    dump(valid_set, open('valid.pkl', 'wb'))\n",
    "    dump(test_set, open('test.pkl', 'wb'))\n",
    "else:\n",
    "    train_set = load(open('train.pkl', 'rb'))\n",
    "    valid_set = load(open('valid.pkl', 'rb')) \n",
    "    test_set = load(open('test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_encoder(arch='capsnet'):\n",
    "    \"\"\"\n",
    "        Description: Initiate the encoder \n",
    "        :arch: 'capsnet' or 'vgg'\n",
    "    \"\"\"\n",
    "    if arch=='capsnet':\n",
    "        encoder_model = model_utils.load_DeepCapsNet(input_shape=(64,64,3), n_class=10, routings=3, \\\n",
    "                        weights=r'weights\\deep_caps_best_weights.h5')\n",
    "    else:\n",
    "        encoder_model = model_utils.load_VGG()\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, directory, arch, path):\n",
    "    \"\"\"\n",
    "        Description: Function to extract features through the model\n",
    "        :model: The model object\n",
    "        :directory: Path of the directory of images\n",
    "        :path: Path to save the file\n",
    "    \"\"\"\n",
    "    features = dict()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    print('Feature extraction started')\n",
    "    for name in os.listdir(directory):\n",
    "        image_path = directory + '/' + name\n",
    "        target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "        try:\n",
    "            image = load_img(image_path, target_size=target_size)\n",
    "        except:\n",
    "            print('{} could not be opened. Skipping'.format(image_path))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # Extract the features from the last layer\n",
    "        if arch=='capsnet':\n",
    "            feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "        else:\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        # Populate the dictionary\n",
    "        features[image_id] = feature\n",
    "    path = os.path.join(path, 'features_{}.pkl'.format(arch))\n",
    "    dump(features, open(path, 'wb'))\n",
    "    print('Features extracted and stored at {}'.format(path))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Complete Capsule Architecture\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "VGG16 as feature extractor\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_dir = r'..\\Flickr8k\\Flicker8k_Dataset'\n",
    "arch = 'vgg'\n",
    "encoder_model = initiate_encoder(arch=arch)\n",
    "if not os.path.exists('features_{}.pkl'.format(arch)):\n",
    "    extract_features(encoder_model, img_dir, arch, r'..\\Flickr8k_image_captioning_using_CapsNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filename):\n",
    "    \"\"\"\n",
    "        Description: Generic function to read files and return contents\n",
    "        :filename: Path of the files\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fh:\n",
    "        content = fh.readlines()\n",
    "    return ''.join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean descriptions of the images\n",
    "def map_descriptions(desc_content):\n",
    "    \"\"\"\n",
    "        Description: Map the descriptions <image>:[description_list]\n",
    "        :desc_content: File content\n",
    "    \"\"\"\n",
    "    # Each image contains 5 descriptions in the format\n",
    "    # <image_name>#<1-5> sentence\n",
    "    mapping = dict()\n",
    "    lines = list()\n",
    "    for line in desc_content.split('\\n'):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], ' '.join(tokens[1:])\n",
    "        image_id = image_id.split('.')[0]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        image_desc = image_desc.split()\n",
    "        image_desc = [word.lower() for word in image_desc]\n",
    "        image_desc = [w.translate(table) for w in image_desc]\n",
    "        image_desc = [word for word in image_desc if (len(word)>1 and word.isalpha())]\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # Append the list of the dictionary\n",
    "        clean_desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "        mapping[image_id].append(clean_desc)\n",
    "        lines.append(image_id+' '+clean_desc)\n",
    "    # Write the files to a clean description file\n",
    "    with open('descriptions.txt', 'w') as fh:\n",
    "        fh.writelines('\\n'.join(lines))\n",
    "    return mapping\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    \"\"\"\n",
    "    Description: Build the vocabulary from the complete set\n",
    "    :descriptions: Get the mappings of the dataset\n",
    "    \"\"\"\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Desciptions: 8092 \n",
      "Total Vocabulary: 8765\n"
     ]
    }
   ],
   "source": [
    "filename = r'..\\Flickr8k\\Flickr8k_text\\Flickr8k.token.txt'\n",
    "doc = read_files(filename)\n",
    "descriptions = map_descriptions(doc)\n",
    "print('Total Desciptions: %d ' % len(descriptions))\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Total Vocabulary: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparaing Training Set\n",
    "* The dataset contains multiple files inside Flickr8k_text. The 8000 images are divided into:\n",
    "    * Training Set: 6400\n",
    "    * Validation Set: 800\n",
    "    * Test Set: 800\n",
    "* The images names for the training names are stored in the Flickr_8k.trainImages.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training dataset: 6400\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Training dataset: {}\".format(len(set(train_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"\n",
    "    Description: Tokenize the description\n",
    "    \"\"\"\n",
    "    all_desc = list()\n",
    "    for _, desc in descriptions.items():\n",
    "        [all_desc.append(d) for d in desc]\n",
    "    tokenizer = Tokenizer()\n",
    "    max_length = max([len(desc.split()) for desc in all_desc])\n",
    "    tokenizer.fit_on_texts(all_desc)\n",
    "    dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "    return tokenizer, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training descriptions\n",
    "train_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in train_set}\n",
    "# Tokenize the the train description\n",
    "train_tokenizer, max_length = create_tokenizer(train_desc)\n",
    "# Get the features of training dataset\n",
    "feature_path = \"features_{}.pkl\".format(arch)\n",
    "# feature_path = \"features_VGG.pkl\"\n",
    "all_features = load(open(feature_path, 'rb'))\n",
    "train_features = {image_id:feat for image_id, feat in all_features.items() if image_id in train_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7864\n",
      "Maximum Legth: 34\n",
      "loaded photo features: 6400\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(train_tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: {}\\nMaximum Legth: {}\\nloaded photo features: {}'\\\n",
    "      .format(vocab_size, max_length, len(train_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = {image_id:feat for image_id, feat in all_features.items() if image_id in valid_set}\n",
    "val_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in valid_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(encoder_shape, vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    Description: Define the decoder model\n",
    "    :encoder_shape: Input from the image feature\n",
    "    :vocab_size: \n",
    "    :max_length: maximum length of the description\n",
    "    \"\"\"\n",
    "    # Image Encoder\n",
    "    inputs1 = Input(shape=(None, encoder_shape))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    encoder, state_h, state_c = LSTM(256, return_state=True)(fe2)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    # Text encoder\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    # SeqToSeq Word Model\n",
    "    decoder1,_,_ = LSTM(256, return_sequences=True, return_state=True)(se2, initial_state=encoder_states)\n",
    "    nonmasking = model_utils.NonMasking()(decoder1)\n",
    "    flatten1 = Flatten()(nonmasking)\n",
    "    decoder2 = Dense(256, activation='relu')(flatten1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None, 320)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 320)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      2013184     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 256)    82176       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 525312      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 34, 256), (N 525312      dropout_2[0][0]                  \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "non_masking_1 (NonMasking)      (None, 34, 256)      0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8704)         0           non_masking_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          2228480     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 7864)         2021048     dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,395,512\n",
      "Trainable params: 7,395,512\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_op_shape = prod(list(filter(None, encoder_model.layers[-1].output.shape.as_list())))\n",
    "model = define_model(encoder_op_shape, vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    \"\"\"\n",
    "    Description: Create seqences for input <photo>, <description>, <output>\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(np.expand_dims(photo, axis=0))\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 512s 80ms/step - loss: 4.7760 - val_loss: 4.3206\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.32059, saving model to model-ep001-loss4.794-val_loss4.321_VGG_init.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_3/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_3/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 555s 87ms/step - loss: 4.2004 - val_loss: 4.1759\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.32059 to 4.17592, saving model to model-ep002-loss4.218-val_loss4.176_VGG_init.h5\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 663s 104ms/step - loss: 4.0318 - val_loss: 4.1611\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.17592 to 4.16110, saving model to model-ep003-loss4.048-val_loss4.161_VGG_init.h5\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 685s 107ms/step - loss: 3.9389 - val_loss: 4.1107\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.16110 to 4.11073, saving model to model-ep004-loss3.955-val_loss4.111_VGG_init.h5\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 690s 108ms/step - loss: 3.8862 - val_loss: 4.1125\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.11073\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 507s 79ms/step - loss: 3.8448 - val_loss: 4.1298\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.11073\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 502s 78ms/step - loss: 3.8202 - val_loss: 4.1083\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.11073 to 4.10834, saving model to model-ep007-loss3.836-val_loss4.108_VGG_init.h5\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 501s 78ms/step - loss: 3.8063 - val_loss: 4.2015\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.10834\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 502s 78ms/step - loss: 3.7969 - val_loss: 4.2062\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4.10834\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 501s 78ms/step - loss: 3.7941 - val_loss: 4.1733\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.10834\n"
     ]
    }
   ],
   "source": [
    "# Train model for VGG\n",
    "epochs = 10\n",
    "train_steps = len(train_desc)\n",
    "val_steps = len(val_desc)\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}_VGG_init.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    # create the data generator\n",
    "train_generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "valid_generator = data_generator(val_desc, val_features, train_tokenizer, max_length, vocab_size)\n",
    "# fit for one epoch\n",
    "history = model.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps, verbose=1, validation_data=valid_generator,\\\n",
    "                        validation_steps=val_steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Init-inject VGG loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 679s 106ms/step - loss: 4.6740 - val_loss: 4.1357\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.13566, saving model to model-ep001-loss4.691-val_loss4.136_CapsNet_init.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 697s 109ms/step - loss: 3.9052 - val_loss: 3.9655\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.13566 to 3.96545, saving model to model-ep002-loss3.920-val_loss3.965_CapsNet_init.h5\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 708s 111ms/step - loss: 3.6292 - val_loss: 3.9363\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.96545 to 3.93628, saving model to model-ep003-loss3.643-val_loss3.936_CapsNet_init.h5\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 612s 96ms/step - loss: 3.4445 - val_loss: 3.9531\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.93628\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 495s 77ms/step - loss: 3.3084 - val_loss: 3.9972\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.93628\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 488s 76ms/step - loss: 3.2029 - val_loss: 4.0510\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.93628\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 495s 77ms/step - loss: 3.1146 - val_loss: 4.1101\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.93628\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 492s 77ms/step - loss: 3.0443 - val_loss: 4.1693\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.93628\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 494s 77ms/step - loss: 2.9840 - val_loss: 4.2277\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.93628\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 497s 78ms/step - loss: 2.9378 - val_loss: 4.2834\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.93628\n"
     ]
    }
   ],
   "source": [
    "# Train model for CapsNet\n",
    "epochs = 10\n",
    "train_steps = len(train_desc)\n",
    "val_steps = len(val_desc)\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}_CapsNet_init.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "train_generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "valid_generator = data_generator(val_desc, val_features, train_tokenizer, max_length, vocab_size)\n",
    "# fit for one epoch\n",
    "history = model.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps, verbose=1, validation_data=valid_generator,\\\n",
    "                        validation_steps=val_steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU5dn/8c+VfQ9JIEAWSBBlDwESRHGHKi5FqmxabW2l1C7WpX3a2s2l7fOzi7a17dMWl1arVRE3qnUDRLSCkMi+yW5CIAkJSwIkZLl+f5wTMomTECCTE5Lr/XrNK8nZ5ppR5jv3fZ9zH1FVjDHGmOaCvC7AGGNM52QBYYwxxi8LCGOMMX5ZQBhjjPHLAsIYY4xfFhDGGGP8soAw7UZE3hCRL5/E9heKyOb23tb4JyL/EJFfeF0HgIgsFpFZXtdhWmcBYVolIjtFZGJbtlXVK1X1SXe/W0TkgxNs/76qDmrjsdu8bWtEREVk4Am26Ssij4vIHhGpEJFNInK/iESf7vO38pz3ubVN81kW4i7LaMP+l4hIYaDq84qIZLjvQYjXtXRHFhDG+BCRRGApEAmcp6qxwOeAHsBZAX76cuABEQkO8PMY0yYWEKbNGloFIvJbEdkvIjtE5Eqf9YtFZJaIDAH+CpwnIpUicqCF4zX51uu2Vr4nImtE5KCIPC8iES1smyIiL4pIqVvHd3zWBYvIj0Rkm9sCyBeRdBFZ4m6y2q1rhp+y7gYqgJtUdSeAqhao6h2qusY9/h9EpEBEDrnHvtDnue8TkXlu7RUi8rGIjPRZ/wMR2e2u2ywiE3ye+03gGHBTC+9XuPvefyoixSLyVxGJdFs2bwAp7uuqFJEUf8dodryvichWESkXkfkN+4jjdyJS4v53WCMiw911V4nIBrf+3SLyvRaOfYuI/FdE/ugeY1Oz1+q7bZCI/EREdrnP+ZSIxLurG/6bHXBf13knel2m/VhAmJN1LrAZ6An8GnhcRMR3A1XdCNwGLFXVGFXtcRLHnw5MAjKBLOCW5huISBDwb2A1kApMAO4UkSvcTe4GbgCuAuKArwJHVPUid/1It67n/Tz/ROAlVa1vpcYVQDaQCPwLeKEhyFzXAi/4rH9FREJFZBDwbSDXbZlcAez02U+BnwL3ikion+f9FXCO+9wD3df+M1U9DFwJFLmvK0ZVi1qpHxG5DPh/OO93X2AX8Jy7+nLgIve5egAzgDJ33ePA1936hwOLWnmac4HtOP+v3Au85LbQmrvFfVwKDABigD+56xr+m/VwX9fS1l6XaV8WEOZk7VLVR1W1DngS58Oldzse/xFVLVLVcpwQyPazTS7QS1UfUNVjqrodeBSY6a6fBfxEVTerY7Wqlvk5jj9JwJ7WNlDVp1W1TFVrVfUhIBzwHR/JV9V5qloDPAxEAOOAOnfboSISqqo7VXVbs2PPB0rd13CcG8JfA+5S1XJVrQD+1+c1n6wvAk+o6seqWg3cg9PiywBqgFhgMCCqulFVG96TGrf+OFXdr6oft/IcJcDvVbXGDePNwNUt1PKwqm5X1Uq3lpk27uA9CwhzsvY2/KKqR9xfY060k4j08+n+qGzL8YEjLRy7P053yoGGB/AjGoMqHdjmZ7+2KMMJvRaJyHdFZKPbdXIAiMf5ltygoOEXtyVSCKSo6lbgTuA+oEREnmuhK+gnwI9xgqVBLyAKyPd5zW+6y09FCk6roaHOSpzXnqqqi3C+wf8ZKBaROSIS5256PU7LbJeIvHeCLp/d2nQ20F3u87Zai/t7CO37xcOcAgsIEyhNpglW1U99uj9OGCgnUADsUNUePo9YVb3KZ/2pDigvAL7gdmN9hjve8AOcrpkEt/vsIODbzZbus30QkAYUAajqv1T1ApyQU5xuoyZU9R1gK/BNn8X7gKPAMJ/XHO/zXp7stMxFbg0NdUbjtJ52uzU8oqpjgGE4XU3/4y5foarXAsnAK8DcVp4jtVn3Yz/3eVutxd2uFijm5F+XaUcWECZQioE0EQkLwLGXA4fcAd9Id1B6uIjkuusfA34uIme7A65ZIpLkU9eAVo79MM64xZMi0h9ARFJF5GERycLpeqnF6QYKEZGfudv7GiMi17ldJHcC1cAyERkkIpeJSDhQhfOBX9dCHT8Gvt/wh9sSeRT4nYgk+9TVMO5SDCT5DO6eyL+Ar4hItlvP/wIfqepOEckVkXPdcZDDbq11IhImIl8UkXi3++xQK/WDEyLfccdfpgFDgP/42e5Z4C4RyRSRGLeW51W14X2up/X/ZiZALCBMoCwC1gN7RWRfex7YHf/4PM74xA6cb9eP4XT1gPMhPxd4G+dD7HGc01bB6d550u2mme7n2OXA+Th97R+JSAWwEKeVsBV4C+eMoU9wukKq8OlScr2KM7C7H7gZuM79QA0HHnTr3YvzAfqjFl7jf3GC0NcP3BqWicghnNbOIHf7TTgftNvd19bqWUyquhBnQPxFnDGXs2gcz4jDCaP97mssA37rrrsZ2Ok+/220cMaV6yPgbPf1/hKY2sJY0BPAP3HOWNqB857e7tZ5xN33v+7rGtfa6zLtS+yGQeZM4J5185iqdupvkiJyHzBQVVv74OzyROQWYJbbnWbOUNaCMGeK4TjfLo0xHcROIzOdnoj8AZgMtHmeJ2PM6bMuJmOMMX5ZF5Mxxhi/At7FJM7EY3k4F81c02zd73AurwfnIqDkhmkZRKQOWOuu+1RVJ7f2PD179tSMjIz2LN0YY7q8/Pz8farq94LLjhiDuAPYyGfPFUdV72r4XURuB0b5rD6qqv6mWfArIyODvLy806nTGGO6HRHZ1dK6gHYxiUgaztwrj7Vh8xtwzuM2xhjTCQR6DOL3OFeDtjYzJu4Vq5k0nRkyQkTyRGSZiEwJYI3GGGP8CFhAiMg1QImq5rdh85nAPPcK2Qb9VDUHuBH4vYh8Zm4dEZnthkheaWlp+xRujDEGCOwYxHhgsohchTMrZZyIPN3CFaYzgW/5LmiYz15Vt4vIYpzxieZTI88B5gDk5OTY+brGmJNSU1NDYWEhVVVVXpcScBEREaSlpREa6u9WI/4FLCBU9R6ced0RkUuA7/kLB/cmKgk4t3lsWJaAc4OXahHpiRM2vw5UrcaY7qmwsJDY2FgyMjJodt+rLkVVKSsro7CwkMzMzDbv1+HXQYjIAyLie8rqDcBzzeaNHwLkichq4F3gQVXd0JF1GmO6vqqqKpKSkrp0OACICElJSSfdUuqQqTZUdTGw2P39Z83W3edn+w+BER1QmjGmm+vq4dDgVF5nt7+S+sCRY/zunU/YvLfC61KMMaZT6fYBAfCX97bx7PJPvS7DGNPNHDhwgP/7v/876f2uuuoqDhw4EICKmur2AdEjKowrhvXh5ZW7qapp7eZYxhjTvloKiLq61j+L/vOf/9CjR49AlXVctw8IgBk56Rw8WsPbG4q9LsUY04388Ic/ZNu2bWRnZ5Obm8ull17KjTfeyIgRzhDslClTGDNmDMOGDWPOnDnH98vIyGDfvn3s3LmTIUOG8LWvfY1hw4Zx+eWXc/To0Xarz+4HAZx/VhKpPSKZu6KAySNbvVOjMaaLuv/f69lQdKhdjzk0JY57Pz+sxfUPPvgg69atY9WqVSxevJirr76adevWHT8V9YknniAxMZGjR4+Sm5vL9ddfT1JSUpNjbNmyhWeffZZHH32U6dOn8+KLL3LTTe1zQ0NrQQBBQcK0nDQ+2LqPgvIjXpdjjOmmxo4d2+Q6hUceeYSRI0cybtw4CgoK2LJly2f2yczMJDvbmdd0zJgx7Ny5s93qsRaEa1pOOn9YuIV5+YXc9blzvC7HGNPBWvum31Gio6OP/7548WIWLFjA0qVLiYqK4pJLLvF7HUN4ePjx34ODg9u1i8laEK7UHpFcMLAn8/ILqau3WTuMMYEXGxtLRYX/U+wPHjxIQkICUVFRbNq0iWXLlnVwdRYQTczITWf3gaP8d+s+r0sxxnQDSUlJjB8/nuHDh/M///M/TdZNmjSJ2tpasrKy+OlPf8q4ceM6vL4uc0/qnJwcPd0bBlXX1jHufxdy/sCe/PnG0e1UmTGms9q4cSNDhgzxuowO4+/1iki+O3P2Z1gLwkd4SDBTRqXyzvpi9h8+5nU5xhjjKQuIZmbkpnOsrp6XV+72uhRjjPGUBUQzg/vEkZUWz9y8ArpK95sxxpwKCwg/pueks2lvBWsKD3pdijHGeMYCwo/J2SlEhAYxN6/A61KMMcYzFhB+xEWEctXwvsxfVcTRYzaBnzGme7KAaMH03HQqqmt5Y90er0sxxhgAYmJiACgqKmLq1Kl+t7nkkks43VP+G1hAtODczEQykqJ4foV1MxljOpeUlBTmzZsX8OexgGiBiDAtJ52PdpSzc99hr8sxxnRBP/jBD5rcD+K+++7j/vvvZ8KECYwePZoRI0bw6quvfma/nTt3Mnz4cACOHj3KzJkzycrKYsaMGTbdd0e5fnQaD729mbl5BXx/0mCvyzHGBNIbP4S9a9v3mH1GwJUPtrh65syZ3HnnnXzzm98EYO7cubz55pvcddddxMXFsW/fPsaNG8fkyZNbvKf0X/7yF6KiolizZg1r1qxh9Oj2mwUi4C0IEQkWkZUi8pqfdbeISKmIrHIfs3zWfVlEtriPLwe6Tn/6xEdwyaBk5uUXUltX70UJxpgubNSoUZSUlFBUVMTq1atJSEigb9++/OhHPyIrK4uJEyeye/duiotbvpnZkiVLjt//ISsri6ysrHarryNaEHcAG4G4FtY/r6rf9l0gIonAvUAOoEC+iMxX1f0BrdSP6TnpLNpUwpItpVw2uHdHP70xpqO08k0/kKZOncq8efPYu3cvM2fO5JlnnqG0tJT8/HxCQ0PJyMjwO823r5ZaF6croC0IEUkDrgYeO8ldrwDeUdVyNxTeASa1d31tMWFIMj1jwmyw2hgTEDNnzuS5555j3rx5TJ06lYMHD5KcnExoaCjvvvsuu3btanX/iy66iGeeeQaAdevWsWbNmnarLdBdTL8Hvg+01j9zvYisEZF5IpLuLksFfD+RC91lTYjIbBHJE5G80tLSdivaV2hwENeNTmPhxhJKK6oD8hzGmO5r2LBhVFRUkJqaSt++ffniF79IXl4eOTk5PPPMMwwe3Pr45ze+8Q0qKyvJysri17/+NWPHjm232gLWxSQi1wAlqpovIpe0sNm/gWdVtVpEbgOeBC4D/LWXPjMxkqrOAeaAM913uxTux/ScNOYs2c7LKwuZfdFZgXoaY0w3tXZt4+B4z549Wbp0qd/tKisrAcjIyGDdunUAREZG8txzzwWkrkC2IMYDk0VkJ/AccJmIPO27gaqWqWrD1/JHgTHu74VAus+maUBRAGtt1cDkWEb368HzK2wCP2NM9xGwgFDVe1Q1TVUzgJnAIlW9yXcbEenr8+dknMFsgLeAy0UkQUQSgMvdZZ6ZkZvOttLDfPxph4+TG2OMJzr8QjkReUBEJrt/fkdE1ovIauA7wC0AqloO/BxY4T4ecJd55uqsFKLCgm2w2pguprv0CpzK6+yQgFDVxap6jfv7z1R1vvv7Pao6TFVHquqlqrrJZ58nVHWg+/h7R9TZmpjwEK7J6stra/ZwuLrW63KMMe0gIiKCsrKyLh8SqkpZWRkREREntZ9dSX0SZuSmMzevkNfX7GF6bvqJdzDGdGppaWkUFhYSqLMgO5OIiAjS0tJOah8LiJMwul8CZ/WK5vm8AgsIY7qA0NBQMjMzvS6j07LJ+k6CiDA9J538XfvZWlLhdTnGGBNQFhAn6brRaYQECXPzCr0uxRhjAsoC4iT1ig3nssHJvPRxITU2gZ8xpguzgDgFM3LT2Vd5jIUbS7wuxRhjAsYC4hRcfE4vkmPDeSHProkwxnRdFhCnICQ4iKlj0nh3cwnFh1qfhtcYY85UFhCnaHpOOvUK8/JtsNoY0zVZQJyijJ7RjM1M5IU8m8DPGNM1WUCchhk56ewsO8JHOzydJsoYYwLCAuI0XDWiL7HhIcy1CfyMMV2QBcRpiAwL5vPZKfxn3R4OVdV4XY4xxrQrC4jTNCMnnaqaeuav8ux+RsYYExAWEKcpKy2ewX1i7ZoIY0yXYwFxmkSEaTnprC48yKa9h7wuxxhj2o0FRDv4wqhUQoPF7jZnjOlSLCDaQWJ0GJcP7cPLK3dTXVvndTnGmO5CFfbvhD2rA3J4u2FQO5mem87ra/fwzoZirslK8bocY0xXU1MFpZtg79rGR/E6qD4EKaNh9rvt/pQBDwgRCQbygN0N96X2WXc3MAuoBUqBr6rqLnddHbDW3fRTVZ0c6FpPxwUDe5ISH8HzKwosIIwxp+dwGRSvbRoG+z6B+lpnfVgM9B4GWdOhzwjomx2QMjqiBXEHsBGI87NuJZCjqkdE5BvAr4EZ7rqjqhqYVx0AwUHC1Jx0/rhoC4X7j5CWEOV1ScaYzq6+Hvbv+Gyr4NDuxm1iU5wQGHSV87PPCEjIhKDAjxAENCBEJA24GvglcHfz9arq2yZaBtwUyHoCbdqYNB5ZuIUX83dzx8SzvS7HGNOZ1ByFkg3NwmA9HKt01ksw9BoEGRc0BkHvERCd5FnJgW5B/B74PhDbhm1vBd7w+TtCRPJwup8eVNVXmu8gIrOB2QD9+vU7/WpPU3piFOMHJvFCfgG3XzaQoCDxuiRjjBcqS2Hvmqatgn2fgLp3oQyLdQIg+8bGMOg1BEIjvK27mYAFhIhcA5Soar6IXHKCbW8CcoCLfRb3U9UiERkALBKRtaq6zXc/VZ0DzAHIycnpFFOqTs9J547nVvHhtjIuOLun1+UYYwKpvg7Kt/uEwTrnZ+Xexm3i0pwAGDK5MQx69O+QLqLTFcgWxHhgsohcBUQAcSLytKo26UYSkYnAj4GLVbW6YbmqFrk/t4vIYmAU0CQgOqMrhvUhPjKU5/MKLCCM6UrqapyziPashqJVzs/idVBzxFkfFAK9BsNZl/p0EQ2HqERv6z4NAQsIVb0HuAfAbUF8z084jAL+BkxS1RKf5QnAEVWtFpGeOGHz60DV2p4iQoOZkp3CsysKOHDkGD2iwrwuyRhzsmqqnPGCPathT0MYrIe6Y876sBjokwWjboa+WW4X0WAICfe27nbW4ddBiMgDQJ6qzgd+A8QAL4gINJ7OOgT4m4jU41zM96CqbujoWk/V9Nx0nly6i1dW7uaW8Zlel2OMac2xw86Hv2/LoHRj4ymlEfHQdySc+3XndNK+IyHxrDOii+h0SVe5G1pOTo7m5eV5XcZx1/zxferq4T/fuQA3/IwxXqs65IwR+LYMfAePo5IaQ6DhkZABXfjfsIjkq2qOv3V2JXWATM9J52evrmd90SGGp8Z7XY4x3c+Rcmfw2LdlUO4zjBnb1wmAodc2hkFcapcOg5NlAREg145M5Revb+T5FQUWEMYEWmWpT6vADYMDnzauj+/njBWMvKExDGJ7e1fvGcICIkDio0K5cngfXlm1mx9fPYSI0GCvSzLmzKcKFXvcMPBpGVT43LArcQCkjoGcr7phkH1Gn0nkJQuIAJqRk86rq4p4c91epoxK9bocY84sqs6UE0U+rYKiVXC44YRHgZ7nOFce9x0JKdnO2UQR1mJvLxYQATRuQBLpiZE8v6LAAsKY1qjCwYLGMGhoGRzZ56yXIOc00oETG8Og93AIj/G27i7OAiKAgoKE6WPSeeidT9hVdpj+SdFel2SM91ThwK7PhsHRcme9BEPyEDhnkhMEfUc6YRBmE2B2NAsIVVjyG8iaAQn92/3wU3PSeHjBJ7yQV8j3rhjU7sc3plNTdaaiaBhAbgiDqgPO+qAQJwwGX+2GwSjoPRRCI72t2wAWEFC2Dd5/GJb8Fs7/NlxwF4S3ZW7BtukbH8lFZ/diXn4hd33uHIJtAj/TVdXXu2Hg2zJYA9UHnfVBoc6H/9Br3TDIhuShnW6COtPIAqLnQLg9HxbeD+8/BCufhgk/g5E3ttuVkjNy0/nmMx+zZEsplw5KbpdjGuOp+noo29p08HjvGufuZgDBYc4NbYZf1zQMQmzqmTOJBQRAfCpcNwfGzoY3fwivfguWz4FJD0L/80/78BOH9CYxOoy5KwosIMyZp74O9m1p2kW0d03jfQxCIpwxghHTfMJgCASHelu3OW0WEL7ScuDWd2DtPFhwL/z9Shg6BT73wGmNT4SFBPGFUak8tXQnZZXVJMV0rQm9TBdSV+tMPdEkDNZCzWFnfUikcyrpyBsaw6DXIAuDLsoCojkRyJrmDJp9+Ah88HvY/MZpj0/MyE3n8Q928PLK3cy6cEA7F23MKairdaev9u0mWgu1R531oVHOjKWjb2684KznORBsHxvdhU3WdyIHdzvjE2ueh5jepzU+MeXP/+VwdS1v33WRTeBnOlZdDZRsbBoGxeugtspZ3zB9dcNppX2zoefZEGQzAHR1Nlnf6WjH8YkZuenc89JaVhYcYHS/hAAVbLq92mPuvQx8w2A91Ln34wqLdUIgd1ZjGCQN7BbTV5uTYy2Ik6EK616Ed37mTAEwdAp87n5nOuA2qKiqYewvFzJlVAr/77qswNZquofa6sZ7GTSMG5RsaLyxTXi8M0ldw3hB32xnriILA+OyFkR7EYERU2HQVfDhH+G/7vjEed+CC+8+4fhEbEQoV43oy79X7+Gn1wwlKszefnMSaqqgZH3TK5BLNkJ9jbM+It4JgHHfaGwZJGRaGJhTZp9QpyIsCi75AYy6yRmf+ODhxusnsr/Y6j/IGbnpvPhxIa+v2cO0nPQOLNqcUSpLnDGCveucFkLxOmdAueEuZ5EJTgCc/+3GG9x08RvbmI5nXUztoTDPGZ8oXOEM9E16EDLG+91UVZnw0HskxYTxwm2nf42FOcPVVkPpZicAGoKgeD0cLm3cJjbFuQL5+F3OsqFHPwsD0y6siynQGq6fWPcivHMv/OMqZzqBzz3wmfEJEWFaTjq/enMT20orOauXzUbZLTTcx6B4vXMqafF657HvE9A6Z5uQCHeSuiucC896D3N+2r0MjEesBdHejh2BpX+CD37nXIF63jfhwu82GZ8oOVTFeQ8uYtaFmdxz5RAPizUBceyI0x10vFXgtgyO7m/cJr6fGwDuo88Id/DYTis1Hau1FkTAA0JEgoE8YLeqXtNsXTjwFDAGKANmqOpOd909wK1AHfAdVX2rtefpNAHR4OBuWPgArHkOopPd8Ykbj38AzHpyBasKDrL0nssIDbZBxDNSwz0Mite7YwVuIJRvA613tgmNdrqHGloDvYc5cxJF9vC2dmNcXncx3QFsBOL8rLsV2K+qA0VkJvArYIaIDAVmAsOAFGCBiJyj2tAWPwPEp8J1f2u8fmL+txuvn8gYz/ScdBZsLOHdTSVcPqyP19WaE6mudM4YKvbpHipe3zg5HThnDPUeBsOvb2wZ2FlE5gwW0IAQkTTgauCXwN1+NrkWuM/9fR7wJ3EuMb4WeE5Vq4EdIrIVGAssDWS9AZE2Bm59+zPjE5dedj89Y8KZm1doAdFZqEJlsTMFfNnWxkfJRti/o3G78Djnwz9remPLIHlIu04Tb0xnEOgWxO+B7wMt/ctJBQoAVLVWRA4CSe7yZT7bFbrLmhCR2cBsgH79+rVf1e3N9/oJd3widPMbPJI8k29svpiSQ8NJjrM58TtM1UH3w79ZEJRta5yhFCA43BkX6JvlnL7c0CqwM4hMNxGwgBCRa4ASVc0XkUta2szPMm1ledMFqnOAOeCMQZxiqR0nLAou/r57/cQDnL/6KRaEvsq613eRPONOG6BsTzVVzrf+5gFQtrXpKaSI84GfNBDSxzk/k85yfsan2X8T060FsgUxHpgsIlcBEUCciDytqjf5bFMIpAOFIhICxAPlPssbpAFFAay1Y8WlwBf+Crlfo/zJb3Hp5gfQOS8jn/s5pI+FMLt3dZvU1zmDxP5aAwcKaPKdIqa386E/6Eo3BNxHQgaE2PTrxvjTIae5ui2I7/k5i+lbwAhVvc0dpL5OVaeLyDDgXzjjDinAQuDs1gapO91ZTG30wopPWfLy3/htwkuEH3YzMCwWYntDTJ/Wf0b06PpdHarOVcX+WgL7dzTOOQTO2EDDt//jj7Mg8SyI8HeOhDHG67OYmhfzAJCnqvOBx4F/uoPQ5ThnLqGq60VkLrABqAW+dUadwXQSrspK4b5/X8i96ZN5cFiB8424ohgq9zo/d3/sDJzWHPnsziEREJPcGBixfZ1vyrF9moZJVJL3Z9LUHoPqCuf+xNUVUHXI/bvCOROo+tBnl1XsdccFKhqPExzmjAv0PBsGTWoaBtG9un5gGtOB7EK5TuCHL67h1VVFLP/xBGIj/NyZS9X50Kwsdj40j//c2zRMKvc6A7DNBYU412KcqFUSk/zZO4PV1Tof0E0+1H1+fmZZhVOD77KqQ41TTbcmKNT5ph8e67QGontC0tluq6BhXCDdxgWMaUedqgVhPmt6bjrPrSjgtTV7uGGsn7OxRJwPzog455tza2qONguRZj8PFjhzRh3Z52dncVobkT2c8/6rKxpvNdkaCXI+0MPjGj/gY5KdD/Tw2KYf+uFxLS8LtTO5jOlMLCA6gVHpPTg7OYbnVxT4D4iTERoJiZnOozV1NU7ffpNWiPuoPtTKB3qsc48B32WhUda1Y0wXZAHRCYgIM3LT+cXrG1mwoZiJQ3sH/kmDQ52rveM/c3mJMcYAYHMAdBIzctMZnhrHbU/n8+qq3V6XY4wxFhCdRWxEKM9+bRxj+idw5/Or+OeyXV6XZIzp5iwgOpHYiFCe/OpYJgxO5qevrONPi7bQVc4yM8aceSwgOpmI0GD+ctMYrhuVym/f/oRfvL6R+noLCWNMx7NB6k4oNDiI304bSVxkKI9/sIODR2t48LoRhNh9I4wxHahNnzgicoeIxInjcRH5WEQuD3Rx3VlQkHDv54dy18RzmJdfyDef+Ziqmi55MbkxppNq61fSr6rqIeByoBfwFeDBgFVlAOf01zsmns19nx/K2xuK+crfV1BZXet1WcaYbqKtAdFwFdRVwN9VdTX+p+Q2AXDL+Ex+N2Mky3eWc+Ojyyg/fOzEOxljzGlqa0Dki8jbOAHxlojEAvWBKxwG+J4AABnVSURBVMs094VRafztpjFs3lvBtL9+SNGBo16XZIzp4toaELcCPwRyVfUIEIrTzWQ60MShvXnqq2MpOVTNtL8uZXtp5Yl3MsaYU9TWgDgP2KyqB0TkJuAngJ9pQ02gnTsgiWdnj6Oqpo5pf13Kut32n8EYExhtDYi/AEdEZCTOPaZ3AU8FrCrTquGp8bxw23lEhAZzw5xlfLS9zOuSjDFdUFsDoladS3qvBf6gqn8AYgNXljmRAb1ieOG280iOC+dLTyxn0aZir0syxnQxbQ2IChG5B7gZeF1EgnHGIYyHUnpE8sJt5zOoTyyzn8rnlZU2yZ8xpv20NSBmANU410PsBVKB3wSsKtNmidFhPDPrXHIynEn+nvxwp9clGWO6iDYFhBsKzwDxInINUKWqNgbRScRGhPKPr4xl4pDe3Dt/PX9YYJP8GWNOX1un2pgOLAemAdOBj0Rk6gn2iRCR5SKyWkTWi8j9frb5nYisch+fiMgBn3V1Puvmn9zL6n4iQoP5602juX50Gr9b8An3/3uDTfJnjDktbZ2s78c410CUAIhIL2ABMK+VfaqBy1S1UkRCgQ9E5A1VXdawgare1fC7iNwOjPLZ/6iqZrexPgOEBAfxm6lZxEeG8sR/d3DoaA2/mppFqE3yZ4w5BW0NiKCGcHCVcYLWh3vWU8OVXKHuo7WvtDcA97axHtOCoCDhp9cMISEqlIfe+YRDVTX86cbRRIQGe12aMeYM09avlm+KyFsicouI3AK8DvznRDuJSLCIrAJKgHdU9aMWtusPZAKLfBZHiEieiCwTkSkt7Dfb3SavtLS0jS+l6xMRbp9wNj+/dhgLN5Xw5SeWU1FV43VZxpgzjLR1MFNErgfG40zSt0RVX27zk4j0AF4GblfVdX7W/wBIU9XbfZalqGqRiAzACY4JqrqtpefIycnRvLy8tpbUbby6ajffnbuawX1jefIrY0mKCfe6JGNMJyIi+aqa429dmzunVfVFVb1bVe86mXBw9z0ALAYmtbDJTODZZvsUuT+3u/uO+uxu5kSuzU7l0S/lsKW4kml/W8pum+TPGNNGrQaEiFSIyCE/jwoROXSCfXu5LQdEJBKYCGzys90gIAFY6rMsQUTC3d974rRcNpzsizOOSwcn8/SscymtqGbaXz5km03yZ4xpgxMNNMeqapyfR6yqxp3g2H2Bd0VkDbACZwziNRF5QEQm+2x3A/CcNu3rGgLkichq4F3gQVW1gDgNuRmJPDd7HMfq6m2SP2NMm7R5DKKzszGIttmx7zA3PfYRB4/W8NiXcxg3IMnrkowxHmqXMQjTNWT2jGbeN86jT3wEX3piOe9ssEn+jDH+WUB0Q33jI5n79fMY0ieW257O56WPC70uyRjTCVlAdFOJ0WE887VxnJuZyN1zV/PEBzu8LskY08lYQHRjMeEhPHFLLlcM680Dr23g4Xc+sUn+jDHHWUB0cxGhwfz5xtFMG5PGIwu3cN/89TbJnzEGaPtcTKYLCwkO4tdTs+gRFcqj7+9g/5EafvmF4cRG2D2hjOnOLCAM4Mzf9KOrhpAQHcZv3trMRzvKuO/zw5g0vA8i4nV5xhgPWBeTOU5E+OYlA3n5m+NJig7nG898zK1P5lFQfsTr0owxHrCAMJ+Rnd6D+d8ez0+uHsKy7WVc/rsl/PW9bdTU1XtdmjGmA1lAGL9CgoOYdeEA3rn7Yi44uycPvrGJz//xA/J37fe6NGNMB7GAMK1K7RHJo1/KYc7NYzh0tIbr//Ih97y0loNH7P4SxnR1FhCmTS4f1od37r6YWRdkMjevgAkPL+bVVbvtugljujALCNNm0eEh/OSaocz/9nhSE6K447lV3Pz4cnbuO+x1acaYALCAMCdtWEo8L33jfH5+7TBWFxzg8t8v4ZGFW6iurfO6NGNMO7KAMKckOEi4+bwMFn73Yi4f2puH3/mEK//wPku3lXldmjGmnVhAmNOSHBfBn24czT++kktNXT03PLqM785dTfnhY16XZow5TRYQpl1cMiiZt++8mG9echavrtrNhIcWMzevwAaxjTmDWUCYdhMZFsz3Jw3mP3dcyMDkGL4/bw0z5ixja0mF16UZY06BBYRpd+f0juX52efxq+tHsHlvBVf+4X1++9ZmqmpsENuYM0nAAkJEIkRkuYisFpH1InK/n21uEZFSEVnlPmb5rPuyiGxxH18OVJ0mMIKChBm5/Vj43Yv5fFYKf3p3K1f8fglLPin1ujRjTBtJoPqIxZkCNFpVK0UkFPgAuENVl/lscwuQo6rfbrZvIpAH5AAK5ANjVLXFeR5ycnI0Ly+v/V+IaRcfbt3HT15Zx/Z9h/n8yBR+es0QkmMjvC7LmG5PRPJVNcffuoC1INRR6f4Z6j7amkZXAO+oarkbCu8AkwJQpukg5w/syRt3XsidE8/mrXV7mfDQe/xz2S67OZExnVhAxyBEJFhEVgElOB/4H/nZ7HoRWSMi80Qk3V2WChT4bFPoLmt+/NkikicieaWl1nXR2YWHBHPnxHN4884LGZEaz09fWcd1f/mQDUWHvC7NGONHQANCVetUNRtIA8aKyPBmm/wbyFDVLGAB8KS73N8daj7zVVNV56hqjqrm9OrVqz1LNwE0oFcMz8w6l4enj6Sg/Aif/9MH/PL1DRyurvW6NGOMjw45i0lVDwCLadZNpKplqlrt/vkoMMb9vRBI99k0DSgKcJmmA4kI141OY+F3L2bamDQefX8Hl/9uCQs2FHtdmjHGFcizmHqJSA/390hgIrCp2TZ9ff6cDGx0f38LuFxEEkQkAbjcXWa6mB5RYTx4fRYv3HYe0eHBzHoqj6//M489B496XZox3V4gWxB9gXdFZA2wAmcM4jUReUBEJrvbfMc9BXY18B3gFgBVLQd+7u63AnjAXWa6qNyMRF67/UK+P2kQ731SysSH3uOv722j0rqdjPFMwE5z7Wh2mmvXUVB+hJ+9uo53N5cSGxHCF8/tz1fGZ9A7zk6LNaa9tXaaqwWE6bRWfrqfR9/fzpvr9hIcJEzJTmX2RQM4u3es16UZ02VYQJgz2q6ywzz+wQ7m5hVQVVPPpYN6Mfuisxg3IBHnekxjzKmygDBdQvnhY/xz6S6eWrqTssPHyEqL52sXDuDK4X0ICbZpxYw5FRYQpkupqqnjxY8Leez9HezYd5j0xEhuHZ/J9Nx0osJCvC7PmDOKBYTpkurqlQUbi5mzZDv5u/YTHxnKzeP68+XzM+gVG+51ecacESwgTJeXv6ucOUu28/aGYkKDg7h+dCq3XjCAgckxXpdmTKdmAWG6je2llTz+wQ7m5RdSXVvPxCG9+frFA8jpn2AD2sb4YQFhup19ldU8tXQX/1y6k/1HashO78HXLxrA5cP6EBxkQWFMAwsI020dPVbHvPwCHn1/B5+WH6F/UhSzLshk6ph0IsOCvS7PGM9ZQJhur65eeWv9Xv62ZDurCw6QGB3GzeP686Xz+pMUYwPapvuygDDGpaqs2LmfOUu2sWBjCeEhQUwdk8asCweQ2TPa6/KM6XCtBYSdNG66FRFhbGYiYzMT2VpSwWPv7+CFvEL+tfxTrhjah69dNIAx/RO8LtOYTsFaEKbbK6mo4qkPd/HPZbs4eLSGnP4JzL5oABOH9CbIBrRNF2ddTMa0weHqWubmFfD4Bzso3H+UAT2jmXXhAK4bnUpEqA1om67JAsKYk1BbV88b6/YyZ8l21u4+SGxECFcO78OU7FTOHZBkp8maLsUCwphToKp8tKOcefmFvLluL5XVtfSOC2fyyBSuzU5lWEqcXXxnzngWEMacpqqaOhZsLOaVlUW890kJNXXKwOQYpmQ7YZGeGOV1icacEgsIY9rRgSPHeH3tHl5dWcTync6dcMf0T2BKdgpXZ6WQGB3mcYXGtJ0FhDEBUrj/CPNXF/HqyiI2F1cQEiRcdE4vrs1O4fKhfexqbdPpWUAY0wE27jnEK6t2M39VEXsOVhEVFswVw/pwbXYKFwzsaTc1Mp2SJwEhIhHAEiAc54K8eap6b7Nt7gZmAbVAKfBVVd3lrqsD1rqbfqqqk1t7PgsI01nU1yvLd5bz6qrdvL5mD4eqaukZE8Y1WSlMGZXKyLR4G9w2nYZXASFAtKpWikgo8AFwh6ou89nmUuAjVT0iIt8ALlHVGe66SlVt82T+FhCmM6qurePdTaW8umo3CzeVcKy2noykKK7NTmXKqFSb3sN4zpOpNtRJnkr3z1D3oc22edfnz2XATYGqxxgvhIcEM2l4HyYN78OhqhreXLuXV1bt5pFFW/jDwi2MTIvn2uxUrhnZl+TYCK/LNaaJgI5BiEgwkA8MBP6sqj9oZds/AXtV9Rfu37XAKpzupwdV9RU/+8wGZgP069dvzK5du9r/RRgTAHsPVvHv1UW8smo364sOESQwfmBPpmSncsXwPsSE2zRppmN4PkgtIj2Al4HbVXWdn/U3Ad8GLlbVandZiqoWicgAYBEwQVW3tfQc1sVkzlRbSyp4ZaUTFoX7jxIRGsTEIb2Zkp3KRef0IizEBrdN4HgeEG4R9wKHVfW3zZZPBP6IEw4lLez7D+A1VZ3X0vEtIMyZTlX5+NP9vLKyiNfWFLH/SA0JUaFcNaIvU0alMqZfgk0eaNqdV4PUvYAaVT0gIpHA28CvVPU1n21GAfOASaq6xWd5AnBEVatFpCewFLhWVTe09HwWEKYrqamr5/0tpby8soh3NuylqqaeXrHhXDYomcuGJHPBwJ5EWzeUaQde3Q+iL/CkOw4RBMxV1ddE5AEgT1XnA78BYoAX3NP+Gk5nHQL8TUTq3X0fbC0cjOlqQoODuGxwby4b3JvK6loWbChmwcZi/rNuD8/nFRAWHMS5AxKZMDiZCUN621QfJiDsQjljziA1dfXk7dzPok3FLNxUwvbSwwCcnRzDZUOSmTC4N6P79bCL8kybdYoxiECzgDDd0Y59h1m0qYRFm4r5aHs5tfVKfGQoF5/TiwlDkrn4nF70iLK5oUzLLCCM6QYqqmr4YMs+Fm4q4d1NJZQdPkaQQE7/RLd1kczA5Bi7its0YQFhTDdTX6+sLjzAok0lLNxYwoY9hwBIT4x0B7p7c25mot0pz1hAGNPd7Tl41OmK2ljCf7fto6qmnqiwYC4Y2JMJQ5K5dFAyyXF2JXd3ZAFhjDmuqqaOpdvKWLipmEUbSyg6WAXAiNR4LhuczIQhyQxPibdrLroJCwhjjF+qyqa9FW5XVDErCw6gil1z0Y1YQBhj2qSssprFm0tZtLmEJZtLqaiubXLNxWWDe9Mvya656EosIIwxJ62mrp4VO8tZtLGERZtK2L7PueYitUckYzMTGZuZSG5GImf1irYzo85gFhDGmNO2Y99hFm8uYfmOcpbvKKfs8DEAkqLDyM1IJDczkXMzExnSN45gG784Y3g11YYxpgvJ7BlNZs9MvjI+E1Vl+77DrHDDYvnOct5cvxeAmPAQRvdP4Fy3hZGVFm+n056hrAVhjGkXRQeOsmKnExgrdpbzSbFzv7CwkCCy03qQm5lAbkYiY/onEBsR6nG1poF1MRljOlz54WPk7Sw/Hhrrig5RV68ECQxNiSM3w+mSyslIpGdMuNfldlsWEMYYzx2urmXlpwdYvqOM5TvLWfnpAapr6wEY0Cv6eJfU2MxE0hLsTKmOYgFhjOl0jtXWs3b3AZbv2M8Kt6VRUVULQEp8BLnumVJjMxJtDqkAsoAwxnR6dfXK5r0Vx7uklu8sp7SiGoCEqNDjrYvcjESGpcTZlObtxALCGHPGUVV2lR05HhYrdpazq+wIAJGhwQxPjWNEag+y0uIZkRZPZlK0TQ9yCiwgjDFdQvGhKpbvKCd/137W7j7I+qKDVNU44xgx4SEMS4lzA6MHWanx9E+Ksq6pE7CAMMZ0SbV19WwtrWRN4UHW7T7ImsKDbNhziGPu4HdsRAgjUp0WRpbb2khLiLTQ8GEBYYzpNmrq6vmkuIK1hQdZu9t5bNxziJo657OuR1SoExqp8WSlxTM8NZ7UHt03NDy5klpEIoAlQLj7PPNU9d5m24QDTwFjgDJghqrudNfdA9wK1AHfUdW3AlWrMabrCA0OYlhKPMNS4pnpLquureOTvZWs2X3geEtjzpLt1NY7oZEYHXY8MBpaHH3iIrptaDQI5FQb1cBlqlopIqHAByLyhqou89nmVmC/qg4UkZnAr4AZIjIUmAkMA1KABSJyjqrWBbBeY0wXFR4SzAh3MLtBVU0dm/ZWOK2MwgOsKTzI/y3eR50bGj1jwo8HRsPP7nZTpYAFhDp9V5Xun6Huo3l/1rXAfe7v84A/iRPZ1wLPqWo1sENEtgJjgaWBqtcY071EhAaTnd6D7PQeQH8Ajh6rY8OeQ8dbGWt3H2Dx5hLczKB3XHiTM6eGpcTRKya8y7Y0AjpZn4gEA/nAQODPqvpRs01SgQIAVa0VkYNAkrvct6VR6C5rfvzZwGyAfv36tXv9xpjuJTIsmDH9ExjTP+H4ssPVtWzYc+j4mMaawgMs3FRMw/BtXEQIZyXHcFavhkc0ZyXH0C8xitAz/FqNgAaE2yWULSI9gJdFZLiqrvPZxF/saivLmx9/DjAHnEHqdijZGGOaiA4PcaYzz0g8vqyyupb1u50zpraXHmZbaSVLPillXn7h8W1CgoT+SVFOaCQ3DY+4M2Sywg6Z7ltVD4jIYmAS4BsQhUA6UCgiIUA8UO6zvEEaUNQRtRpjzInEhIdw7oAkzh2Q1GT5oaoaJzBKKtlW2vA4zKJNJccHxAGSY8Pd4IhubHkkx9A3LqJTXewXyLOYegE1bjhEAhNxBqF9zQe+jDO2MBVYpKoqIvOBf4nIwziD1GcDywNVqzHGtIe4iFCfcY1GNXX1FJQfYZvb2mgIkPmrijjkzj8FzhXiA3r5hobze2bPaE/uqRHIFkRf4El3HCIImKuqr4nIA0Ceqs4HHgf+6Q5Cl+OcuYSqrheRucAGoBb4lp3BZIw5U4UGBzGgVwwDesXwOXofX66qlB0+5gbGYba6wfHxp/v595qi4+McIpCWEMlZvWIY2KzLKjE6LGCD5HahnDHGdEJHj9WxY9/hJl1V20oq2b6v8vj0IuBc+Hfh2b344w2jTul57JajxhhzhokMC2ZoShxDU+KaLK+vV4oOHj0eGNtKK4mPDMygtwWEMcacQYKChLSEKNISorj4nF6Bfa6AHt0YY8wZywLCGGOMXxYQxhhj/LKAMMYY45cFhDHGGL8sIIwxxvhlAWGMMcYvCwhjjDF+dZmpNkSkFNh1GofoCexrp3LOdPZeNGXvR1P2fjTqCu9Ff1X1e8VdlwmI0yUieS3NR9Ld2HvRlL0fTdn70airvxfWxWSMMcYvCwhjjDF+WUA0muN1AZ2IvRdN2fvRlL0fjbr0e2FjEMYYY/yyFoQxxhi/LCCMMcb41e0DQkQmichmEdkqIj/0uh4viUi6iLwrIhtFZL2I3OF1TV4TkWARWSkir3ldi9dEpIeIzBORTe7/I+d5XZOXROQu99/JOhF5VkQivK6pvXXrgBCRYODPwJXAUOAGERnqbVWeqgW+q6pDgHHAt7r5+wFwB7DR6yI6iT8Ab6rqYGAk3fh9EZFU4DtAjqoOB4KBmd5W1f66dUAAY4GtqrpdVY8BzwHXelyTZ1R1j6p+7P5egfMBkOptVd4RkTTgauAxr2vxmojEARcBjwOo6jFVPeBtVZ4LASJFJASIAoo8rqfddfeASAUKfP4upBt/IPoSkQxgFPCRt5V46vfA94F6rwvpBAYApcDf3S63x0Qk2uuivKKqu4HfAp8Ce4CDqvq2t1W1v+4eEOJnWbc/71dEYoAXgTtV9ZDX9XhBRK4BSlQ13+taOokQYDTwF1UdBRwGuu2YnYgk4PQ2ZAIpQLSI3ORtVe2vuwdEIZDu83caXbCZeDJEJBQnHJ5R1Ze8rsdD44HJIrITp+vxMhF52tuSPFUIFKpqQ4tyHk5gdFcTgR2qWqqqNcBLwPke19TuuntArADOFpFMEQnDGWSa73FNnhERwelj3qiqD3tdj5dU9R5VTVPVDJz/Lxapapf7hthWqroXKBCRQe6iCcAGD0vy2qfAOBGJcv/dTKALDtqHeF2Al1S1VkS+DbyFcxbCE6q63uOyvDQeuBlYKyKr3GU/UtX/eFiT6TxuB55xv0xtB77icT2eUdWPRGQe8DHO2X8r6YLTbthUG8YYY/zq7l1MxhhjWmABYYwxxi8LCGOMMX5ZQBhjjPHLAsIYY4xfFhDGdAIiconNGGs6GwsIY4wxfllAGHMSROQmEVkuIqtE5G/u/SIqReQhEflYRBaKSC9322wRWSYia0TkZXf+HkRkoIgsEJHV7j5nuYeP8bnfwjPuFbrGeMYCwpg2EpEhwAxgvKpmA3XAF4Fo4GNVHQ28B9zr7vIU8ANVzQLW+ix/Bvizqo7Emb9nj7t8FHAnzr1JBuBc2W6MZ7r1VBvGnKQJwBhghfvlPhIowZkO/Hl3m6eBl0QkHuihqu+5y58EXhCRWCBVVV8GUNUqAPd4y1W10P17FZABfBD4l2WMfxYQxrSdAE+q6j1NFor8tNl2rc1f01q3UbXP73XYv0/jMetiMqbtFgJTRSQZQEQSRaQ/zr+jqe42NwIfqOpBYL+IXOguvxl4z72/RqGITHGPES4iUR36KoxpI/uGYkwbqeoGEfkJ8LaIBAE1wLdwbp4zTETygYM44xQAXwb+6gaA7+ynNwN/E5EH3GNM68CXYUyb2WyuxpwmEalU1Riv6zCmvVkXkzHGGL+sBWGMMcYva0EYY4zxywLCGGOMXxYQxhhj/LKAMMYY45cFhDHGGL/+P3YqpSDKWY8YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Init-inject CapsNet loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statement to instantiate the models\n",
    "from utils import model_utils\n",
    "# Import statements for other calculations\n",
    "from pickle import load\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from numpy import array\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_beam_search(model, tokenizer, photo, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Description: This function can be used to create description\n",
    "    :model: The decoder model object\n",
    "    :tokenizer: The tokenizer object used to get the words from predicted indexes\n",
    "    :max_length: The maximum length of the sentence to be generated\n",
    "    :beam_length: Length to check conditional probability. \n",
    "                1: for greedy search\n",
    "                1+: For beam search\n",
    "    \"\"\"\n",
    "    in_text = 'startseq'\n",
    "    beam_list = list()\n",
    "    for i in range(max_length):\n",
    "        if not beam_list:\n",
    "            sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "            yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "            yhat_idx = yhat.argsort()[-beam_length:]\n",
    "            for idx in yhat_idx:\n",
    "                word = tokenizer.index_word[idx]\n",
    "                in_text += ' ' + word\n",
    "                beam_list.append((in_text, log(yhat[idx])))\n",
    "        else:\n",
    "            combination_list = list()\n",
    "            for elems in beam_list:\n",
    "                if elems[0].endswith('endseq'):\n",
    "                    combination_list.append(elems)\n",
    "                    continue\n",
    "                in_text = elems[0]\n",
    "                sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "                sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "                yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "                yhat_idx = yhat.argsort()[-beam_length:]\n",
    "                for idx in yhat_idx:\n",
    "                    word = tokenizer.index_word[idx]\n",
    "                    if word is None:\n",
    "                        continue\n",
    "                    in_text += ' ' + word\n",
    "                    combination_list.append((in_text, elems[1]*log(yhat[idx])))\n",
    "            probs = array([combinations[1] for combinations in combination_list])\n",
    "            top_idx = probs.argsort()[-beam_length:]\n",
    "            for i, idx in enumerate(top_idx):\n",
    "                beam_list[i] = combination_list[idx]\n",
    "    probs = array([prob[1] for prob in beam_list])\n",
    "    top_idx = argmax(probs)        \n",
    "    return beam_list[top_idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BLEU(model, test_desc, photo_feature, tokenizer, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Decription: This function can be used to evaluate BLEU score of the model word by word\n",
    "    :model: The Decoder model\n",
    "    :test_desc: test description\n",
    "    :photo_feature: Extracted features of photos\n",
    "    :tokenizer: Tokenizer object\n",
    "    :max_length: Maximum length of the expected sentence\n",
    "    :beam_length: Beam Length for beam search\n",
    "    \"\"\"\n",
    "    actual, predicted = list(), list()\n",
    "    count = 0\n",
    "    for key, desc_list in test_desc.items():\n",
    "        yhat = generate_desc_beam_search(model, tokenizer, photo_feature[key], max_length, beam_length)\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "features = 'features_vgg.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "test_features = {image_id: np.expand_dims(feat, axis=0) for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, None, 4096)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, None, 4096)   0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 34, 256)      2013184     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, None, 256)    1048832     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 34, 256)      0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, 256), (None, 525312      dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, 34, 256), (N 525312      dropout_8[0][0]                  \n",
      "                                                                 lstm_7[0][1]                     \n",
      "                                                                 lstm_7[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "non_masking_4 (NonMasking)      (None, 34, 256)      0           lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 8704)         0           non_masking_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 256)          2228480     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 7864)         2021048     dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,362,168\n",
      "Trainable params: 8,362,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "BLEU-1: 0.531800\n",
      "BLEU-2: 0.276549\n",
      "BLEU-3: 0.178230\n",
      "BLEU-4: 0.078420\n"
     ]
    }
   ],
   "source": [
    "### Get the BLEU score VGG extracted features with beam length 1\n",
    "vgg_decoder_path = \"model-ep007-loss3.836-val_loss4.108_VGG_init.h5\"\n",
    "test_model = define_model(encoder_op_shape, vocab_size, max_length)\n",
    "test_model.load_weights(vgg_decoder_path)\n",
    "beam_length = 1\n",
    "get_BLEU(model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.288492\n",
      "BLEU-2: 0.102407\n",
      "BLEU-3: 0.045346\n",
      "BLEU-4: 0.006545\n"
     ]
    }
   ],
   "source": [
    "# Beam length 2\n",
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by Capsule Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "features = 'features_capsnet.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "test_features = {image_id: np.expand_dims(feat, axis=0) for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, None, 320)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, None, 320)    0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 34, 256)      2013184     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, None, 256)    82176       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 34, 256)      0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 256), (None, 525312      dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 34, 256), (N 525312      dropout_6[0][0]                  \n",
      "                                                                 lstm_5[0][1]                     \n",
      "                                                                 lstm_5[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "non_masking_3 (NonMasking)      (None, 34, 256)      0           lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 8704)         0           non_masking_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          2228480     flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 7864)         2021048     dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,395,512\n",
      "Trainable params: 7,395,512\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "BLEU-1: 0.470296\n",
      "BLEU-2: 0.220183\n",
      "BLEU-3: 0.152471\n",
      "BLEU-4: 0.068897\n"
     ]
    }
   ],
   "source": [
    "capsnet_decoder_path = \"model-ep003-loss3.643-val_loss3.936_CapsNet_init.h5\"\n",
    "beam_length = 1\n",
    "test_model = define_model(encoder_op_shape, vocab_size, max_length)\n",
    "test_model.load_weights(capsnet_decoder_path)\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.375935\n",
      "BLEU-2: 0.124800\n",
      "BLEU-3: 0.057325\n",
      "BLEU-4: 0.011984\n"
     ]
    }
   ],
   "source": [
    "# Beam length 2\n",
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model, arch, image_path):\n",
    "    \"\"\"\n",
    "    Description: Extract features for a given image\n",
    "    :model: The Encoder model\n",
    "    :arch: The arch type\n",
    "    :image_path: Path to the image\n",
    "    \"\"\"\n",
    "    feature = None\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "    try:\n",
    "        image = load_img(image_path, target_size=target_size)\n",
    "    except Exception as e:\n",
    "        print('{} could not be opened. Skipping\\n {}'.format(image_path,e))\n",
    "        return None\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    if arch=='capsnet':\n",
    "        feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "    else:\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'capsnet'\n",
    "encoder_model = initiate_encoder(arch)\n",
    "# Extract features of the image\n",
    "image_path = r'C:\\Users\\jayde\\OneDrive\\Desktop\\247778426_fd59734130.jpg'\n",
    "photo_feature = extract_feature(encoder_model, arch, image_path)\n",
    "max_length = 34\n",
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# Load the decoder model\n",
    "decoder_path = r''\n",
    "test_model = model.load_weights(decoder_path)\n",
    "# Beam Search length\n",
    "beam_length = 1\n",
    "print(generate_desc_beam_search(test_model, tokenizer, photo_feature, max_length, beam_length))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_gpu",
   "language": "python",
   "name": "keras_tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

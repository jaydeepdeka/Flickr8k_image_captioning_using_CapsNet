{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from utils import model_utils\n",
    "import os\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump, load\n",
    "import string\n",
    "\n",
    "# Decoder model imports\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_encoder(arch='capsnet'):\n",
    "    if arch=='capsnet':\n",
    "        encoder_model = model_utils.load_DeepCapsNet(input_shape=(64,64,3), n_class=10, routings=3, \\\n",
    "                        weights=r'D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\weights\\deep_caps_best_weights.h5')\n",
    "    else:\n",
    "        encoder_model = model_utils.load_VGG()\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_using_caps(model, directory, arch, path):\n",
    "    \"\"\"\n",
    "        Description: Function to extract features through the model\n",
    "        :model: The model object\n",
    "        :directory: Path of the directory of images\n",
    "        :path: Path to save the file\n",
    "    \"\"\"\n",
    "    features = dict()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    print('Feature extraction started')\n",
    "    for name in os.listdir(directory):\n",
    "        image_path = directory + '/' + name\n",
    "        target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "        try:\n",
    "            image = load_img(image_path, target_size=target_size)\n",
    "        except:\n",
    "            print('{} could not be opened. Skipping'.format(image_path))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # Extract the features from the last layer\n",
    "        if arch=='capsnet':\n",
    "            feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "        else:\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        # Populate the dictionary\n",
    "        features[image_id] = feature\n",
    "    path = os.path.join(path, 'features_{}.pkl'.format(arch))\n",
    "    dump(features, open(path, 'wb'))\n",
    "    print('Features extracted and stored at {}'.format(path))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Complete Capsule Architecture\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "VGG16 as feature extractor\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Features extracted and stored at D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet\\features_VGG.pkl\n"
     ]
    }
   ],
   "source": [
    "img_dir = r'D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k\\Flicker8k_Dataset'\n",
    "encoder_model = initiate_encoder(arch='VGG')\n",
    "extract_features_using_caps(, img_dir, 'VGG', 'D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k_image_captioning_using_CapsNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filename):\n",
    "    \"\"\"\n",
    "        Description: Generic function to read files and return contents\n",
    "        :filename: Path of the files\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fh:\n",
    "        content = fh.readlines()\n",
    "    return ''.join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Descriptions of the images\n",
    "def map_descriptions(desc_content):\n",
    "    \"\"\"\n",
    "        Description: Map the descriptions <image>:[description_list]\n",
    "    \"\"\"\n",
    "    # Each image contains 5 descriptions in the format\n",
    "    # <image_name>#<1-5> sentence\n",
    "    mapping = dict()\n",
    "    lines = list()\n",
    "    for line in desc_content.split('\\n'):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], ' '.join(tokens[1:])\n",
    "        image_id = image_id.split('.')[0]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        image_desc = image_desc.split()\n",
    "        image_desc = [word.lower() for word in image_desc]\n",
    "        image_desc = [w.translate(table) for w in image_desc]\n",
    "        image_desc = [word for word in image_desc if (len(word)>1 and word.isalpha())]\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # Append the list of the dictionary\n",
    "        mapping[image_id].append(' '.join(image_desc))\n",
    "        lines.append(image_id+' '+' '.join(image_desc))\n",
    "    # Write the files to a clean description file\n",
    "    with open('descriptions.txt', 'w') as fh:\n",
    "        fh.writelines('\\n'.join(lines))\n",
    "    return mapping\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Desciptions: 8092 \n",
      "Total Vocabulary: 8763\n"
     ]
    }
   ],
   "source": [
    "filename = r'D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k\\Flickr8k_text\\Flickr8k.token.txt'\n",
    "doc = read_files(filename)\n",
    "descriptions = map_descriptions(doc)\n",
    "print('Total Desciptions: %d ' % len(descriptions))\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Total Vocabulary: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training dataset: 6000\n"
     ]
    }
   ],
   "source": [
    "train_path = r'D:\\CapsuleNetwork_ImageCaptioning\\Flickr8k\\Flickr8k_text\\Flickr_8k.trainImages.txt'\n",
    "content = read_files(train_path)\n",
    "train_set = list()\n",
    "for line in content.split('\\n'):\n",
    "    if len(line) < 1:\n",
    "        continue\n",
    "    image_id = line.split('.')[0]\n",
    "    train_set.append(image_id)\n",
    "print(\"Size of Training dataset: {}\".format(len(set(train_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(descriptions):\n",
    "    all_desc = list()\n",
    "    for _, desc in descriptions.items():\n",
    "        [all_desc.append(d) for d in desc]\n",
    "    tokenizer = Tokenizer()\n",
    "    max_length = max([len(desc.split()) for desc in all_desc])\n",
    "    tokenizer.fit_on_texts(all_desc)\n",
    "    dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "    return tokenizer, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training descriptions\n",
    "train_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in train_set}\n",
    "# Tokenize the the train description\n",
    "train_tokenizer, max_length = create_tokenizer(train_desc)\n",
    "# Get the features of training dataset\n",
    "all_features = load(open(\"features_VGG.pkl\", 'rb'))\n",
    "train_features = {image_id:feat for image_id, feat in all_features.items() if image_id in train_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7577\n",
      "Maximum Legth: 32\n",
      "loaded photo features: 6000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: {}\\nMaximum Legth: {}\\nloaded photo features: {}'\\\n",
    "      .format(vocab_size, max_length, len(train_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(encoder_shape, vocab_size, max_length):\n",
    "    inputs1 = Input(shape=(encoder_shape,))\n",
    "    fe1 = Dropout(0.2)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.2)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 256)      1939712     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7577)         1947289     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,526,937\n",
      "Trainable params: 5,526,937\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model(encoder_model.layers[-1].output.shape.as_list()[1], vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 416s 69ms/step - loss: 5.1181\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 380s 63ms/step - loss: 4.1951\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 376s 63ms/step - loss: 3.8685\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 378s 63ms/step - loss: 3.6594\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 377s 63ms/step - loss: 3.5126\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 379s 63ms/step - loss: 3.4050\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 378s 63ms/step - loss: 3.3129\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 413s 69ms/step - loss: 3.2417\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 496s 83ms/step - loss: 3.1766\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 513s 85ms/step - loss: 3.1204\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps = len(train_desc)\n",
    "for i in range(epochs):\n",
    "    # create the data generator\n",
    "    generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "    # fit for one epoch\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # save model\n",
    "model.save('model_arch_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_gpu",
   "language": "python",
   "name": "keras_tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "UsageError: Line magic function `%inline` not found.\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "import os\n",
    "import string\n",
    "from utils import model_utils\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump, load\n",
    "\n",
    "# Decoder model imports\n",
    "import numpy as np\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array, prod\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#plot curve\n",
    "import matplotlib.pyplot as plt\n",
    "%inline matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File split for train, test and validation\n",
    "if not ((os.path.exists('train.pkl')) and (os.path.exists('valid.pkl')) and (os.path.exists('test.pkl'))):\n",
    "    train_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.devImages.txt'\n",
    "    test_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.trainImages.txt'\n",
    "    valid_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.testImages.txt'\n",
    "    paths = []\n",
    "    for path in [train_path, valid_path, test_path]:\n",
    "        with open(path, 'r') as fh:\n",
    "            paths = paths + fh.readlines()\n",
    "    sample_idx = np.random.choice(len(paths), size=int(len(paths)), replace=False)\n",
    "\n",
    "    # Train Set 80% of the data\n",
    "    train_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[:int(len(sample_idx)*.80)]]\n",
    "    valid_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.80):int(len(sample_idx)*.90)]]\n",
    "    test_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.90):]]\n",
    "    dump(train_set, open('train.pkl', 'wb'))\n",
    "    dump(valid_set, open('valid.pkl', 'wb'))\n",
    "    dump(test_set, open('test.pkl', 'wb'))\n",
    "else:\n",
    "    train_set = load(open('train.pkl', 'rb'))\n",
    "    valid_set = load(open('valid.pkl', 'rb')) \n",
    "    test_set = load(open('test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_encoder(arch='capsnet'):\n",
    "    \"\"\"\n",
    "        Description: Initiate the encoder \n",
    "        :arch: 'capsnet' or 'vgg'\n",
    "    \"\"\"\n",
    "    if arch=='capsnet':\n",
    "        encoder_model = model_utils.load_DeepCapsNet(input_shape=(64,64,3), n_class=10, routings=3, \\\n",
    "                        weights=r'..\\weights\\deep_caps_best_weights.h5')\n",
    "    else:\n",
    "        encoder_model = model_utils.load_VGG()\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, directory, arch, path):\n",
    "    \"\"\"\n",
    "        Description: Function to extract features through the model\n",
    "        :model: The model object\n",
    "        :directory: Path of the directory of images\n",
    "        :path: Path to save the file\n",
    "    \"\"\"\n",
    "    features = dict()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    print('Feature extraction started')\n",
    "    for name in os.listdir(directory):\n",
    "        image_path = directory + '/' + name\n",
    "        target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "        try:\n",
    "            image = load_img(image_path, target_size=target_size)\n",
    "        except:\n",
    "            print('{} could not be opened. Skipping'.format(image_path))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # Extract the features from the last layer\n",
    "        if arch=='capsnet':\n",
    "            feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "        else:\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        # Populate the dictionary\n",
    "        features[image_id] = feature\n",
    "    path = os.path.join(path, 'features_{}.pkl'.format(arch))\n",
    "    dump(features, open(path, 'wb'))\n",
    "    print('Features extracted and stored at {}'.format(path))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv_capsule_layer3d_2/stack:0\", shape=(5,), dtype=int32)\n",
      "Complete Capsule Architecture\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  3584        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 128)  512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "convert_to_caps_2 (ConvertToCap (None, 64, 64, 128,  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_16 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      convert_to_caps_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_18 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_19 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_17 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 32, 4 0           conv2d_caps_19[0][0]             \n",
      "                                                                 conv2d_caps_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_20 (Conv2DCaps)     (None, 16, 16, 32, 8 294912      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_22 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_23 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_21 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 32, 8 0           conv2d_caps_23[0][0]             \n",
      "                                                                 conv2d_caps_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_24 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_26 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_27 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_25 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 32, 8)  0           conv2d_caps_27[0][0]             \n",
      "                                                                 conv2d_caps_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_28 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_29 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_30 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_capsule_layer3d_2 (ConvCap (None, 4, 4, 32, 8)  18688       conv2d_caps_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 4, 4, 32, 8)  0           conv2d_caps_30[0][0]             \n",
      "                                                                 conv_capsule_layer3d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_3 (FlattenCaps)    (None, 512, 8)       0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_4 (FlattenCaps)    (None, 2048, 8)      0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2560, 8)      0           flatten_caps_3[0][0]             \n",
      "                                                                 flatten_caps_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "digit_caps (CapsuleLayer)       (None, 10, 32)       6553920     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_cid_3 (Mask_CID)           (None, 32)           0           digit_caps[0][0]                 \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (CapsToScalars)         (None, 10)           0           digit_caps[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 64, 64, 3)    67603       mask_cid_3[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 13,427,283\n",
      "Trainable params: 13,426,995\n",
      "Non-trainable params: 288\n",
      "__________________________________________________________________________________________________\n",
      "Capsule Network as feature extractor\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  3584        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 128)  512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "convert_to_caps_2 (ConvertToCap (None, 64, 64, 128,  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_16 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      convert_to_caps_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_18 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_19 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_17 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 32, 4 0           conv2d_caps_19[0][0]             \n",
      "                                                                 conv2d_caps_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_20 (Conv2DCaps)     (None, 16, 16, 32, 8 294912      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_22 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_23 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_21 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 32, 8 0           conv2d_caps_23[0][0]             \n",
      "                                                                 conv2d_caps_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_24 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_26 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_27 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_25 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 32, 8)  0           conv2d_caps_27[0][0]             \n",
      "                                                                 conv2d_caps_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_28 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_29 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_30 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_capsule_layer3d_2 (ConvCap (None, 4, 4, 32, 8)  18688       conv2d_caps_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 4, 4, 32, 8)  0           conv2d_caps_30[0][0]             \n",
      "                                                                 conv_capsule_layer3d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_3 (FlattenCaps)    (None, 512, 8)       0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_4 (FlattenCaps)    (None, 2048, 8)      0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2560, 8)      0           flatten_caps_3[0][0]             \n",
      "                                                                 flatten_caps_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "digit_caps (CapsuleLayer)       (None, 10, 32)       6553920     concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 13,359,680\n",
      "Trainable params: 13,359,424\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "Feature extraction started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted and stored at ..\\Flickr8k_image_captioning_using_CapsNet\\features_capsnet.pkl\n"
     ]
    }
   ],
   "source": [
    "img_dir = r'..\\Flickr8k\\Flicker8k_Dataset'\n",
    "arch = 'capsnet'\n",
    "encoder_model = initiate_encoder(arch=arch)\n",
    "if not os.path.exists('features_{}.pkl'.format(arch)):\n",
    "    extract_features(encoder_model, img_dir, arch, r'..\\Flickr8k_image_captioning_using_CapsNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filename):\n",
    "    \"\"\"\n",
    "        Description: Generic function to read files and return contents\n",
    "        :filename: Path of the files\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fh:\n",
    "        content = fh.readlines()\n",
    "    return ''.join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean descriptions of the images\n",
    "def map_descriptions(desc_content):\n",
    "    \"\"\"\n",
    "        Description: Map the descriptions <image>:[description_list]\n",
    "        :desc_content: File content\n",
    "    \"\"\"\n",
    "    # Each image contains 5 descriptions in the format\n",
    "    # <image_name>#<1-5> sentence\n",
    "    mapping = dict()\n",
    "    lines = list()\n",
    "    for line in desc_content.split('\\n'):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], ' '.join(tokens[1:])\n",
    "        image_id = image_id.split('.')[0]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        image_desc = image_desc.split()\n",
    "        image_desc = [word.lower() for word in image_desc]\n",
    "        image_desc = [w.translate(table) for w in image_desc]\n",
    "        image_desc = [word for word in image_desc if (len(word)>1 and word.isalpha())]\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # Append the list of the dictionary\n",
    "        clean_desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "        mapping[image_id].append(clean_desc)\n",
    "        lines.append(image_id+' '+clean_desc)\n",
    "    # Write the files to a clean description file\n",
    "    with open('descriptions.txt', 'w') as fh:\n",
    "        fh.writelines('\\n'.join(lines))\n",
    "    return mapping\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Desciptions: 8092 \n",
      "Total Vocabulary: 8765\n"
     ]
    }
   ],
   "source": [
    "filename = r'..\\Flickr8k\\Flickr8k_text\\Flickr8k.token.txt'\n",
    "doc = read_files(filename)\n",
    "descriptions = map_descriptions(doc)\n",
    "print('Total Desciptions: %d ' % len(descriptions))\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Total Vocabulary: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparaing Training Set\n",
    "* The dataset contains multiple files inside Flickr8k_text. The 8000 images are divided into:\n",
    "    * Training Set: 6000\n",
    "    * Validation Set: 1000\n",
    "    * Test Set: 1000\n",
    "* The images names for the training names are stored in the Flickr_8k.trainImages.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training dataset: 6400\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Training dataset: {}\".format(len(set(train_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"\n",
    "    Description: Tokenize the description\n",
    "    \"\"\"\n",
    "    all_desc = list()\n",
    "    for _, desc in descriptions.items():\n",
    "        [all_desc.append(d) for d in desc]\n",
    "    tokenizer = Tokenizer()\n",
    "    max_length = max([len(desc.split()) for desc in all_desc])\n",
    "    tokenizer.fit_on_texts(all_desc)\n",
    "    dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "    return tokenizer, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training descriptions\n",
    "train_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in train_set}\n",
    "# Tokenize the the train description\n",
    "train_tokenizer, max_length = create_tokenizer(train_desc)\n",
    "# Get the features of training dataset\n",
    "feature_path = \"features_{}.pkl\".format(arch)\n",
    "# feature_path = \"features_VGG.pkl\"\n",
    "all_features = load(open(feature_path, 'rb'))\n",
    "train_features = {image_id:feat for image_id, feat in all_features.items() if image_id in train_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7864\n",
      "Maximum Legth: 34\n",
      "loaded photo features: 6400\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(train_tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: {}\\nMaximum Legth: {}\\nloaded photo features: {}'\\\n",
    "      .format(vocab_size, max_length, len(train_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = {image_id:feat for image_id, feat in all_features.items() if image_id in valid_set}\n",
    "valid_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in valid_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(encoder_shape, vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    Description: Define the decoder model\n",
    "    :encoder_shape: Input from the image feature\n",
    "    :vocab_size: \n",
    "    :max_length: maximum length of the description\n",
    "    \"\"\"\n",
    "    inputs1 = Input(shape=(encoder_shape,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    # Add layer\n",
    "    addlayer = add([fe2, se2])\n",
    "    # decoder model\n",
    "    decoder1 = LSTM(256)(addlayer)\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 320)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 320)          0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 34, 256)      2013184     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          82176       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 34, 256)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 34, 256)      0           dense_6[0][0]                    \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          525312      add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          65792       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 7864)         2021048     dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,707,512\n",
      "Trainable params: 4,707,512\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_op_shape = prod(list(filter(None, encoder_model.layers[-1].output.shape.as_list())))\n",
    "model = define_model(encoder_op_shape, vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    \"\"\"\n",
    "    Description: Create seqences for input <photo>, <description>, <output>\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 645s 101ms/step - loss: 5.1305 - val_loss: 4.3998\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.39979, saving model to model-ep001-loss5.147-val_loss4.400_VGG_par.h5\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 642s 100ms/step - loss: 4.1739 - val_loss: 4.0906\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.39979 to 4.09058, saving model to model-ep002-loss4.194-val_loss4.091_VGG_par.h5\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 625s 98ms/step - loss: 3.8580 - val_loss: 3.9840\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.09058 to 3.98403, saving model to model-ep003-loss3.878-val_loss3.984_VGG_par.h5\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 570s 89ms/step - loss: 3.6827 - val_loss: 3.9413\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.98403 to 3.94134, saving model to model-ep004-loss3.703-val_loss3.941_VGG_par.h5\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 516s 81ms/step - loss: 3.5700 - val_loss: 3.9078\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.94134 to 3.90776, saving model to model-ep005-loss3.591-val_loss3.908_VGG_par.h5\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 513s 80ms/step - loss: 3.4911 - val_loss: 3.9041\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.90776 to 3.90409, saving model to model-ep006-loss3.513-val_loss3.904_VGG_par.h5\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 516s 81ms/step - loss: 3.4244 - val_loss: 3.8810\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.90409 to 3.88098, saving model to model-ep007-loss3.446-val_loss3.881_VGG_par.h5\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 522s 82ms/step - loss: 3.3811 - val_loss: 3.8849\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.88098\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 510s 80ms/step - loss: 3.3400 - val_loss: 3.8976\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.88098\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 517s 81ms/step - loss: 3.3071 - val_loss: 3.9148\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.88098\n"
     ]
    }
   ],
   "source": [
    "# Train model for VGG\n",
    "epochs = 10\n",
    "train_steps = len(train_desc)\n",
    "val_steps = len(valid_desc)\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}_VGG_par.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "train_generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "valid_generator = data_generator(valid_desc, valid_features, train_tokenizer, max_length, vocab_size)\n",
    "# fit for one epoch\n",
    "history = model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, verbose=1, validation_data=valid_generator,\\\n",
    "                    validation_steps=val_steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU5b348c83+74HEhIgbMoS9qiorYK4ACriDi7V21avrW2t7VVrf+219bb32tZ7azfbulYrbsXigoKighYVJMgWQGSXhCWQEJKQhGzf3x/nJEzCJExgJpPl+369zmtmnvOcme8ZyPnO8zznPEdUFWOMMaa1kGAHYIwxpmuyBGGMMcYrSxDGGGO8sgRhjDHGK0sQxhhjvLIEYYwxxitLEKbHEpGFInJLsOMIFBGZLCKFwY4DQERuFZFlwY7D+JclCONXIrJTRKpFpFJE9ovI0yIS5+f3v9CXuqo6XVWfOcXP+5mIPNfO+rdF5EEv5VeIyD4RCXNf54nIAhE5JCJlIrJRRH4pIske22SKyOMissf9/raLyN9EZPip7ENXIyJLReSbwY7DnJglCBMIl6tqHDABOAP4SUffoOnA2g38DbhZRKRV+c3AXFWtF5FzgKXAR8BwVU0CpgH1wFgAEUkFPgZigK8C8Tjf3wfARYHfDWO8UFVbbPHbAuwELvR4/Rtggfv834BNQAWwHfh3j3qTgULgPmAf8PcTvT9wK7AMeBg4BOwApnvUXQp80+P1193PPwS8DQz0WDcKWAyUAvuBH+McxGuBOqASWOslnmjgMHCeR1kyUAOMdV8vA/5wgu/tF8BaIKQD3/VkoNDj9Qh3n8uADcBMj3UzgI3ud18E/IdbngYscLcpBf7VVgyAAt9z/+0Ouv+2IZ7/Fh51zwFWut/NSuAct/yXQIP7/VQCfwz2/1lb2l6sBWECRkT64xyYVrtFxcBlQAJOsvitiEzw2CQDSAEGArf7+DFnAZtxDnS/Bp708mseEZmFc9C/CkjHORC+4K6LB94FFgH9gKHAe6q6CPhv4CVVjVPVsa3fV1WrgZeBr3kUXwd8rqprRSQWOBt45QT7cSEwX1Ubfdzv1vsXDrwBvAP0Ab4LzBWR090qT+Ik5HggF3jfLf8hTmJOB/rifEftzb9zJZCH07q5Aifpto4lBXgT+D2QCvwf8KaIpKrq/8P57r/jfqffOZn9NZ3DEoQJhFdFpAznl/MHOAdZVPVNVd2mjg9wDmZf9diuEXhAVY+6B15f7FLVx1W1AXgGyMQ50LX278D/qOomVa13YxonIgNxktY+Vf1fVa1R1QpVXdGB/X0GuFZEot3XX3PLwGlNhOC0igAQkV+74xBHRKSp+y2tVZ2Zbp0KEXnHhxgmAXHAQ6paq6rv47QM5rjr64CRIpKgqodU9TOP8kyc1lSdqv5LVdtLEL9S1VJV/RJ4xOP9PV0KbFHVv6tqvaq+AHwOXO7DfpguxBKECYRZqpqkqgNV9dtNB3sRmS4iy0Wk1E0gM3AOjE0OqGpN0wv3LKRKd7mxjc9qPqiqapX71Nug+EDgd+5Bt6k7RYAsoD+w7WR3VlWXAQeAK0RkMM64y/Pu6kM4iS/To/696oxDzAeaxlpKWtV53a1zNxDhQxj9gN2tWiC7cPYP4Gqc73uXiHwgIme75b8BtgLvuIPiPzrB5+xu9f792ohlV6syz1hMN2EJwnQKEYnE6WZ5GOjrHvzewjlIN2nxy1Wds5Di3GXuKYawG6eLJcljiVbVj911Q9rYztfpjp/FaTncDLyjqvvdfTgCrMDp2mrPe8AsETnZv8k9QP9W2w/AGW9AVVeq6hU43U+v4nSL4baWfqiqg3F+4f9ARKa28zn9W73/njZiGdiqrDkWfP9OTZBZgjCdJQKIxPmlXS8i04GLO/Hz/wLcLyKjAEQkUUSuddctADJE5PsiEiki8SJylrtuP5Djw4H7WZxxhNs41r3U5F7g6yLyIxHp435+NjDIo87/4XRH/V1EhogjHhjn4/6tAI4A94pIuIhMxjngvygiESJyo4gkqmodUI4zUIyIXCYiQ91xm6byhnY+5x4RSXbHl+4CXvJS5y3gNBG5QUTCROR6YCTO9wzOdzrYx/0yQWQJwnQKVa3AOQPmZZxulxuA1zvx8+cDv8I5YJYDBcB0j9guwjmg7gO2AFPcTf/hPpaIyGe0QVV34pymGkur/XK7oC4AzgO+cLu4FuGccfQHt85BnHGEGpyxmwpgDc7prt/yYf9qgZnuPh0EHgW+pqqfu1VuBna6+34HcJNbPgxngL4S+AR4VFWXtvNRrwGr3NjexBn8bh1LCc64zg9xus7uBS5z9xHgd8A17jUhvz/RvpngkfbHo4zpvkTkQ+AJVX022LH0BCKiwDBV3RrsWEznsBaE6ZFEJAanG2NHsGMxpruyBGF6HLeffx/OKbY2P5AxJ8m6mIwxxnhlLQhjjDFedZcJ0U4oLS1Nc3Jygh2GMcZ0K6tWrTqoqune1vWYBJGTk0N+fn6wwzDGmG5FRFpf9d7MupiMMcZ4ZQnCGGOMV5YgjDHGeNVjxiCMMaaj6urqKCwspKam5sSVu7moqCiys7MJDw/3eRtLEMaYXquwsJD4+HhycnLwcp+pHkNVKSkpobCwkEGDBp14A5d1MRljeq2amhpSU1N7dHIAEBFSU1M73FKyBGGM6dV6enJocjL72esTRFlVLY+8+wUb95QHOxRjjOlSen2CEIQ/LdnKa2uKTlzZGGP8qKysjEcffbTD282YMYOysrIARNRSr08QiTHhnDMkjbcK9mITFxpjOlNbCaKhob2b+sFbb71FUlJSoMJq1usTBMCM0RnsLq1mg3UzGWM60Y9+9CO2bdvGuHHjOOOMM5gyZQo33HADo0ePBmDWrFlMnDiRUaNG8dhjjzVvl5OTw8GDB9m5cycjRozgtttuY9SoUVx88cVUV1f7LT47zRW4aGQGP55fwMKCveRmJQY7HGNMEPz8jQ1+H4sc2S+BBy4f1eb6hx56iIKCAtasWcPSpUu59NJLKSgoaD4V9amnniIlJYXq6mrOOOMMrr76alJTU1u8x5YtW3jhhRd4/PHHue6663jllVe46aabvH1ch1kLAkiJjWDS4BQWrt9n3UzGmKA588wzW1yn8Pvf/56xY8cyadIkdu/ezZYtW47bZtCgQYwbNw6AiRMnsnPnTr/FYy0I17TcTH76agFf7K/k9Iz4YIdjjOlk7f3S7yyxsbHNz5cuXcq7777LJ598QkxMDJMnT/Z6HUNkZGTz89DQUL92MVkLwnXJqL6IwMKCvcEOxRjTS8THx1NRUeF13eHDh0lOTiYmJobPP/+c5cuXd3J0liCa9YmP4oyBTjeTMcZ0htTUVM4991xyc3O55557WqybNm0a9fX1jBkzhp/+9KdMmjSp0+PrMfekzsvL01O9YdDTH+3g529s5L0fns+Q9Dg/RWaM6ao2bdrEiBEjgh1Gp/G2vyKySlXzvNW3FoSHabkZACwqsFaEMcZYgvCQmRjN+AFJNg5hjDFYgjjO9NwMCorK+bKkKtihGGNMUFmCaGV6biYAizZYK8IY07sFNEGIyE4RWS8ia0TkuBFkcfxeRLaKyDoRmeCx7hYR2eIutwQyTk/9U2LIzUrgLTubyRjTy3VGC2KKqo5rY5R8OjDMXW4H/gwgIinAA8BZwJnAAyKS3AmxOkHlZrJmdxl7yvx3wYkxxnQ3we5iugJ4Vh3LgSQRyQQuARaraqmqHgIWA9M6K6jpdjaTMaYLiotzTr/fs2cP11xzjdc6kydP5lRP+W8S6AShwDsiskpEbveyPgvY7fG60C1rq7wFEbldRPJFJP/AgQN+C3pwehzDM+ItQRhjuqR+/foxb968gH9OoBPEuao6Aacr6U4ROa/Vem/3wNN2ylsWqD6mqnmqmpeenn7q0XqYlpvByl2lFFd07B6uxhjjq/vuu6/F/SB+9rOf8fOf/5ypU6cyYcIERo8ezWuvvXbcdjt37iQ3NxeA6upqZs+ezZgxY7j++uu7z3TfqrrHfSwWkfk44wkfelQpBPp7vM4G9rjlk1uVLw1krK3NGJ3JI+9u4e0N+7l50sDO/GhjTDAs/BHsW+/f98wYDdMfanP17Nmz+f73v8+3v/1tAF5++WUWLVrE3XffTUJCAgcPHmTSpEnMnDmzzXtK//nPfyYmJoZ169axbt06JkyY4LXeyQhYC0JEYkUkvuk5cDFQ0Kra68DX3LOZJgGHVXUv8DZwsYgku4PTF7tlnWZYnzgGp8eycL2d7mqMCYzx48dTXFzMnj17WLt2LcnJyWRmZvLjH/+YMWPGcOGFF1JUVMT+/fvbfI8PP/yw+f4PY8aMYcyYMX6LL5AtiL7AfDfrhQHPq+oiEbkDQFX/ArwFzAC2AlXAv7nrSkXkv4CV7ns9qKqlAYz1OCLCjNxM/vzBNkoqj5IaF3nijYwx3Vc7v/QD6ZprrmHevHns27eP2bNnM3fuXA4cOMCqVasIDw8nJyfH6zTfntpqXZyqgLUgVHW7qo51l1Gq+ku3/C9ucsA9e+lOVR2iqqNVNd9j+6dUdai7PB2oONszLTeDhkZl8ca2s7cxxpyK2bNn8+KLLzJv3jyuueYaDh8+TJ8+fQgPD2fJkiXs2rWr3e3PO+885s6dC0BBQQHr1q3zW2zBPs21SxvVL4EBKTEstLOZjDEBMmrUKCoqKsjKyiIzM5Mbb7yR/Px88vLymDt3LsOHD293+29961tUVlYyZswYfv3rX3PmmWf6LTa7o1w7RITpuRk8uWwHh6vqSIwJD3ZIxpgeaP36Y4PjaWlpfPLJJ17rVVZWApCTk0NBgTOkGx0dzYsvvhiQuKwFcQLTR2dS36i8u8m6mYwxvYsliBMYm51Iv8QomwLcGNPrWII4ARFhWm4mH245SEVNXbDDMcb4WU+5q+aJnMx+WoLwwfTRGdTWN/L+58XBDsUY40dRUVGUlJT0+CShqpSUlBAVFdWh7WyQ2gcTByTTJz6SRQX7uGLccVNCGWO6qezsbAoLC/HnXG5dVVRUFNnZ2R3axhKED0JChEtGZfCPVbupqq0nJsK+NmN6gvDwcAYNGhTsMLos62Ly0fTRGdTUNfLB5p7/S8MYY8AShM/OzEkhJTaCt+yiOWNML2EJwkdhoSFcMqov72/aT01dQ7DDMcaYgLME0QHTcjM5UtvAv7YcDHYoxhgTcJYgOuCcIakkRofbRXPGmF7BEkQHhIeGcOGIvizeuJ/a+sZgh2OMMQFlCaKDZozOoKKmno+3WTeTMaZnswTRQV8ZlkZcZBgL19vZTMaYns0SRAdFhoUydUQf3tm4j/oG62YyxvRcliBOwvTcDA5V1bFiR6feBdUYYzpVwBOEiISKyGoRWeBl3W9FZI27fCEiZR7rGjzWvR7oODvi/NP6EB0eamczGWN6tM6YVOguYBOQ0HqFqt7d9FxEvguM91hdrarjAh9ex0VHhDJleDqLCvbz85m5hIYE5obhxhgTTAFtQYhINnAp8IQP1ecALwQyHn+anpvJwcqjrNp1KNihGGNMQAS6i+kR4F6g3dFcERkIDALe9yiOEpF8EVkuIrPa2O52t05+Z0/XO2V4HyLCQnhrvXUzGWN6poAlCBG5DChW1VU+VJ8NzFNVz0mOBqhqHnAD8IiIDGm9kao+pqp5qpqXnp7un8B9FBcZxvmnpfP2hn00Nvbsm40YY3qnQLYgzgVmishO4EXgAhF5ro26s2nVvaSqe9zH7cBSWo5PdAnTczPYe7iGNYVlJ65sjDHdTMAShKrer6rZqpqDkwDeV9WbWtcTkdOBZOATj7JkEYl0n6fhJJuNgYr1ZE0d0ZfwUGGRTQFujOmBOv06CBF5UERmehTNAV7UljeFHQHki8haYAnwkKp2uQSRGB3OuUPTeGv93h5/T1tjTO/TKffOVNWlON1EqOp/tlr3My/1PwZGd0Jop2xGbib3vrKODXvKyc1KDHY4xhjjN3Yl9Sm6aGRfQkPEzmYyxvQ4liBOUXJsBGcPTmVhwT7rZjLG9CiWIPxgWm4GOw4eYfP+imCHYowxfmMJwg8uGZWBCDYFuDGmR7EE4Qfp8ZGckZNik/cZY3oUSxB+MiM3gy/2V7K1uDLYoRhjjF9YgvCTabmZACyyVoQxpoewBOEnGYlRTBiQxEK7qtoY00NYgvCj6bmZbNhTzpclVcEOxRhjTpklCD+alpsBYIPVxpgewRKEH/VPiWF0ViJvWTeTMaYHsAThZ9NHZ7B2dxlFZdXBDsUYY06JJQg/m958NpO1Iowx3ZslCD8blBbL8Ix4O93VGNPtWYIIgOm5meTvOkRxeU2wQzHGmJNmCSIAZozOQBXe3mDdTMaY7ssSRAAM6xvPkPRY3rLJ+4wx3ZgliACZMTqTFTtKKKk8GuxQjDHmpAQ8QYhIqIisFpEFXtbdKiIHRGSNu3zTY90tIrLFXW4JdJz+Ni03g0aFdzbuD3YoxhhzUjqjBXEXsKmd9S+p6jh3eQJARFKAB4CzgDOBB0QkOfCh+s/IzAQGpsbY3EzGmG4roAlCRLKBS4EnOrjpJcBiVS1V1UPAYmCav+MLJBFhWm4GH289yOGqumCHY4wxHRboFsQjwL1AYzt1rhaRdSIyT0T6u2VZwG6POoVuWbcyIzeT+kZl8SbrZjLGdD8BSxAichlQrKqr2qn2BpCjqmOAd4Fnmjb3Ule9fMbtIpIvIvkHDhw45Zj9bUx2IllJ0SxcbxfNGWO6n0C2IM4FZorITuBF4AIRec6zgqqWqGrTaT6PAxPd54VAf4+q2cCe1h+gqo+pap6q5qWnp/s7/lPW1M30ry0HqaixbiZjTPcSsAShqveraraq5gCzgfdV9SbPOiKS6fFyJscGs98GLhaRZHdw+mK3rNuZnptBbUMj739eHOxQjDGmQzr9OggReVBEZrovvyciG0RkLfA94FYAVS0F/gtY6S4PumXdzoQByfSJj2ShXTRnjOlmwjrjQ1R1KbDUff6fHuX3A/e3sc1TwFOdEF5AhYQ43Uwv5++mqraemIhO+cqNMeaU2ZXUnWB6biY1dY0s3dz1BtKNMaYtliA6wZmDUkiNjeAtO5vJGNONWILoBKEhwsWjMljyeTE1dQ3BDscYY3xiCaKTTM/N4EhtAx9+Yd1MxpjuwRIEwBdvQ31gZ109e0gqidHhditSY0y3YQniwBfw/PXw0k1QF7g7wIWHhnDRyL4s3rSf2vr2Zh4xxpiuwRJE+mlw+SOw5Z2AJ4kZozOoqKnno20HA/YZxhjjL5YgACbeCpf/HrYuhpduDFiSOHdoGvGRYTY3kzGmW7AE0WTiLTDzD7D1PXjxhoAkiciwUKaO6MM7G/dT12DdTMaYrs0ShKcJX3OSxLb34cU5UFft94+YlptJWVUdK7Z3y5lDjDG9iCWI1ibcDFf8CbYtgRf8nyQmn55OTEQoCwusm8kY07VZgvBm/I0w61HYvhRemA21VX5766jwUKac3oe3N+yjofG4W1wYY0yXYQmiLeNugFl/hu0f+D1JTB+dwcHKWvJ3WjeTMabrsgTRnnFz4Mq/wI4P4YXr/ZYkppzeh8iwEBbaRXPGmC7MEsSJjJ0NV/4Vdi6D56+D2iOn/JaxkWGcf1o6iwr20WjdTMaYLsoShC/GXu8kiV0fOVdd+yFJTB+dwb7yGtYUlvkhQGOM8T9LEL4acx1c+ZiTJOaeekti6oi+hIeKXTRnjOmyfEoQInKXiCSI40kR+UxELg50cF3OmGvhqsfhy49h7rVwtPKk3yohKpyvDE1jYcE+VK2byRjT9fjagvi6qpYDFwPpwL8BDwUsqq5s9DVw9RPw5fJTThLTR2dSeKiagqJyPwZojDH+4WuCEPdxBvC0qq71KGt/Q5FQEVktIgu8rPuBiGwUkXUi8p6IDPRY1yAia9zldR/j7By5VztJYvcKmHsNHK04qbe5aERfQkPELpozxnRJviaIVSLyDk6CeFtE4gFfJxO6C9jUxrrVQJ6qjgHmAb/2WFetquPcZaaPn9V5cq+Ca56E3Z/CcyeXJJJjIzhnSKp1MxljuiRfE8Q3gB8BZ6hqFRCO083ULhHJBi4FnvC2XlWXuO8HsBzI9jGermHUlXDNU1C4Ep67Gmo63lU0LTeDHQePsHn/ybVCjDEmUHxNEGcDm1W1TERuAn4CHPZhu0eAe/GttfENYKHH6ygRyReR5SIyy9sGInK7Wyf/wIEg3cpz1Cy49mkoWnVSSeLikRmECLy13i6aM8Z0Lb4miD8DVSIyFueAvwt4tr0NROQyoFhVV53ozd2kkwf8xqN4gKrmATcAj4jIkNbbqepjqpqnqnnp6ek+7koAjLwCrnka9nwGz10FNb7kTkd6fCRn5KSwyMYhjDFdjK8Jol6dTvIrgN+p6u+A+BNscy4wU0R2Ai8CF4jIc60riciFwP8DZqpq842hVXWP+7gdWAqM9zHW4Bg5E679G+xZDX/vWJKYMTqTL/ZXsrX45M+IMsYYf/M1QVSIyP3AzcCbIhKKMw7RJlW9X1WzVTUHmA28r6o3edYRkfHAX3GSQ7FHebKIRLrP03CSzUYfYw2eEZfDtc/A3jXw9yuh2rerpC8ZlQFgrQhjTJfia4K4HjiKcz3EPiCLlt1BPhORB0Wk6ayk3wBxwD9anc46AsgXkbXAEuAhVe36CQJgxGVw3bOwd53PSSIjMYqJA5NtHMIY06WIr6dXikhf4Az35aeev/i7gry8PM3Pzw92GMd8/ha8/DXIyIWb50N0crvVn/jXdn7x5iZevfNcxvVP6qQgjTG9nYiscsd7j+PrVBvXAZ8C1wLXAStE5Br/hdgDDZ8B1/8d9hXAs7Og+lC71a8cn0VWUjS3Pv0pG/fYldXGmODztYvp/+FcA3GLqn4NOBP4aeDC6iFOnw7XPwfFG+HZK6Cq7RsEpcZF8vxtZxEdHspNT65g8z67LsIYE1y+JoiQVl1KJR3Ytnc7fZqbJDadMEkMTI3l+dsmER4q3PD4crbYxXPGmCDy9SC/SETeFpFbReRW4E3grcCF1cOcdglcPxcOfH7CJDEozUkSISHCnMdX2Kmvxpig8SlBqOo9wGPAGGAs8Jiq3hfIwHqc0y6G2S/Agc3w7Mx2k8SQ9DheuO0sQLnh8eXsOHjqNygyxpiO8rmbSFVfUdUfqOrdqjo/kEH1WMMuhDnPw4Ev4JmZcKSkzapD+8Tz/G2TaGhU5jy2nF0lliSMMZ2r3QQhIhUiUu5lqRARO9XmZAx1k8TBL5yWRDtJ4rS+8cy97SyO1jcw57Hl7C6tarOuMcb4W7sJQlXjVTXByxKvqgmdFWSPM/RCmPMClGyFZy6HIwfbrDo8I4G535xEVV0Dsx9bTuEhSxLGmM5hZyIFy9CpMOdFKN3mdje1nSRG9kvguW+cRUVNHXMeX86esupODNQY01tZggimIVM8ksTlUNn2lOW5WYk8982zKKtyksS+wzWdGKgxpjeyBBFsQ6bADS9B6Q545jLYtADqvB/8x2Qn8ezXz6SkspY5jy+nuNyShDEmcCxBdAWDJ8ONL0NVCbx0Izw8DObfAVsWQ0Ndi6rjByTzzNfPoLi8htmPL6e4wpKEMSYwfJ6sr6vrcpP1nYyGetjxART8Eza9AUcPO5P8jbwCRl0FOV+BkFAAVu4s5ZanPqVfUjQv3j6JtLjIIAdvjOmO2puszxJEV1V/FLa9DwWvODPD1h2BuL4wchbkXgXZZ7J85yFuffpTBqbE8vxtZ5FqScIY00GWILq72irY8rbTsvjibWg4CgnZkHslaxOnct0b1QxKi+OF2yaRHBsR7GiNMd2IJYiepKYcNi90Whbb3ofGOqrjBvB0+UTWJ03lf+64nqQYSxLGGN9Yguipqkrh8wVQ8Aq640NEG9kVOpC+Z88haty1kDY02BEaY7o4SxC9QWUxm5c8R/nKlzgj5HOnLHMs5F4No66EpAHBjc8Y0yWd8h3lTvHDQ0VktYgs8LIuUkReEpGtIrJCRHI81t3vlm8WkUsCHWe3F9eH0y//AWWzX+crtX/kqbjbaCAUFv8nPDIanrgIlv8FKuy+18YY33TGdRB3AZvaWPcN4JCqDgV+C/wKQERGArOBUcA04FERCe2EWLu9i0b25Sc3XMQvSy/g+sZfUnXHKpj6n1BXDYvug/8dDn+7DPKfaneiQGOMCWiCEJFs4FLgiTaqXAE84z6fB0wVEXHLX1TVo6q6A9iKc5tT44NpuRn8fvZ4Vu8u49bXDlB11l3wrWVw56dw/n1OK2LB3c4Fec9dDavnQnVZsMM2xnQxgW5BPALcCzS2sT4L2A2gqvXAYSDVs9xV6Ja1ICK3i0i+iOQfOND2PEa90aVjMvnt9ePI31nKN/6WT3VtA6SfDlPuh++shDuWwbnfg4Nb4LVvO8nihRtg/Tw4anexM8ZAWKDeWEQuA4pVdZWITG6rmpcybae8ZYHqYzh3uiMvL69njLb70cyx/WhsVO5+eQ23PZvPE7fkERUeCiKQMdpZpj4ARZ85p81u+CdsfhMkBBKzIXkQpAxyHwcfex4ZF+xdM8Z0goAlCOBcYKaIzACigAQReU5Vb/KoUwj0BwpFJAxIBEo9yptkA3sCGGuPNWt8FvWNyj3z1nL731fx2M0TnSTRRASyJzrLxb+A3cth2xIo3Q6HdsDG16G61e1RY9OPJY+UwS0TSWya857GmG6vU05zdVsQ/6Gql7UqvxMYrap3iMhs4CpVvU5ERgHP44w79APeA4apakNbn9HrT3M9gZdX7ubeV9Yx5fR0/nLzRCLDOjDmX3PYmW320A7nsXQ7HNrpPC8vokXjLiLOI3kMapk8ErOb55IyxnQN7Z3mGsgWRFvBPAjkq+rrwJPA30VkK07LYTaAqm4QkZeBjUA9cGd7ycGc2HVn9Ke+Ufnx/PV8+7nP+PNNE4kI83EIKioR+o1zltbqaqDsy1bJYwcUb4IvFkFD7bG6IeHO9Rie3VVNj8k5EB7ll301xviHXSjXy/x9+S5++moBF43sy6M3TiA8NIDnKTQ2QPkeN3lsb9kKObQTjra6rXlClps0co4ljYR+zhKfCWE2GaEx/talWhAmuG6eNMs/mHkAABbFSURBVJCGhkZ+9sZGvvfCan4/Z3zgkkRIKCT1d5ZB57Vcp+pMFeIteWxZDJX7j3+/mLRjCaM5cbR6HRkfmH0xpheyBNEL3XruIOoblV+8uYnvv7SG310/jrBAtiS8EYHYVGfJ9vLjpfYIlO2Gij1OK8RzOVwEhSudGyy1FpngtDYS+jktkgSP5/GZzmNMig2kG+MDSxC91De/OphGVf77rc8JFeG3148jNKQLHTQjYqHPcGdpS101VOyF8r1u8ihyXxc5r7dthsp9oK0uwwmNdBNH1rHuq6bnTUtcXxtQN11DQx0crXC6ZI9WOEtNecuymFSYcLPfP9oSRC92+3lDaGiEXy36nLAQ4TfXju1aSeJEwqPdAe/BbddpqHe6qzwTh+dSuNJ59BxMB5BQJ0kk9IOoBOd1SKj7GNLqdXvlIV7qnUR5SKjTfRad4rSAolOc19YS6rrqa92D+OFjB/bmpbzVgb7i+CTQ9Lzeh9sKZ020BGH871uTh9DQ2MjD73xBSIjw66vHENKdksSJhIZBYpaz4HUczh0PKfFIHE0tEfd5TTlogzPoro3uY4PHY2Or1+2UH3+958kLCXduSduUMGJSvLxu/ZgMoeH+i6EnaKhzujTrqqGuyl2qjy+rrfKyvsrjV32rRNBw9MSfLaHOD5DIeKd7NDLe+WGSOtQtc8ub68S3LG/aJjw6IF+NJQjDdy4YRn2j8si7WwgLEf77ytE9K0mciIhzgV9sGmSOCexnqXpJJG0lHo/yxnrnF2VVqXPh4nGPh5zB/qbXrVtEniIT3ESS2nYSaV0eEXvqrRWv+17vJZHWe/lO6o//Pjp8MG+9vtq5lW9jfQd3RCA8xjkoR8RAZKJzkI7PdKazaesg3qIs3jnoh0V16VagJQgDwF1Th9HQqPzh/a0A/GzmqJZXXBv/EHFaNYH801N1DojHJZJDXhJLiTMfV/Wh40879hQa4SSK6CRAjk9k3pJb67LWY0GBEhLuHLjDPZYId4lNP3Zg91zvrSzCLQ+Pdde7j138oO5PliAMACLCDy46jYZG5dGl2/jwiwPcO204M8f2612tiZ5AxJkvKzKuYzeKaqhrI4l4PNa4s/42j42EHT/+0lzmMabia5m45ceN67QqCwnzOHC3OtBbF5rf2IVy5jjLt5fwizc3UlBUztjsRH5y2UjOyEkJdljGmAAI6h3lTPczaXAqr9/5Ff7vurHsLz/KtX/5hG89t4pdJUeCHZoxphNZF5PxKiREuGpCNtNzM3n8X9v5ywfbeHfTfm49J4fvTBlGYow1443p6awFYdoVHRHK96YOY+l/TOaq8dk8sWwH5z+8hL99tIO6hk4adDTGBIUlCOOTPglR/OqaMbz53a8yMjOBn72xkUt++yGLN+6np4xjGWNasgRhOmRkvwTmfvMsnrwlDwRuezafGx5fQUHR4WCHZozxM0sQpsNEhKkj+vL298/jwStG8fm+ci7/4zLu+cda9pf7MC2AMaZbsARhTlp4aAhfOzuHpfdM4bavDua1NXuY/JulPPLuF1TVdvTqVGNMV2MJwpyyxOhwfjxjBO/+4HwuGN6HR97dwpSHlzJvVSGNjTY+YUx3ZQnC+M2A1Bj+dOME5t1xNhmJ0fzHP9Yy80/L+GSbl/s2GGO6vIAlCBGJEpFPRWStiGwQkZ97qfNbEVnjLl+ISJnHugaPda8HKk7jf3k5Kcz/1jn8bvY4SitrmfP4cm57Np/tByqDHZoxpgMCNtWGiAgQq6qVIhIOLAPuUtXlbdT/LjBeVb/uvq5U1ThfP8+m2uiaauoaeHLZDh5dspWj9Y3cfPZA7po6jKSYiGCHZowhSFNtqKPpJ2O4u7SXjeYALwQqHhMcUeGh3DllKEvvmcK1ef155uOdnPfrJTzxr+3U1tuFdsZ0ZQEdgxCRUBFZAxQDi1V1RRv1BgKDgPc9iqNEJF9ElovIrDa2u92tk3/gwAG/x2/8Jz0+kv+5ajQL7zqPsf2T+MWbm7j4tx+wqGCfXWhnTBfVKbO5ikgSMB/4rqoWeFl/H5Ctqt/1KOunqntEZDBO4piqqtva+gzrYupelm4u5pdvbmJLcSVn5qTwk8tGMCY7KdhhGdPrBH02V1UtA5YC09qoMptW3Uuqusd93O5uOz5wEZrONvn0Piy866v8YlYu2w5UMvOPH/GDl9awp6w62KEZY1yBPIsp3W05ICLRwIXA517qnQ4kA594lCWLSKT7PA04F9gYqFhNcISFhnDTpIEsuWcyd5w/hAXr9zLl4aX87zubOXLULrQzJtgC2YLIBJaIyDpgJc4YxAIReVBEZnrUmwO8qC37ukYA+SKyFlgCPKSqliB6qISocH40fTjv/eB8Lh6VwR/e38rkh5fyzMc7OVxVF+zwjOm17I5ypsv57MtD/GLBRj77soyI0BCmDE/nyvFZTBneh8gwu0+2Mf7U3hiEJQjTJakqG/aUM391Ea+t2cPByqMkRIVx6ZhMrhyfTd7AZLtXtjF+YAnCdGv1DY18tK2EV1cXsahgH9V1DWQlRTNrfD+uHJ/F0D7xwQ7RmG7LEoTpMY4crWfxxv38c3URy7YcoFFhdFYis8ZncfnYTPrERwU7RGO6FUsQpkcqrqjhjbV7eXV1EeuLDhMi8JVh6Vw5vh+XjMogJsJuuW7MiViCMD3elv0VvLqmiFdX76GorJqYiFAuGZXBrPFZnDsklbBQm7jYGG8sQZheo7FRyd91iPmrC1mwbi8VNfWkx0cyc6wzXjGqXwLOPJLGGLAEYXqpmroGlm4uZv7qIt7/vJi6BmVonziuHJ/FFeP6kZ0cE+wQjQk6SxCm1yurquXN9c54xcqdhwA4c1AKV47PYkZuJokx4UGO0JjgsARhjIfdpVW8tqaIf64uYvuBI0SEhnDB8D7MGp/FlOHpdjGe6VUsQRjjhaqyvugw81cX8cbaPRysrCUxOty9GC+LvIHJNl5hejxLEMacQH1DI8u2HuTV1UW8vWE/1XUNZCdHc+X4LC4ZlcHIzAS7ctv0SJYgjOmAyqP1vLNhH/NXF/HR1oM0KiRGh3PWoBTOHpLKpMGpnN433hKG6RHaSxB2JZExrcRFhnHVhGyumpBNcUUNH209yCfbSvhkewnvbNwPQEpsRHPCOHtwKkP7xFl3lOlxrAVhTAcUHqpi+fZSPtlWwvLtJRS5NzhKi4vgrMFOsjh7SCqD02ItYZhuwbqYjAkAVWV3aTXLtzuti0+2lbCvvAaAPvGRTHKTxdmDUxmYGmMJw3RJ1sVkTACICANSYxiQGsN1Z/RHVdlZUtXcuvhkewmvr90DQGZiFGcPTm1OGv1T7CI90/VZC8KYAFFVth04wifbS1juJo2SI7UAZCVFN7cuJg1JJSspOsjRmt7KupiM6QJUlS3Flc6A97YSVuwo4ZB7S9UBKTHN4xdnD0mlb4JNW246R1AShIhEAR8CkThdWfNU9YFWdW4FfgMUuUV/VNUn3HW3AD9xy3+hqs+093mWIEx309iobN5f0XyG1IrtJZTX1AMwKC22uTtq0uAUu8+FCZhgJQgBYlW1UkTCgWXAXaq63KPOrUCeqn6n1bYpQD6QByiwCpioqofa+jxLEKa7a2hUNu0td8YvtpXw6Y5SKo46CWNIeix5A1MYNyCJsdlJnNY3zqYwN34RlEFqdTJPpfsy3F18zUaXAItVtRRARBYD04AX/B2nMV1FaIiQm5VIblYi3/zqYOobGtmwp7x5wPvtjft4KX83ANHhoYzOTmRc/yTG9U9ibP8k+iVG2ZlSxq8CehaTiITi/PofCvxJVVd4qXa1iJwHfAHcraq7gSxgt0edQres9fvfDtwOMGDAAD9Hb0xwhYWGMNY9+P/7+UNQVXaVVLFmd1nz8rePdlLb0AhAenwkY7OTGO+2Msb0TyQhymapNScvoAlCVRuAcSKSBMwXkVxVLfCo8gbwgqoeFZE7gGeACwBvP4OOa32o6mPAY+B0Mfl9B4zpQkSEnLRYctJimTXe+b1UW9/Ipr3lrC0sY82XTtJ4d9P+5m2GpMcyrn8y4/onMq5/MqdnxBMRZl1Txjedch2EqpaJyFKcbqICj/ISj2qPA79ynxcCkz3WZQNLAxqkMd1QRNixVsbXznbKDlfVsbawjLVuK2Pp5mJe+aywuX5uvwTG9U9mbP9ExvdPpn9KtHVNGa8COUidDtS5ySEaeAf4laou8KiTqap73edXAvep6iR3kHoVMMGt+hnOIHVpW59ng9TGeKeqFB6qbm5lrC0sY33RYWrqnK6plNgIxmYnMrZpPCM7ieTYiCBHbTpLsK6kzgSeccchQoCXVXWBiDwI5Kvq68D3RGQmUA+UArcCqGqpiPwXsNJ9rwfbSw7GmLaJCP1TYuifEsNlY/oBUNfQyOZ9FS2SxtIvDtD0ezEnNaY5YYzrn8SIzASiwu1GSr2NXShnjAGgoqaO9UWHWbP7WPfU/vKjAISHCiMzExiTncSQ9FgGpsUyMCWG7OQYG9Po5mwuJmPMCcVHhXPOkDTOGZLWXLbvcA1rdh9ize7DrNl9iH9+VsiR2obm9SEC/ZKiGZgaw4CUWHJSYxiYGsPA1FgGpMQQG2mHmO7M/vWMMW3KSIxiWmIm03IzAWc840DlUb4sqWJnSRVflhxhV6nzfFHB3uapQ5qkxUWS405oODAllpy0GAakxJCTGktSTLgNjndxliCMMT4TEfrER9EnPoq8nJTj1h+uruPLkip2lR5hV0kVu0qcx0+2lfDPz4pa1I2PCmtubQxMOdbyGJgaQ9/4KLtjXxdgCcIY4zeJ0eGMzk5kdHbicetq6hrYXVrFrpIqdpYc4Uv3+Yaiw7xdsI/6xmPjoZFhIQxolTSaWh5ZydGE2zQjncIShDGmU0SFhzKsbzzD+sYft66+oZG9h2vY6bY4viytYudBJ4l8tLWE6rpj4x6hIUK/pCgGpDhJo7/72LQkRlvXlb9YgjDGBF1YaEjzqbhfHdZynapyoOIou9wWR1O31e5DVSzeuJ+DlbUt6sdHhdE/2U0YqS0TSFZStJ111QGWIIwxXZqI0Cchij4JUZzhZdzjyNF6dh+q4ku35bG71HnceqCS9zcXU1vf6PFekJkQ1bLVkeqcrjsgJYa0uAhrfXiwBGGM6dZiI8MYnpHA8IyE49Y1NrpnXZUen0A+3HKg+TqPJtHhoa26raKbn/dPiel1FwtagjDG9FghIULfhCj6ttH6qKlroPBQlUcCqW5OIh9vO0iVxzUfAH3iI5uTRf+UGLKTo8lIiCIj0fmMhKiwHtUCsQRhjOm1osJDGdonnqF9jh84V1VKjtQea3W4LZAvS6tYsb2EV9cU0XoiipiIUDchRZKREEXfxCgngXg87xMf2W1u9mQJwhhjvBAR0uIiSYuLZMKA5OPWH61vYN/hGmcpr2F/eQ37Dh91HstrWLnzEMUVNdQ1aKv3dS4gzHBbNhmJns+PJZP4yOC3RixBGGPMSYgMC3Wv0Yhts05jo1JaVcu+wzXNiWO/m1D2lR9ld2kVK3eWcri67rhtYyJCWySOvglRZCREHnueGEV6XGBbI5YgjDEmQEJCjrVCcrOOv3iwSXVtw7EEUt66VVLDpztK2V9e0+JiQnDmwkqLi+Sswan8Yc54v8dvCcIYY4IsOiK0+W6BbWlsdMZEvCWQ9PjIgMRlCcIYY7qBkBAhPT6S9Pj2WyN+/cxO+RRjjDHdjiUIY4wxXlmCMMYY41XAEoSIRInIpyKyVkQ2iMjPvdT5gYhsFJF1IvKeiAz0WNcgImvc5fVAxWmMMca7QA5SHwUuUNVKEQkHlonIQlVd7lFnNZCnqlUi8i3g18D17rpqVR0XwPiMMca0I2AtCHVUui/D3UVb1VmiqlXuy+VAdqDiMcYY0zEBHYMQkVARWQMUA4tVdUU71b8BLPR4HSUi+SKyXERmBTJOY4wxxwvodRCq2gCME5EkYL6I5KpqQet6InITkAec71E8QFX3iMhg4H0RWa+q21ptdztwO8CAAQMCth/GGNMbibaejjBQHyTyAHBEVR9uVX4h8AfgfFUtbmPbvwELVHVeO+9/ANh1CiGmAQdPYfuexL6Lluz7aMm+j2N6wncxUFXTva0IWAtCRNKBOlUtE5Fo4ELgV63qjAf+CkzzTA4ikgxUqepREUkDzsUZwG5TWzvYgXjzVTXvVN6jp7DvoiX7Plqy7+OYnv5dBLKLKRN4RkRCccY6XlbVBSLyIJCvqq8DvwHigH+409p+qaozgRHAX0Wk0d32IVXdGMBYjTHGtBKwBKGq64DjphdU1f/0eH5hG9t+DIwOVGzGGGNOzK6kPuaxYAfQhdh30ZJ9Hy3Z93FMj/4uOm2Q2hhjTPdiLQhjjDFeWYIwxhjjVa9PECIyTUQ2i8hWEflRsOMJJhHpLyJLRGSTO8HiXcGOKdjc2QBWi8iCYMcSbCKSJCLzRORz9//I2cGOKZhE5G7376RARF4Qkahgx+RvvTpBuKfg/gmYDowE5ojIyOBGFVT1wA9VdQQwCbizl38fAHcBm4IdRBfxO2CRqg4HxtKLvxcRyQK+hzPZaC4QCswOblT+16sTBHAmsFVVt6tqLfAicEWQYwoaVd2rqp+5zytwDgBZwY0qeEQkG7gUeCLYsQSbiCQA5wFPAqhqraqWBTeqoAsDokUkDIgB9gQ5Hr/r7QkiC9jt8bqQXnxA9CQiOTjXsbQ3wWJP9whwL9AY7EC6gMHAAeBpt8vtCRGJDXZQwaKqRcDDwJfAXuCwqr4T3Kj8r7cnCPFS1uvP+xWROOAV4PuqWh7seIJBRC4DilV1VbBj6SLCgAnAn1V1PHAE6LVjdu50QFcAg4B+QKw76WiP0tsTRCHQ3+N1Nj2wmdgR7s2dXgHmquo/gx1PEJ0LzBSRnThdjxeIyHPBDSmoCoFCjyn75+EkjN7qQmCHqh5Q1Trgn8A5QY7J73p7glgJDBORQSISgTPI1GtvbyrOhFhPAptU9f+CHU8wqer9qpqtqjk4/y/eV9Ue9wvRV6q6D9gtIqe7RVOB3jw/2pfAJBGJcf9uptIDB+0Dej+Irk5V60XkO8DbOGchPKWqG4IcVjCdC9wMrHdv9ATwY1V9K4gxma7ju8Bc98fUduDfghxP0KjqChGZB3yGc/bfanrgtBs21YYxxhivensXkzHGmDZYgjDGGOOVJQhjjDFeWYIwxhjjlSUIY4wxXlmCMKYLEJHJNmOs6WosQRhjjPHKEoQxHSAiN4nIpyKyRkT+6t4volJE/ldEPhOR90Qk3a07TkSWi8g6EZnvzt+DiAwVkXdFZK27zRD37eM87rcw171C15igsQRhjI9EZARwPXCuqo4DGoAbgVjgM1WdAHwAPOBu8ixwn6qOAdZ7lM8F/qSqY3Hm79nrlo8Hvo9zb5LBOFe2GxM0vXqqDWM6aCowEVjp/riPBopxpgN/ya3zHPBPEUkEklT1A7f8GeAfIhIPZKnqfABVrQFw3+9TVS10X68BcoBlgd8tY7yzBGGM7wR4RlXvb1Eo8tNW9dqbv6a9bqOjHs8bsL9PE2TWxWSM794DrhGRPgAikiIiA3H+jq5x69wALFPVw8AhEfmqW34z8IF7f41CEZnlvkekiMR06l4Y4yP7hWKMj1R1o4j8BHhHREKAOuBOnJvnjBKRVcBhnHEKgFuAv7gJwHP205uBv4rIg+57XNuJu2GMz2w2V2NOkYhUqmpcsOMwxt+si8kYY4xX1oIwxhjjlbUgjDHGeGUJwhhjjFeWIIwxxnhlCcIYY4xXliCMMcZ49f8BxrCvk4DwzcoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Par-Inject VGG loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 606s 95ms/step - loss: 4.6620 - val_loss: 4.1499\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.14990, saving model to model-loss4.678-val_loss4.150_Per_CapsNet_par.h5\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 545s 85ms/step - loss: 3.9110 - val_loss: 3.9678\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.14990 to 3.96782, saving model to model-loss3.924-val_loss3.968_Per_CapsNet_par.h5\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 454s 71ms/step - loss: 3.6436 - val_loss: 3.9311\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.96782 to 3.93105, saving model to model-loss3.656-val_loss3.931_Per_CapsNet_par.h5\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 448s 70ms/step - loss: 3.4777 - val_loss: 3.9276\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.93105 to 3.92759, saving model to model-loss3.490-val_loss3.928_Per_CapsNet_par.h5\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 449s 70ms/step - loss: 3.3577 - val_loss: 3.9209\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.92759 to 3.92089, saving model to model-loss3.371-val_loss3.921_Per_CapsNet_par.h5\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 448s 70ms/step - loss: 3.2661 - val_loss: 3.9182\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.92089 to 3.91821, saving model to model-loss3.280-val_loss3.918_Per_CapsNet_par.h5\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 454s 71ms/step - loss: 3.1935 - val_loss: 3.9414\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.91821\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 458s 72ms/step - loss: 3.1348 - val_loss: 3.9750\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.91821\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 539s 84ms/step - loss: 3.0871 - val_loss: 4.0125\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.91821\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 599s 94ms/step - loss: 3.0460 - val_loss: 4.0595\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.91821\n"
     ]
    }
   ],
   "source": [
    "# Train model for CapsNet\n",
    "epochs = 10\n",
    "train_steps = len(train_desc)\n",
    "val_steps = len(valid_desc)\n",
    "filepath = 'model-loss{loss:.3f}-val_loss{val_loss:.3f}_Per_CapsNet_par.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# create the data generator\n",
    "train_generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "valid_generator = data_generator(valid_desc, valid_features, train_tokenizer, max_length, vocab_size)\n",
    "# fit for one epoch\n",
    "history = model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, verbose=1, validation_data=valid_generator,\\\n",
    "                    validation_steps=val_steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Par-Inject CapsNet loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statement to instantiate the models\n",
    "from utils import model_utils\n",
    "# Import statements for other calculations\n",
    "from pickle import load\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from numpy import array\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_beam_search(model, tokenizer, photo, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Description: This function can be used to create description\n",
    "    :model: The decoder model object\n",
    "    :tokenizer: The tokenizer object used to get the words from predicted indexes\n",
    "    :max_length: The maximum length of the sentence to be generated\n",
    "    :beam_length: Length to check conditional probability. \n",
    "                1: for greedy search\n",
    "                1+: For beam search\n",
    "    \"\"\"\n",
    "    in_text = 'startseq'\n",
    "    beam_list = list()\n",
    "    for i in range(max_length):\n",
    "        if not beam_list:\n",
    "            sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "            yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "            yhat_idx = yhat.argsort()[-beam_length:]\n",
    "            for idx in yhat_idx:\n",
    "                word = tokenizer.index_word[idx]\n",
    "                in_text += ' ' + word\n",
    "                beam_list.append((in_text, log(yhat[idx])))\n",
    "        else:\n",
    "            combination_list = list()\n",
    "            for elems in beam_list:\n",
    "                if elems[0].endswith('endseq'):\n",
    "                    combination_list.append(elems)\n",
    "                    continue\n",
    "                in_text = elems[0]\n",
    "                sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "                sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "                yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "                yhat_idx = yhat.argsort()[-beam_length:]\n",
    "                for idx in yhat_idx:\n",
    "                    word = tokenizer.index_word[idx]\n",
    "                    if word is None:\n",
    "                        continue\n",
    "                    in_text += ' ' + word\n",
    "                    combination_list.append((in_text, elems[1]*log(yhat[idx])))\n",
    "            probs = array([combinations[1] for combinations in combination_list])\n",
    "            top_idx = probs.argsort()[-beam_length:]\n",
    "            for i, idx in enumerate(top_idx):\n",
    "                beam_list[i] = combination_list[idx]\n",
    "    probs = array([prob[1] for prob in beam_list])\n",
    "    top_idx = argmax(probs)        \n",
    "    return beam_list[top_idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BLEU(model, test_desc, photo_feature, tokenizer, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Decription: This function can be used to evaluate BLEU score of the model word by word\n",
    "    :model: The Decoder model\n",
    "    :test_desc: test description\n",
    "    :photo_feature: Extracted features of photos\n",
    "    :tokenizer: Tokenizer object\n",
    "    :max_length: Maximum length of the expected sentence\n",
    "    :beam_length: Beam Length for beam search\n",
    "    \"\"\"\n",
    "    actual, predicted = list(), list()\n",
    "    count = 0\n",
    "    for key, desc_list in test_desc.items():\n",
    "        yhat = generate_desc_beam_search(model, tokenizer, photo_feature[key], max_length, beam_length)\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "features = 'features_vgg.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "test_features = {image_id:feat for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.513146\n",
      "BLEU-2: 0.281488\n",
      "BLEU-3: 0.192421\n",
      "BLEU-4: 0.087844\n"
     ]
    }
   ],
   "source": [
    "### Get the BLEU score VGG extracted features with beam length 1\n",
    "vgg_decoder_path = \"model-ep007-loss3.446-val_loss3.881_VGG_par.h5\"\n",
    "test_model = load_model(vgg_decoder_path)\n",
    "beam_length = 1\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.260571\n",
      "BLEU-2: 0.102045\n",
      "BLEU-3: 0.052722\n",
      "BLEU-4: 0.011007\n"
     ]
    }
   ],
   "source": [
    "# Beam length 2\n",
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by CapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "features = 'features_capsnet.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "test_features = {image_id:feat for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.493075\n",
      "BLEU-2: 0.234277\n",
      "BLEU-3: 0.155373\n",
      "BLEU-4: 0.073907\n"
     ]
    }
   ],
   "source": [
    "### Get the BLEU score VGG extracted features with beam length 1\n",
    "caps_decoder_path = \"model-loss3.280-val_loss3.918_Per_CapsNet_par.h5\"\n",
    "test_model = load_model(caps_decoder_path)\n",
    "beam_length = 1\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.286025\n",
      "BLEU-2: 0.091102\n",
      "BLEU-3: 0.031987\n",
      "BLEU-4: 0.000000\n"
     ]
    }
   ],
   "source": [
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model, arch, image_path):\n",
    "    \"\"\"\n",
    "    Description: Extract features for a given image\n",
    "    :model: The Encoder model\n",
    "    :arch: The arch type\n",
    "    :image_path: Path to the image\n",
    "    \"\"\"\n",
    "    feature = None\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "    try:\n",
    "        image = load_img(image_path, target_size=target_size)\n",
    "    except Exception as e:\n",
    "        print('{} could not be opened. Skipping\\n {}'.format(image_path,e))\n",
    "        return None\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    if arch=='capsnet':\n",
    "        feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "    else:\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'capsnet'\n",
    "encoder_model = initiate_encoder(arch)\n",
    "# Extract features of the image\n",
    "image_path = r'C:\\Users\\jayde\\OneDrive\\Desktop\\247778426_fd59734130.jpg'\n",
    "photo_feature = extract_feature(encoder_model, arch, image_path)\n",
    "max_length = 34\n",
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# Load the decoder model\n",
    "decoder_path = r''\n",
    "test_model = load_model(decoder_path)\n",
    "# Beam Search length\n",
    "beam_length = 1\n",
    "print(generate_desc_beam_search(test_model, tokenizer, photo_feature, max_length, beam_length))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_gpu",
   "language": "python",
   "name": "keras_tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

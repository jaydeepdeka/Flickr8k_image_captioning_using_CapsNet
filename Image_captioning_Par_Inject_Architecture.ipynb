{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "UsageError: Line magic function `%inline` not found.\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "import os\n",
    "import string\n",
    "from utils import model_utils\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump, load\n",
    "\n",
    "# Decoder model imports\n",
    "import numpy as np\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array, prod\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#plot curve\n",
    "import matplotlib.pyplot as plt\n",
    "%inline matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File split for train, test and validation\n",
    "if not ((os.path.exists('train.pkl')) and (os.path.exists('valid.pkl')) and (os.path.exists('test.pkl'))):\n",
    "    train_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.devImages.txt'\n",
    "    test_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.trainImages.txt'\n",
    "    valid_path = r'..\\Flickr8k\\Flickr8k_text\\Flickr_8k.testImages.txt'\n",
    "    paths = []\n",
    "    for path in [train_path, valid_path, test_path]:\n",
    "        with open(path, 'r') as fh:\n",
    "            paths = paths + fh.readlines()\n",
    "    sample_idx = np.random.choice(len(paths), size=int(len(paths)), replace=False)\n",
    "\n",
    "    # Train Set 80% of the data\n",
    "    train_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[:int(len(sample_idx)*.80)]]\n",
    "    valid_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.80):int(len(sample_idx)*.90)]]\n",
    "    test_set = [paths[idx].strip('\\n').split('.')[0] for idx in sample_idx[int(len(sample_idx)*.90):]]\n",
    "    dump(train_set, open('train.pkl', 'wb'))\n",
    "    dump(valid_set, open('valid.pkl', 'wb'))\n",
    "    dump(test_set, open('test.pkl', 'wb'))\n",
    "else:\n",
    "    train_set = load(open('train.pkl', 'rb'))\n",
    "    valid_set = load(open('valid.pkl', 'rb')) \n",
    "    test_set = load(open('test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_encoder(arch='capsnet'):\n",
    "    \"\"\"\n",
    "        Description: Initiate the encoder \n",
    "        :arch: 'capsnet' or 'vgg'\n",
    "    \"\"\"\n",
    "    if arch=='capsnet':\n",
    "        encoder_model = model_utils.load_DeepCapsNet(input_shape=(64,64,3), n_class=10, routings=3, \\\n",
    "                        weights=r'weights\\deep_caps_best_weights.h5')\n",
    "    else:\n",
    "        encoder_model = model_utils.load_VGG()\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, directory, arch, path):\n",
    "    \"\"\"\n",
    "        Description: Function to extract features through the model\n",
    "        :model: The model object\n",
    "        :directory: Path of the directory of images\n",
    "        :path: Path to save the file\n",
    "    \"\"\"\n",
    "    features = dict()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    print('Feature extraction started')\n",
    "    for name in os.listdir(directory):\n",
    "        image_path = directory + '/' + name\n",
    "        target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "        try:\n",
    "            image = load_img(image_path, target_size=target_size)\n",
    "        except:\n",
    "            print('{} could not be opened. Skipping'.format(image_path))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # Extract the features from the last layer\n",
    "        if arch=='capsnet':\n",
    "            feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "        else:\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        # Populate the dictionary\n",
    "        features[image_id] = feature\n",
    "    path = os.path.join(path, 'features_{}.pkl'.format(arch))\n",
    "    dump(features, open(path, 'wb'))\n",
    "    print('Features extracted and stored at {}'.format(path))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv_capsule_layer3d_2/stack:0\", shape=(5,), dtype=int32)\n",
      "Complete Capsule Architecture\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  3584        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 128)  512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "convert_to_caps_2 (ConvertToCap (None, 64, 64, 128,  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_16 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      convert_to_caps_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_18 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_19 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_17 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 32, 4 0           conv2d_caps_19[0][0]             \n",
      "                                                                 conv2d_caps_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_20 (Conv2DCaps)     (None, 16, 16, 32, 8 294912      add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_22 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_23 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_21 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32, 8 0           conv2d_caps_23[0][0]             \n",
      "                                                                 conv2d_caps_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_24 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_26 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_27 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_25 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 32, 8)  0           conv2d_caps_27[0][0]             \n",
      "                                                                 conv2d_caps_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_28 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_29 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_30 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_capsule_layer3d_2 (ConvCap (None, 4, 4, 32, 8)  18688       conv2d_caps_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 4, 32, 8)  0           conv2d_caps_30[0][0]             \n",
      "                                                                 conv_capsule_layer3d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_3 (FlattenCaps)    (None, 512, 8)       0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_4 (FlattenCaps)    (None, 2048, 8)      0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2560, 8)      0           flatten_caps_3[0][0]             \n",
      "                                                                 flatten_caps_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "digit_caps (CapsuleLayer)       (None, 10, 32)       6553920     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_cid_3 (Mask_CID)           (None, 32)           0           digit_caps[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (CapsToScalars)         (None, 10)           0           digit_caps[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 64, 64, 3)    67603       mask_cid_3[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 13,427,283\n",
      "Trainable params: 13,426,995\n",
      "Non-trainable params: 288\n",
      "__________________________________________________________________________________________________\n",
      "Capsule Network as feature extractor\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  3584        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 128)  512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "convert_to_caps_2 (ConvertToCap (None, 64, 64, 128,  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_16 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      convert_to_caps_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_18 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_19 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_17 (Conv2DCaps)     (None, 32, 32, 32, 4 147456      conv2d_caps_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 32, 4 0           conv2d_caps_19[0][0]             \n",
      "                                                                 conv2d_caps_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_20 (Conv2DCaps)     (None, 16, 16, 32, 8 294912      add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_22 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_23 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_21 (Conv2DCaps)     (None, 16, 16, 32, 8 589824      conv2d_caps_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32, 8 0           conv2d_caps_23[0][0]             \n",
      "                                                                 conv2d_caps_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_24 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_26 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_27 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_25 (Conv2DCaps)     (None, 8, 8, 32, 8)  589824      conv2d_caps_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 32, 8)  0           conv2d_caps_27[0][0]             \n",
      "                                                                 conv2d_caps_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_28 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_29 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_caps_30 (Conv2DCaps)     (None, 4, 4, 32, 8)  589824      conv2d_caps_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_capsule_layer3d_2 (ConvCap (None, 4, 4, 32, 8)  18688       conv2d_caps_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 4, 32, 8)  0           conv2d_caps_30[0][0]             \n",
      "                                                                 conv_capsule_layer3d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_3 (FlattenCaps)    (None, 512, 8)       0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_caps_4 (FlattenCaps)    (None, 2048, 8)      0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2560, 8)      0           flatten_caps_3[0][0]             \n",
      "                                                                 flatten_caps_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "digit_caps (CapsuleLayer)       (None, 10, 32)       6553920     concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 13,359,680\n",
      "Trainable params: 13,359,424\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_dir = r'..\\Flickr8k\\Flicker8k_Dataset'\n",
    "arch = 'capsnet'\n",
    "encoder_model = initiate_encoder(arch=arch)\n",
    "if not os.path.exists('features_{}.pkl'.format(arch)):\n",
    "    extract_features(encoder_model, img_dir, arch, r'..\\Flickr8k_image_captioning_using_CapsNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filename):\n",
    "    \"\"\"\n",
    "        Description: Generic function to read files and return contents\n",
    "        :filename: Path of the files\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fh:\n",
    "        content = fh.readlines()\n",
    "    return ''.join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean descriptions of the images\n",
    "def map_descriptions(desc_content):\n",
    "    \"\"\"\n",
    "        Description: Map the descriptions <image>:[description_list]\n",
    "        :desc_content: File content\n",
    "    \"\"\"\n",
    "    # Each image contains 5 descriptions in the format\n",
    "    # <image_name>#<1-5> sentence\n",
    "    mapping = dict()\n",
    "    lines = list()\n",
    "    for line in desc_content.split('\\n'):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], ' '.join(tokens[1:])\n",
    "        image_id = image_id.split('.')[0]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        image_desc = image_desc.split()\n",
    "        image_desc = [word.lower() for word in image_desc]\n",
    "        image_desc = [w.translate(table) for w in image_desc]\n",
    "        image_desc = [word for word in image_desc if (len(word)>1 and word.isalpha())]\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # Append the list of the dictionary\n",
    "        clean_desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "        mapping[image_id].append(clean_desc)\n",
    "        lines.append(image_id+' '+clean_desc)\n",
    "    # Write the files to a clean description file\n",
    "    with open('descriptions.txt', 'w') as fh:\n",
    "        fh.writelines('\\n'.join(lines))\n",
    "    return mapping\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Desciptions: 8092 \n",
      "Total Vocabulary: 8765\n"
     ]
    }
   ],
   "source": [
    "filename = r'..\\Flickr8k\\Flickr8k_text\\Flickr8k.token.txt'\n",
    "doc = read_files(filename)\n",
    "descriptions = map_descriptions(doc)\n",
    "print('Total Desciptions: %d ' % len(descriptions))\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Total Vocabulary: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparaing Training Set\n",
    "* The dataset contains multiple files inside Flickr8k_text. The 8000 images are divided into:\n",
    "    * Training Set: 6000\n",
    "    * Validation Set: 1000\n",
    "    * Test Set: 1000\n",
    "* The images names for the training names are stored in the Flickr_8k.trainImages.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training dataset: 6400\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Training dataset: {}\".format(len(set(train_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"\n",
    "    Description: Tokenize the description\n",
    "    \"\"\"\n",
    "    all_desc = list()\n",
    "    for _, desc in descriptions.items():\n",
    "        [all_desc.append(d) for d in desc]\n",
    "    tokenizer = Tokenizer()\n",
    "    max_length = max([len(desc.split()) for desc in all_desc])\n",
    "    tokenizer.fit_on_texts(all_desc)\n",
    "    dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "    return tokenizer, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training descriptions\n",
    "train_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in train_set}\n",
    "# Tokenize the the train description\n",
    "train_tokenizer, max_length = create_tokenizer(train_desc)\n",
    "# Get the features of training dataset\n",
    "feature_path = \"features_{}.pkl\".format(arch)\n",
    "# feature_path = \"features_VGG.pkl\"\n",
    "all_features = load(open(feature_path, 'rb'))\n",
    "train_features = {image_id:feat for image_id, feat in all_features.items() if image_id in train_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7864\n",
      "Maximum Legth: 34\n",
      "loaded photo features: 6400\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(train_tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: {}\\nMaximum Legth: {}\\nloaded photo features: {}'\\\n",
    "      .format(vocab_size, max_length, len(train_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = {image_id:feat for image_id, feat in all_features.items() if image_id in valid_set}\n",
    "valid_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in valid_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(encoder_shape, vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    Description: Define the decoder model\n",
    "    :encoder_shape: Input from the image feature\n",
    "    :vocab_size: \n",
    "    :max_length: maximum length of the description\n",
    "    \"\"\"\n",
    "    inputs1 = Input(shape=(encoder_shape,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    # Add layer\n",
    "    addlayer = add([fe2, se2])\n",
    "    # decoder model\n",
    "    decoder1 = LSTM(256)(addlayer)\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 320)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 320)          0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      2013184     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          82176       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 34, 256)      0           dense_3[0][0]                    \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 7864)         2021048     dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,707,512\n",
      "Trainable params: 4,707,512\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_op_shape = prod(list(filter(None, encoder_model.layers[-1].output.shape.as_list())))\n",
    "model = define_model(encoder_op_shape, vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    \"\"\"\n",
    "    Description: Create seqences for input <photo>, <description>, <output>\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 645s 101ms/step - loss: 5.1305 - val_loss: 4.3998\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.39979, saving model to model-ep001-loss5.147-val_loss4.400_VGG_par.h5\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 642s 100ms/step - loss: 4.1739 - val_loss: 4.0906\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.39979 to 4.09058, saving model to model-ep002-loss4.194-val_loss4.091_VGG_par.h5\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 625s 98ms/step - loss: 3.8580 - val_loss: 3.9840\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.09058 to 3.98403, saving model to model-ep003-loss3.878-val_loss3.984_VGG_par.h5\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 570s 89ms/step - loss: 3.6827 - val_loss: 3.9413\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.98403 to 3.94134, saving model to model-ep004-loss3.703-val_loss3.941_VGG_par.h5\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 516s 81ms/step - loss: 3.5700 - val_loss: 3.9078\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.94134 to 3.90776, saving model to model-ep005-loss3.591-val_loss3.908_VGG_par.h5\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 513s 80ms/step - loss: 3.4911 - val_loss: 3.9041\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.90776 to 3.90409, saving model to model-ep006-loss3.513-val_loss3.904_VGG_par.h5\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 516s 81ms/step - loss: 3.4244 - val_loss: 3.8810\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.90409 to 3.88098, saving model to model-ep007-loss3.446-val_loss3.881_VGG_par.h5\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 522s 82ms/step - loss: 3.3811 - val_loss: 3.8849\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.88098\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 510s 80ms/step - loss: 3.3400 - val_loss: 3.8976\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.88098\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 517s 81ms/step - loss: 3.3071 - val_loss: 3.9148\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.88098\n"
     ]
    }
   ],
   "source": [
    "# Train model for VGG\n",
    "epochs = 10\n",
    "train_steps = len(train_desc)\n",
    "val_steps = len(valid_desc)\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}_VGG_par.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "train_generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "valid_generator = data_generator(valid_desc, valid_features, train_tokenizer, max_length, vocab_size)\n",
    "# fit for one epoch\n",
    "history = model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, verbose=1, validation_data=valid_generator,\\\n",
    "                    validation_steps=val_steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Par-Inject VGG loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 590s 92ms/step - loss: 4.6537 - val_loss: 4.1329\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.13291, saving model to model-loss4.669-val_loss4.133_Per_CapsNet_par.h5\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 602s 94ms/step - loss: 3.8906 - val_loss: 3.9535\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.13291 to 3.95354, saving model to model-loss3.904-val_loss3.954_Per_CapsNet_par.h5\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 595s 93ms/step - loss: 3.6137 - val_loss: 3.9106\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.95354 to 3.91059, saving model to model-loss3.627-val_loss3.911_Per_CapsNet_par.h5\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 488s 76ms/step - loss: 3.4434 - val_loss: 3.9143\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.91059\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 461s 72ms/step - loss: 3.3220 - val_loss: 3.9237\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.91059\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 472s 74ms/step - loss: 3.2285 - val_loss: 3.9543\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.91059\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 473s 74ms/step - loss: 3.1545 - val_loss: 3.9772\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.91059\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 501s 78ms/step - loss: 3.0984 - val_loss: 4.0247\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.91059\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 600s 94ms/step - loss: 3.0501 - val_loss: 4.0691\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.91059\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 584s 91ms/step - loss: 3.0062 - val_loss: 4.1055\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.91059\n"
     ]
    }
   ],
   "source": [
    "# Train model for CapsNet\n",
    "epochs = 10\n",
    "train_steps = len(train_desc)\n",
    "val_steps = len(valid_desc)\n",
    "filepath = 'model-loss{loss:.3f}-val_loss{val_loss:.3f}_Per_CapsNet_par.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# create the data generator\n",
    "train_generator = data_generator(train_desc, train_features, train_tokenizer, max_length, vocab_size)\n",
    "valid_generator = data_generator(valid_desc, valid_features, train_tokenizer, max_length, vocab_size)\n",
    "# fit for one epoch\n",
    "history = model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, verbose=1, validation_data=valid_generator,\\\n",
    "                    validation_steps=val_steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Par-Inject CapsNet loss plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statement to instantiate the models\n",
    "from utils import model_utils\n",
    "# Import statements for other calculations\n",
    "from pickle import load\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from numpy import array\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_beam_search(model, tokenizer, photo, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Description: This function can be used to create description\n",
    "    :model: The decoder model object\n",
    "    :tokenizer: The tokenizer object used to get the words from predicted indexes\n",
    "    :max_length: The maximum length of the sentence to be generated\n",
    "    :beam_length: Length to check conditional probability. \n",
    "                1: for greedy search\n",
    "                1+: For beam search\n",
    "    \"\"\"\n",
    "    in_text = 'startseq'\n",
    "    beam_list = list()\n",
    "    for i in range(max_length):\n",
    "        if not beam_list:\n",
    "            sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "            yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "            yhat_idx = yhat.argsort()[-beam_length:]\n",
    "            for idx in yhat_idx:\n",
    "                word = tokenizer.index_word[idx]\n",
    "                in_text += ' ' + word\n",
    "                beam_list.append((in_text, log(yhat[idx])))\n",
    "        else:\n",
    "            combination_list = list()\n",
    "            for elems in beam_list:\n",
    "                if elems[0].endswith('endseq'):\n",
    "                    combination_list.append(elems)\n",
    "                    continue\n",
    "                in_text = elems[0]\n",
    "                sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "                sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "                yhat = model.predict([photo,sequence], verbose=0).squeeze()\n",
    "                yhat_idx = yhat.argsort()[-beam_length:]\n",
    "                for idx in yhat_idx:\n",
    "                    word = tokenizer.index_word[idx]\n",
    "                    if word is None:\n",
    "                        continue\n",
    "                    in_text += ' ' + word\n",
    "                    combination_list.append((in_text, elems[1]*log(yhat[idx])))\n",
    "            probs = array([combinations[1] for combinations in combination_list])\n",
    "            top_idx = probs.argsort()[-beam_length:]\n",
    "            for i, idx in enumerate(top_idx):\n",
    "                beam_list[i] = combination_list[idx]\n",
    "    probs = array([prob[1] for prob in beam_list])\n",
    "    top_idx = argmax(probs)        \n",
    "    return beam_list[top_idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BLEU(model, test_desc, photo_feature, tokenizer, max_length, beam_length=1):\n",
    "    \"\"\"\n",
    "    Decription: This function can be used to evaluate BLEU score of the model word by word\n",
    "    :model: The Decoder model\n",
    "    :test_desc: test description\n",
    "    :photo_feature: Extracted features of photos\n",
    "    :tokenizer: Tokenizer object\n",
    "    :max_length: Maximum length of the expected sentence\n",
    "    :beam_length: Beam Length for beam search\n",
    "    \"\"\"\n",
    "    actual, predicted = list(), list()\n",
    "    count = 0\n",
    "    for key, desc_list in test_desc.items():\n",
    "        yhat = generate_desc_beam_search(model, tokenizer, photo_feature[key], max_length, beam_length)\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "features = 'features_vgg.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "test_features = {image_id:feat for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.513146\n",
      "BLEU-2: 0.281488\n",
      "BLEU-3: 0.192421\n",
      "BLEU-4: 0.087844\n"
     ]
    }
   ],
   "source": [
    "### Get the BLEU score VGG extracted features with beam length 1\n",
    "vgg_decoder_path = \"model-ep007-loss3.446-val_loss3.881_VGG_par.h5\"\n",
    "test_model = load_model(vgg_decoder_path)\n",
    "beam_length = 1\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.260571\n",
      "BLEU-2: 0.102045\n",
      "BLEU-3: 0.052722\n",
      "BLEU-4: 0.011007\n"
     ]
    }
   ],
   "source": [
    "# Beam length 2\n",
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score on the extracted features by CapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "features = 'features_capsnet.pkl'\n",
    "all_features = load(open(features, 'rb'))\n",
    "test_features = {image_id:feat for image_id, feat in all_features.items() if image_id in test_set}\n",
    "test_desc = {image_id:desc for image_id, desc in descriptions.items() if image_id in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.506081\n",
      "BLEU-2: 0.250129\n",
      "BLEU-3: 0.164627\n",
      "BLEU-4: 0.080211\n"
     ]
    }
   ],
   "source": [
    "### Get the BLEU score VGG extracted features with beam length 1\n",
    "caps_decoder_path = \"model-loss3.627-val_loss3.911_Per_CapsNet_par.h5\"\n",
    "test_model = load_model(caps_decoder_path)\n",
    "beam_length = 1\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3\\envs\\keras_tf_gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.332703\n",
      "BLEU-2: 0.098976\n",
      "BLEU-3: 0.030911\n",
      "BLEU-4: 0.000000\n"
     ]
    }
   ],
   "source": [
    "beam_length = 2\n",
    "get_BLEU(test_model, test_desc, test_features, tokenizer, max_length, beam_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model, arch, image_path):\n",
    "    \"\"\"\n",
    "    Description: Extract features for a given image\n",
    "    :model: The Encoder model\n",
    "    :arch: The arch type\n",
    "    :image_path: Path to the image\n",
    "    \"\"\"\n",
    "    feature = None\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    target_size = (64,64) if arch=='capsnet' else (224,224)\n",
    "    try:\n",
    "        image = load_img(image_path, target_size=target_size)\n",
    "    except Exception as e:\n",
    "        print('{} could not be opened. Skipping\\n {}'.format(image_path,e))\n",
    "        return None\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    if arch=='capsnet':\n",
    "        feature = model.predict(image, verbose=0).reshape(-1, 10*32)\n",
    "    else:\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'capsnet'\n",
    "encoder_model = initiate_encoder(arch)\n",
    "# Extract features of the image\n",
    "image_path = r'C:\\Users\\jayde\\OneDrive\\Desktop\\247778426_fd59734130.jpg'\n",
    "photo_feature = extract_feature(encoder_model, arch, image_path)\n",
    "max_length = 34\n",
    "# Load the tokenizer model\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# Load the decoder model\n",
    "decoder_path = r''\n",
    "test_model = load_model(decoder_path)\n",
    "# Beam Search length\n",
    "beam_length = 1\n",
    "print(generate_desc_beam_search(test_model, tokenizer, photo_feature, max_length, beam_length))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_gpu",
   "language": "python",
   "name": "keras_tf_gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
